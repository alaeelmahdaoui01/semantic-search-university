[
  {
    "text": "Algorithmic Foundations : Graph Algorithms Omar Saadi omar.saadi@um6p.ma UM6P - College of Computing 1st year of the engineering degree of UM6P-CS Academic year 2024-2025 Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 1 / 28",
    "source": "Chapter1_AlgoFoundations_GA.pdf",
    "page": 1
  },
  {
    "text": "Why Graphs ? Graphs have applications in many areas, including : Social networks, air networks, road networks, energy networks (electricity, gas, water,...) Mathematics : graphs are useful is many areas of mathematics (geometry, dynamic systems, operations research,...) Computer science : links between web pages, networks of communication, data organization, map networks, ... Statistical physics (interactions between parts of a system), study of molecules, ... Some of the objectives of the course : Understand the structure of graphs Learn the techniques used to analyze problems in graph theory Learn algorithms on trees and graphs Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 2 / 28",
    "source": "Chapter1_AlgoFoundations_GA.pdf",
    "page": 2
  },
  {
    "text": "Chapter 1 : Fundamental concepts about graphs 1 Definition of a graph 2 Graphs as models 3 Matrix representation and isomorphism 4 Decomposition and Special graphs Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 3 / 28",
    "source": "Chapter1_AlgoFoundations_GA.pdf",
    "page": 3
  },
  {
    "text": "Definition of a graph Outline 1 Definition of a graph 2 Graphs as models 3 Matrix representation and isomorphism 4 Decomposition and Special graphs Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 4 / 28",
    "source": "Chapter1_AlgoFoundations_GA.pdf",
    "page": 4
  },
  {
    "text": "Definition of a graph The K¨ onigsberg Bridge Problem The city of K¨ onigsberg was located on the Pregel river in Prussia which divides it into four regions. These regions are connected together using seven bridges, as shown in the following drawing : Citizens wondered if they could take a walk in the city while crossing each bridge exactly once. The resolution of this problem was given by Leonhard Euler in 1736. Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 5 / 28",
    "source": "Chapter1_AlgoFoundations_GA.pdf",
    "page": 5
  },
  {
    "text": "Definition of a graph The K¨ onigsberg Bridge Problem To simplify this problem, we reduce the drawing to the following diagram : a b c d e1 e2 e3 e4 e5 e6 e7 The four regions are represented by vertices and the seven bridges by edges. Answer : Such walk is not possible. Proof : Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 6 / 28",
    "source": "Chapter1_AlgoFoundations_GA.pdf",
    "page": 6
  },
  {
    "text": "Definition of a graph Definition A graph G is a triple consisting of a vertex setV (G), an edge setE(G), and a relation that associates with each edge two vertices (not necessarily distinct) called its endpoints. Example In the graph showed above, the vertex set is {a, b, c, d}, the edge set is {e1, e2, e3, e4, e5, e6, e7}, and the assignment of endpoints to edges can be read from the diagram. Definition A loop is an edge whose endpoints are equal. Multiple edgesare edges having the same pair of endpoints. a a b Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 7 / 28",
    "source": "Chapter1_AlgoFoundations_GA.pdf",
    "page": 7
  },
  {
    "text": "Definition of a graph Definition A simple graphis a graph without loops and multiple edges. A graph is loopless means that multiple edges are allowed but loops are not. Example The graph of the K¨ onigsberg bridge problem is loopless but it is not simple. Remark We specify a simple graph G by its vertex set V (G) and edge set E(G). The edge set is treated as a set of unordered pairs of vertices. We write e = uv (or e = vu) for an edge e with endpoints u and v. Definition We say that the vertices u and v are adjacent (or neighbors) if they are the endpoints of the same edge. Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 8 / 28",
    "source": "Chapter1_AlgoFoundations_GA.pdf",
    "page": 8
  },
  {
    "text": "Definition of a graph Example The two following drawings show the same simple graph. The vertex set is {a, b, c, d, e} and the edge set is {ab, ac, ad, bc, bd, cd, de } : a bc d e a b c d e Remark The terms ”vertex” and ”edge” arise from solid geometry. For example the following 3-d cube has vertices linked by edges. Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 9 / 28",
    "source": "Chapter1_AlgoFoundations_GA.pdf",
    "page": 9
  },
  {
    "text": "Graphs as models Outline 1 Definition of a graph 2 Graphs as models 3 Matrix representation and isomorphism 4 Decomposition and Special graphs Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 10 / 28",
    "source": "Chapter1_AlgoFoundations_GA.pdf",
    "page": 10
  },
  {
    "text": "Graphs as models Acquaintance relations and subgraphs Question Does every set of six people contain three mutual acquaintances or three mutual strangers ? We can model this problem using a simple graph with a vertex for each person and an edge for each acquainted pair. Note that the “nonacquaintance” relation yields another graph with the “complementary” set of edges. Definition The complement ¯G of a simple graph G is the simple graph with vertex set V (G) defined by uv ∈ E( ¯G) if and only if uv ̸∈ E(G). A clique in a graph is a set of pairwise adjacent vertices. An independent set (or stable set) in a graph is a set of pairwise nonadjacent vertices. Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 11 / 28",
    "source": "Chapter1_AlgoFoundations_GA.pdf",
    "page": 11
  },
  {
    "text": "Graphs as models In the graph G, {u, x, y} is a clique of size 3 and {u, w} is an independent set of size 2, and these are the largest such sets. Remark In the complement graph ¯G, cliques become independent sets, and independent sets become cliques. Reformulation of the above question :Is it true that every 6-vertex graph has a clique of size 3 or an independent set of size 3 ? Answer :(As exercice) Notice that by deleting the edge ux from the above graph G, we get a 5-vertex graph having no clique or independent set of size 3. So the answer is no for 5-vertex graphs. Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 12 / 28",
    "source": "Chapter1_AlgoFoundations_GA.pdf",
    "page": 12
  },
  {
    "text": "Graphs as models Job assignments and bipartite graphs Question We have m jobs, and n people, but not all people are qualified for all jobs. Can we fill the jobs with qualified people ? We model this using a simple graph G with vertices for the jobs and people ; job j is adjacent to person p if p can do j. So the question is to find m pairwise disjoint edges in G. Definition A graph G is bipartite if V (G) is the union of two disjoint independent sets called partite setsof G. Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 13 / 28",
    "source": "Chapter1_AlgoFoundations_GA.pdf",
    "page": 13
  },
  {
    "text": "Graphs as models Scheduling and graph coloring Question Suppose we must schedule Parliament committee meetings into designated weekly time periods. We cannot assign two committees to the same time if they have a common member. How many different time periods do we need ? Modeling : We create a vertex for each committee, with two vertices adjacent when the two committees have a common member. We must assign labels (time periods) to the vertices such that adjacent vertices receive different labels. Example : We can use one color (label) for each of the three independent sets. The members of a clique must receive distinct labels ⇒ the minimum number of colors (time periods) is three. Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 14 / 28",
    "source": "Chapter1_AlgoFoundations_GA.pdf",
    "page": 14
  },
  {
    "text": "Graphs as models Scheduling and graph coloring Question Suppose we must schedule Parliament committee meetings into designated weekly time periods. We cannot assign two committees to the same time if they have a common member. How many different time periods do we need ? Modeling : We create a vertex for each committee, with two vertices adjacent when the two committees have a common member. We must assign labels (time periods) to the vertices such that adjacent vertices receive different labels. Example : We can use one color (label) for each of the three independent sets. The members of a clique must receive distinct labels ⇒ the minimum number of colors (time periods) is three. Example : We can use one color (label) for each of the three independent sets. The members of a clique must receive distinct labels ⇒ the minimum number of colors (time periods) is three. Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 14 / 28",
    "source": "Chapter1_AlgoFoundations_GA.pdf",
    "page": 15
  },
  {
    "text": "Graphs as models Scheduling and graph coloring Question Suppose we must schedule Parliament committee meetings into designated weekly time periods. We cannot assign two committees to the same time if they have a common member. How many different time periods do we need ? Modeling : We create a vertex for each committee, with two vertices adjacent when the two committees have a common member. We must assign labels (time periods) to the vertices such that adjacent vertices receive different labels. Example : We can use one color (label) for each of the three independent sets. The members of a clique must receive distinct labels ⇒ the minimum number of colors (time periods) is three. Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 14 / 28",
    "source": "Chapter1_AlgoFoundations_GA.pdf",
    "page": 16
  },
  {
    "text": "Graphs as models Definition A coloring of a graph is an assignment of a color to each vertex such that adjacent vertices receive different colors. The chromatic number of a graph G, written χ(G), is the minimum number of colors needed to color G. A graph G is k-partite if V (G) can be expressed as the union of k (possibly empty) independent sets. So the above question is asking about the chromatic number of the graph and the corresponding coloring. Remark The k-partite notion generalizes the idea of bipartite graphs, which are 2-partite. A graph is k-partite if and only if its chromatic number is at most k. Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 15 / 28",
    "source": "Chapter1_AlgoFoundations_GA.pdf",
    "page": 17
  },
  {
    "text": "Graphs as models Routes in road networks Modeling : We can model a road network using a graph where : Vertices are intersections, Edges are road segments between intersections, We can assign edge weights to measure distance or travel time. Question How can we find the shortest route from a point x to a point y ? Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 16 / 28",
    "source": "Chapter1_AlgoFoundations_GA.pdf",
    "page": 18
  },
  {
    "text": "Graphs as models Definition A path is a simple graph whose vertices can be ordered so that two vertices are adjacent if and only if they are consecutive in the list. A cycle is a graph with an equal number of vertices and edges whose vertices can be placed around a circle so that two vertices are adjacent if and only if they appear consecutively along the circle. Example : a path a cycle Definition A subgraph of a graph G is a graph H such that V (H) ⊂ V (G) and E(H) ⊂ E(G) and the assignment of endpoints to edges in H is the same as in G. We write H ⊂ G and say that “ G contains H”. A graph G is connected if each pair of vertices in G is connected by a path ; otherwise, G is disconnected. Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 17 / 28",
    "source": "Chapter1_AlgoFoundations_GA.pdf",
    "page": 19
  },
  {
    "text": "Matrix representation and isomorphism Outline 1 Definition of a graph 2 Graphs as models 3 Matrix representation and isomorphism 4 Decomposition and Special graphs Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 18 / 28",
    "source": "Chapter1_AlgoFoundations_GA.pdf",
    "page": 20
  },
  {
    "text": "Matrix representation and isomorphism Matrix representation Definition Let G be a loopless graph with vertex set V (G) = {v1, . . . ,vn} and edge set E(G) = {e1, ··· , em}. The adjacency matrixof G, written A(G), is the n-by-n matrix in which entry aij is the number of edges in G with endpoints {vi , vj }. The incidence matrixM(G) is the n-by-m matrix in which entry mij is 1 if vi is an endpoint of ej and otherwise is 0. Example : Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 19 / 28",
    "source": "Chapter1_AlgoFoundations_GA.pdf",
    "page": 21
  },
  {
    "text": "Matrix representation and isomorphism Definition If vertex v is an endpoint of edge e, we say that v and e are incident. The degree of vertex v (in a loopless graph) is the number of incident edges to v. We denote the degree of v by deg(v). Remark An adjacency matrix is determined by a vertex ordering. Every adjacency matrix is symmetric ( aij = aji for all i, j). An adjacency matrix of a simple graph G has entries 0 or 1, with 0s on the diagonal. The degree of v is the sum of the entries in the row for v in either A(G) or M(G). Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 20 / 28",
    "source": "Chapter1_AlgoFoundations_GA.pdf",
    "page": 22
  },
  {
    "text": "Matrix representation and isomorphism Graph isomorphism Definition An isomorphism from a simple graph G to a simple graph H is a bijection f : V (G) → V (H) such that uv ∈ E(G) if and only if f (u)f (v) ∈ E(H). We say “G is-isomorphic to H”, written G ∼= H, if there is an isomorphism from G to H. Example : The following graphs G and H are isomorphic : Proposition The isomorphism relation is an equivalence relation on the set of (simple) graphs. Proof : Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 21 / 28",
    "source": "Chapter1_AlgoFoundations_GA.pdf",
    "page": 23
  },
  {
    "text": "Matrix representation and isomorphism Recall An equivalence relation partitions a set into equivalence classes ; two elements satisfy the relation if and only if they lie in the same class. Definition An isomorphism classof graphs is an equivalence class of graphs under the isomorphism relation. We use the expression “unlabeled graph” to mean an isomorphism class of graphs. Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 22 / 28",
    "source": "Chapter1_AlgoFoundations_GA.pdf",
    "page": 24
  },
  {
    "text": "Matrix representation and isomorphism Definition The (unlabeled) path and cycle with n vertices are denoted Pn and Cn, respectively. A complete graphis a simple graph whose vertices are pairwise adjacent ; the (unlabeled) complete graph with n vertices is denoted Kn. A complete bipartite graph(or biclique) is a simple bipartite graph such that two vertices are adjacent if and only if they are in different partite sets. When the sets have sizes r and s, the (unlabeled) biclique is denoted Kr,s . Example : Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 23 / 28",
    "source": "Chapter1_AlgoFoundations_GA.pdf",
    "page": 25
  },
  {
    "text": "Decomposition and Special graphs Outline 1 Definition of a graph 2 Graphs as models 3 Matrix representation and isomorphism 4 Decomposition and Special graphs Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 24 / 28",
    "source": "Chapter1_AlgoFoundations_GA.pdf",
    "page": 26
  },
  {
    "text": "Decomposition and Special graphs Decomposition of graphs Definition A decomposition of a graph is a list of subgraphs such that each edge appears in exactly one subgraph in the list. Definition and Remark A graph is self-complementary if it is isomorphic to its complement. An n-vertex graph H is self-complementary if and only if Kn has a decomposition consisting of two copies of H. Example : We can decompose K5 into two 5-cycles, and thus the 5-cycle is self-complementary (left figure). We can decompose K4 using three copies of P3 (right figure). Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 25 / 28",
    "source": "Chapter1_AlgoFoundations_GA.pdf",
    "page": 27
  },
  {
    "text": "Decomposition and Special graphs Petersen graph Definition The Petersen graphis the simple graph whose vertices are the 2-element subsets of a 5-element set and whose edges are the pairs of disjoint 2-element subsets. Here, we take [5] = {1, 2, 3, 4, 5} as our 5-element set, we write the pair {a, b} as ab or ba. In the following figure, we have three ways to draw the Petersen graph : The Petersen graph consists of two disjoint 5-cycles plus edges that pair up vertices on the two 5-cycles. Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 26 / 28",
    "source": "Chapter1_AlgoFoundations_GA.pdf",
    "page": 28
  },
  {
    "text": "Decomposition and Special graphs Proposition If two vertices are nonadjacent in the Petersen graph, then they have exactly one common neighbor. Proof : Definition The girth of a graph with a cycle is the length of its shortest cycle. A graph with no cycle has an infinite girth. Lemma The girth of the Petersen graph is equal to5. Proof : Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 27 / 28",
    "source": "Chapter1_AlgoFoundations_GA.pdf",
    "page": 29
  },
  {
    "text": "Decomposition and Special graphs Book references Introduction to graph theory [2nded], by Douglas B. West (2001) : (See Sect 1 .1 of West’s book for more details on our chapter 1.) Algorithms on trees and graphs [2nded], by Gabriel Valiente (2021) : Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 28 / 28",
    "source": "Chapter1_AlgoFoundations_GA.pdf",
    "page": 30
  },
  {
    "text": "Chapter 2 : Paths and Cycles Omar Saadi omar.saadi@um6p.ma UM6P - College of Computing 1st year of the engineering degree of UM6P-CS Academic year 2024-2025 Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 1 / 27",
    "source": "Chapter2_AlgoFoundations3_GA.pdf",
    "page": 1
  },
  {
    "text": "Outline 1 Connection in graphs 2 Cycles and bipartite graphs 3 Eulerian circuits 4 Hamiltonian Cycles More details on this chapter can be found in Sect 1.2 and Sect 7.2 of D. B. West’s book. Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 2 / 27",
    "source": "Chapter2_AlgoFoundations3_GA.pdf",
    "page": 2
  },
  {
    "text": "Connection in graphs Outline 1 Connection in graphs 2 Cycles and bipartite graphs 3 Eulerian circuits 4 Hamiltonian Cycles Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 3 / 27",
    "source": "Chapter2_AlgoFoundations3_GA.pdf",
    "page": 3
  },
  {
    "text": "Connection in graphs Recall A path is a simple graph whose vertices can be ordered so that two vertices are adjacent if and only if they are consecutive in the list. A cycle is a graph with an equal number of vertices and edges whose vertices can be placed around a circle so that two vertices are adjacent if and only if they appear consecutively along the circle. Note that a path in a graph G is a subgraph of G that is a path (similarly for cycles). Definition A walk is a list v0, e1, v1, . . . ,ek , vk of vertices and edges such that, for 1 < i < k, the edge ei has endpoints vi−1 and vi . A trail is a walk with no repeated edge. A u, v-walk or u, v-trail has first vertex u and last vertex v ; these are its endpoints. A u, v-path is a path whose vertices of degree 1 (its endpoints) are u and v ; the others are internal vertices. The length of a walk, trail, path, or cycle is its number of edges. A walk or trail is closed if its endpoints are the same. Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 4 / 27",
    "source": "Chapter2_AlgoFoundations3_GA.pdf",
    "page": 4
  },
  {
    "text": "Connection in graphs Example : In the K¨ onigsberg graph : a b c d e1 e2 e3 e4 e5 e6 e7 the list b, e4, c, e2, a, e1, b, e5, c, e4, b is a closed walk of length 5 ; it repeats edge e4 and hence is not a trail. If we delete the last edge and vertex e4, b, we obtain trail. The list a, e1, b, e4, c, e7, d is a a, d-path, and d, e6, c, e2, a, e3, d is a cycle. Remark : In a simple graph, a walk (or trail) is completely specified by its ordered list of vertices. Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 5 / 27",
    "source": "Chapter2_AlgoFoundations3_GA.pdf",
    "page": 5
  },
  {
    "text": "Connection in graphs Lemma Every u, v-walk contains a u, v-path. Proof : by induction on the length l of a u, v-walk. Definition A graph G is connected if for all u, v ∈ V (G), there exists u, v-path in G (otherwise, G is disconnected). If G has a u, v-path, then u is connected to v in G. The connection relation on V (G) consists of the ordered pairs ( u, v) such that u is connected to v. Remark We recall the distinction between “adjacent” and “connected” vertices : Vertices u and v are adjacent if and only if uv ∈ E(G). Vertices u and v are connected if and only if G contains a u, v-path. Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 6 / 27",
    "source": "Chapter2_AlgoFoundations3_GA.pdf",
    "page": 6
  },
  {
    "text": "Connection in graphs Remark Using the previous lemma, a way to prove that a graph is connected is by fixing a particular vertex v∗ and showing that from each vertex u ∈ V (G) there is a walk to that particular vertex v∗. Proposition The connection relation is an equivalence relation. Proof : Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 7 / 27",
    "source": "Chapter2_AlgoFoundations3_GA.pdf",
    "page": 7
  },
  {
    "text": "Connection in graphs Components of a graph Definition A maximal connected subgraph of G is a subgraph that is connected and that is not contained in any other connected subgraph of G. The components of a graph G are its maximal connected subgraphs. A component (or graph) is trivial if it has no edges ; otherwise it is nontrivial. An isolated vertex is a vertex of degree 0. Remark The equivalence classes of the connection relation on V (G) are the vertex sets of the components of G. An isolated vertex forms a trivial component, consisting of one vertex and no edge. Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 8 / 27",
    "source": "Chapter2_AlgoFoundations3_GA.pdf",
    "page": 8
  },
  {
    "text": "Connection in graphs Example In the following graph : we have four components. The vertex sets of these components are {p}, {q, r}, {s, t, u, v, w}, and {x, y, z}; these are the equivalence classes of the connection relation. Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 9 / 27",
    "source": "Chapter2_AlgoFoundations3_GA.pdf",
    "page": 9
  },
  {
    "text": "Connection in graphs Remark Components are pairwise disjoint ; no two share a vertex. Adding an edge with endpoints in distinct components combines them into one component. Thus, adding an edge decreases the number of components by 0 or 1, Deleting an edge increases the number of components by 0 or 1. Proposition Every graph with n vertices and k edges has at least n − k components. Proof : Remark Deleting a vertex (and also all edges incident to it, to still have a graph) can increase the number of components by many. Consider the example of the K1,m, where deleting the vertex corresponding to the partite set with one vertex increases the number of components from 1 to m. Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 10 / 27",
    "source": "Chapter2_AlgoFoundations3_GA.pdf",
    "page": 10
  },
  {
    "text": "Connection in graphs Cut-deges and cut-vertices Definition A cut-edge or cut-vertex of a graph is an edge or vertex whose deletion increases the number of components. We write G − e or G − M for the subgraph of G obtained by deleting an edge e or set of edges M. Similarly, we write G − v or G − S for the subgraph obtained by deleting a vertex v or set of vertices S. An induced subgraph is a subgraph obtained by deleting a set of vertices. We write G[T ] for G − ¯T , where ¯T = V (G) − T ; this is the subgraph of G induced by T . Note that when T ⊂ V (G), the induced subgraph G[T ] consists of T and all edges whose endpoints are contained in T. Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 11 / 27",
    "source": "Chapter2_AlgoFoundations3_GA.pdf",
    "page": 11
  },
  {
    "text": "Connection in graphs Example : We consider the same above graph : It has cut-vertices v and y. Its cut-edges are qr, vw, xy, and yz. This graph has C4 [ (t, s, u, v) ] and P5 [ {t, s, u, v, w} ] as subgraphs but not as induced subgraphs. The graph P4 is an induced subgraph ; it is the subgraph induced by {s, t, v, w}. Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 12 / 27",
    "source": "Chapter2_AlgoFoundations3_GA.pdf",
    "page": 12
  },
  {
    "text": "Connection in graphs Characterization of cut-edges in terms of cycles Theorem An edge is a cut-edge if and only if it belongs to no cycle. Proof : Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 13 / 27",
    "source": "Chapter2_AlgoFoundations3_GA.pdf",
    "page": 13
  },
  {
    "text": "Cycles and bipartite graphs Outline 1 Connection in graphs 2 Cycles and bipartite graphs 3 Eulerian circuits 4 Hamiltonian Cycles Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 14 / 27",
    "source": "Chapter2_AlgoFoundations3_GA.pdf",
    "page": 14
  },
  {
    "text": "Cycles and bipartite graphs Definition : A walk is odd (resp. even) if its length is odd (resp. even). A closed walk W contains a cycle C if the vertices and edges of C occur as a sublist of W , in cyclic order but not necessarily consecutive. Lemma Every closed odd walk contains an odd cycle. Proof : By induction on the length l of a closed odd walk. Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 15 / 27",
    "source": "Chapter2_AlgoFoundations3_GA.pdf",
    "page": 15
  },
  {
    "text": "Cycles and bipartite graphs Theorem (K¨ onig [1936]) A graph is bipartite if and only if it has no odd cycle. Proof : Remark The theorem implies that we can prove that a graph G is not bipartite by finding an odd cycle in G ; this is much easier than examining all possible bipartitions to prove that none work. When we want to prove that G is bipartite, we define a bipartition and prove that the two sets are independent ; this is easier than examining all cycles. Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 16 / 27",
    "source": "Chapter2_AlgoFoundations3_GA.pdf",
    "page": 16
  },
  {
    "text": "Eulerian circuits Outline 1 Connection in graphs 2 Cycles and bipartite graphs 3 Eulerian circuits 4 Hamiltonian Cycles Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 17 / 27",
    "source": "Chapter2_AlgoFoundations3_GA.pdf",
    "page": 17
  },
  {
    "text": "Eulerian circuits Definition A graph is Eulerian if it has a closed trail containing all edges. We call a closed trail a circuit when we do not specify the first vertex but keep the list in cyclic order. An Eulerian circuit or Eulerian trail in a graph is a circuit or trail containing all the edges. An even graph is a graph with vertex degrees all even. A vertex is odd (resp. even) when its degree is odd (resp. even). A maximal path in a graph G is a path P in G that is not contained in a longer path. When a graph is finite, no path can extend forever, so maximal (non-extendible) paths exist. Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 18 / 27",
    "source": "Chapter2_AlgoFoundations3_GA.pdf",
    "page": 18
  },
  {
    "text": "Eulerian circuits Lemma If every vertex of a graph G has degree at least 2, then G contains a cycle. Proof : by considering a maximal path P in G, and an endpoint of P. Theorem A graph G is Eulerian if and only if it has at most one nontrivial component and its vertices all have even degree. Proof of the theorem Necessity. Suppose that G has an Eulerian circuit C. Each passage of C through a vertex uses two incident edges, and the first edge is paired with the last at the first vertex. Hence every vertex has even degree. Also, two edges can be in the same trail only when they lie in the same component, so there is at most one nontrivial component. Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 19 / 27",
    "source": "Chapter2_AlgoFoundations3_GA.pdf",
    "page": 19
  },
  {
    "text": "Eulerian circuits Proof (Cont.) Sufficiency. Assuming that the condition holds, we obtain an Eulerian circuit using induction on the number of edges, m. Basis step : m = 0. A closed trail consisting of one vertex suffices. Induction step : m > 0. With even degrees, each vertex in the nontrivial component of G has degree at least 2. By the previous Lemma, the nontrivial component has a cycle C. Let G′ be the graph obtained from G by deleting E(C). Since C has 0 or 2 edges at each vertex, each component of G′ is also an even graph. Since each component also is connected and has less than m edges, we can apply the induction hypothesis to conclude that each component of G′ has an Eulerian circuit. To combine these into an Eulerian circuit of G, we traverse C, but when a component of G′ is entered for the first time we detour along an Eulerian circuit of that component. This circuit ends at the vertex where we began the detour. When we complete the traversal of C, we have completed an Eulerian circuit of G. Illustration of the Eulerian circuit constructed : Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 20 / 27",
    "source": "Chapter2_AlgoFoundations3_GA.pdf",
    "page": 20
  },
  {
    "text": "Eulerian circuits Proposition Every even graph decomposes into cycles. Proof : by induction on the number of edges. Proposition If G is a simple graph in which every vertex has degree at least k, then G contains a path of length at least k. If k ≥ 2, then G also contains a cycle of length at least k + 1. Proof : Consider an endpoint of a maximal path P. Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 21 / 27",
    "source": "Chapter2_AlgoFoundations3_GA.pdf",
    "page": 21
  },
  {
    "text": "Hamiltonian Cycles Outline 1 Connection in graphs 2 Cycles and bipartite graphs 3 Eulerian circuits 4 Hamiltonian Cycles Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 22 / 27",
    "source": "Chapter2_AlgoFoundations3_GA.pdf",
    "page": 22
  },
  {
    "text": "Hamiltonian Cycles Definition A graph G is Hamiltonian if it contains a spanning cycle, which is a cycle that goes through each vertex of G exactly once. Such cycle is also called a Hamiltonian cycle. Example : The notion of a Hamiltonian cycle has its origins in a game invented in 1859 by Hamilton : The game consists of a regular wooden dodecahedron (polyhedron with 12 faces and 20 vertices), as shown in the following figure The player must go through the twenty vertices exactly once following the edges of the dodecahedron and return to his starting point. Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 23 / 27",
    "source": "Chapter2_AlgoFoundations3_GA.pdf",
    "page": 23
  },
  {
    "text": "Hamiltonian Cycles Some solutions in 2D : In 3D : Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 24 / 27",
    "source": "Chapter2_AlgoFoundations3_GA.pdf",
    "page": 24
  },
  {
    "text": "Hamiltonian Cycles Sufficient conditions Theorem (Dirac [1952]) If G is a simple graph with n ≥ 3 vertices and ∀v ∈ V (G), deg(v) ≥ n/2, then G is Hamiltonian. Proof The proof uses contradiction and extremality. If there is a non-Hamiltonian graph satisfying the hypotheses, then adding edges cannot reduce the minimum degree. Thus we may restrict our attention to maximal non-Hamiltonian graphs with minimum degree at least n/2, where “maximal” means that adding any edge joining nonadjacent vertices creates a spanning cycle. When uv ̸∈ E(G), the maximality of G implies that G + uv contains a Hamiltonian cycle and that cycle contains the edge uv. This implies that G has a spanning path v1, ··· , vn from u = v1 to v = vn. To prove the theorem, it suffices to make a small change in this cycle to avoid using the edge uv ; this will build a spanning cycle in G. Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 25 / 27",
    "source": "Chapter2_AlgoFoundations3_GA.pdf",
    "page": 25
  },
  {
    "text": "Hamiltonian Cycles Proof (Cont.) If a neighbor of u directly follows a neighbor of v on the path, such as uvi+1 ∈ E(G) and vvi ∈ E(G), then ( u, vi+1, vi+2, ··· , v, vi , vi−1, ··· , u) is a spanning cycle in G. To prove that such a cycle exists, we show that there is a common index in the sets S and T defined by S = {i : uvi+1 ∈ E(G), 1 ≤ i ≤ n − 2} and T = {i : vvi ∈ E(G), 2 ≤ i ≤ n − 1}. Summing the sizes of these sets yields |S ∪ T | + |S ∩ T | = |S| + |T | = deg(u) + deg(v) ≥ n/2 + n/2 = n. We have S ∪ T ⊂ J1, n − 1K, then |S ∪ T | ≤n − 1, hence |S ∩ T | ≥1. This means that S ∩ T ̸= ∅. So, we have established a contradiction by finding a spanning cycle in G ; hence there is no (maximal) non-Hamiltonian graph satisfying the hypotheses. Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 26 / 27",
    "source": "Chapter2_AlgoFoundations3_GA.pdf",
    "page": 26
  },
  {
    "text": "Hamiltonian Cycles Remark Ore observed that the argument uses ∀v ∈ V (G), deg(v) ≥ n/2 only to show that deg( u) + deg(v) ≥ n. Therefore, we can weaken the requirement of minimum degree n/2 to require only that deg(u) + deg(v) ≥ n whenever uv ̸∈ E(G). Theorem (Ore [1960]) If G is a simple graph with n ≥ 3 vertices and deg(u) + deg(v) ≥ n for every pair of distinct non-adjacent vertices u and v of G, then G is Hamiltonian. Proof : The same as the above proof of Dirac’s theorem using the above remark. Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 27 / 27",
    "source": "Chapter2_AlgoFoundations3_GA.pdf",
    "page": 27
  },
  {
    "text": "Chapter 3 : Directed graphs, and Coloring Omar Saadi omar.saadi@um6p.ma UM6P - College of Computing 1st year of the engineering degree of UM6P-CS Academic year 2023-2024 Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 1 / 16",
    "source": "Chapter3_AlgoFoundations3.pdf",
    "page": 1
  },
  {
    "text": "Outline 1 Directed graphs 2 Coloring of graphs More details on this chapter can be found in Sect 1.4 and Sect. 5.1 of D. B. West’s book. Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 2 / 16",
    "source": "Chapter3_AlgoFoundations3.pdf",
    "page": 2
  },
  {
    "text": "Directed graphs Outline 1 Directed graphs 2 Coloring of graphs Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 3 / 16",
    "source": "Chapter3_AlgoFoundations3.pdf",
    "page": 3
  },
  {
    "text": "Directed graphs Definition A directed graph or digraph G is a triple consisting of a vertex set V (G), an edge set E(G), and a function assigning each edge an ordered pair of vertices. The first vertex of the ordered pair is the tail of the edge, and the second is the head ; together, they are the endpoints. We say that an edge is an edge from its tail to its head. Definition In a digraph, a loop is an edge whose endpoints are equal. Multiple edges are edges having the same ordered pair of endpoints. A digraph is simple if each ordered pair is the head and tail of at most one edge ; one loop may be present at each vertex. In a simple digraph, we write uv for an edge with tail u and head v. If there is an edge from u to v, then v is a successor of u, and u is a predecessor of v. We write u → v for “there is an edge from u to v”. Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 4 / 16",
    "source": "Chapter3_AlgoFoundations3.pdf",
    "page": 4
  },
  {
    "text": "Directed graphs Example : Digraph of a Markov chain Consider a system that can be in n possible states s1, s2, ··· , sn. After being in the state si , the system will go to state sj with probability Pij . Note that for each state si , we have P j Pij = 1. The digraph G representing this Markov chain is such that : V (G) = {s1, s2, ··· , sn} si sj ∈ E(G) iff Pij > 0, in this case the edge si sj has a weight Pij . Definition : We say that a graph (or digraph) G is weighted if we associate to each edge a value, called a weight. Example : Suppose that weather has two states : good (G) and bad (B). Air masses move slowly enough that tomorrow’s weather tends to be like today’s. In most places, storms don’t linger long, so we might have transition probabilities as follows : Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 5 / 16",
    "source": "Chapter3_AlgoFoundations3.pdf",
    "page": 5
  },
  {
    "text": "Directed graphs Definition A digraph is a path if it is a simple digraph whose vertices can be linearly ordered so that there is an edge with tail u and head v if and only if v immediately follows u in the vertex ordering. A cycle is defined similarly using an ordering of the vertices on a circle. Definition The underlying graph of a digraph D is the graph G obtained by treating the edges of D as unordered pairs ; the vertex set and edge set remain the same, and the endpoints of an edge are the same in G as in D, but in G they become an unordered pair. Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 6 / 16",
    "source": "Chapter3_AlgoFoundations3.pdf",
    "page": 6
  },
  {
    "text": "Directed graphs Definition The definitions of subgraph, isomorphism and decomposition are the same for graphs and digraphs. In the adjacency matrix A(G) of a digraph G, the entry in position i, j is the number of edges from vi to vj . In the incidence matrix M(G) of a loopless digraph G, we set mij = 1 if vi is the tail of ej and mij = −1 if vi is the head of ej . Example : Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 7 / 16",
    "source": "Chapter3_AlgoFoundations3.pdf",
    "page": 7
  },
  {
    "text": "Directed graphs Definition A digraph is weakly connected if its underlying graph is connected. A digraph is strongly connected or strong if for each ordered pair u, v of vertices, there is a path from u to v. The strong components of a digraph are its maximal strong subgraphs. Examples : As a digraph, an n-vertex path has n strong components, but a cycle has only one strong component. In the diagraph below, the three circled subdigraphs are the strong components : Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 8 / 16",
    "source": "Chapter3_AlgoFoundations3.pdf",
    "page": 8
  },
  {
    "text": "Directed graphs Vertex degrees Definition Let v be a vertex in a digraph. The outdegree d+(v) is the number of edges with tail v. The indegree d−(v) is the number of edges with head v. The out-neighborhood or successor set N+(v) is {x ∈ V (G) : v → x}. The in-neighborhood or predecessor set N−(v) is {x ∈ V (G) : x → v}. The minimum and maximum indegree are δ−(G) and ∆ −(G) ; for outdegree we use δ+(G) and ∆ +(G). Proposition In a digraph G, P v∈V (G) d+(v) = |E(G)| = P v∈V (G) d−(v). Proof : Every edge has exactly one tail and exactly one head. Remark : In a graph, the minimum and maximum degree are denoted δ(G) and ∆( G), and we have P v∈V (G) d(v) = 2|E(G)|. ∆(G) is also called the degree of the graph. Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 9 / 16",
    "source": "Chapter3_AlgoFoundations3.pdf",
    "page": 9
  },
  {
    "text": "Directed graphs Eulerian digraphs The definitions of trail, walk, circuit, and the connection relation are the same in graphs and digraphs when we list edges as ordered pairs of vertices. Definition An Eulerian trail in a digraph (or graph) is a trail containing all edges. An Eulerian circuit is a closed trail containing all edges. A digraph is Eulerian if it has an Eulerian circuit. Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 10 / 16",
    "source": "Chapter3_AlgoFoundations3.pdf",
    "page": 10
  },
  {
    "text": "Directed graphs Characterization of an Eulerian digraph Lemma If G is a digraph withδ+(G) ≥ 1, then G contains a cycle. The same conclusion holds whenδ−(G) ≥ 1. Proof : Consider a maximal path in G. Theorem A digraph is Eulerian if and only ifd+(v) = d−(v) for each vertexv and the underlying graph has at most one nontrivial component. Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 11 / 16",
    "source": "Chapter3_AlgoFoundations3.pdf",
    "page": 11
  },
  {
    "text": "Coloring of graphs Outline 1 Directed graphs 2 Coloring of graphs Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 12 / 16",
    "source": "Chapter3_AlgoFoundations3.pdf",
    "page": 12
  },
  {
    "text": "Coloring of graphs Definition A k-coloring of a graph G is a labeling f : V (G) → S, where |S| = k. The labels are colors; the vertices of one color form a color class. A k-coloring is proper if adjacent vertices have different labels. A graph is k-colorable if it has a proper k-coloring. The chromatic number χ(G) is the least k such that G is k-colorable. If χ(G) = k, we say that G is k-chromatic. Remark In a proper coloring, each color class is an independent set, so G is k-colorable if and only if V (G) is the union of k independent sets. Thus “k-colorable” and “ k-partite” have the same meaning. Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 13 / 16",
    "source": "Chapter3_AlgoFoundations3.pdf",
    "page": 13
  },
  {
    "text": "Coloring of graphs Example Since a graph is 2-colorable if and only if it is bipartite, and C5 is not bipartite (because it contains an odd cycle), then C5 has a chromatic number at least 3. Since C5 is 3-colorable, as shown below, it has a chromatic number exactly 3. Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 14 / 16",
    "source": "Chapter3_AlgoFoundations3.pdf",
    "page": 14
  },
  {
    "text": "Coloring of graphs Lower bounds onχ(G) Recall : The independence number of a graph G, written α(G), is the biggest size of an independent subset of V (G). Definition The clique number of a graph G, written ω(G), is the maximum size of a clique in G. Proposition For every graph G with n vertices, we have χ(G) ≥ ω(G) and χ(G) ≥ n α(G). Proof : Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 15 / 16",
    "source": "Chapter3_AlgoFoundations3.pdf",
    "page": 15
  },
  {
    "text": "Coloring of graphs An upper bound onχ(G) Algorithm : (Greedy coloring) The greedy coloring relative to a vertex ordering {v1, ··· , vn} of V (G) by the color set S is obtained by coloring vertices in the order v1, ··· , vn, assigning to vi the color with the smallest index in S that was not already used to color the neighbors of vi that were already colored. Proposition For every graph G, we have χ(G) ≤ ∆(G) + 1. Recall that ∆( G) is the degree of the graph G (i.e. the largest degree of its vertices). Proof : The greedy coloring doesn’t use more that ∆( G) + 1 colors. Remark : Note that the bound ∆( G) + 1 is optimal in the case of complete graphs and cycles. Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 16 / 16",
    "source": "Chapter3_AlgoFoundations3.pdf",
    "page": 16
  },
  {
    "text": "Chapter 4 : Trees Omar Saadi omar.saadi@um6p.ma UM6P - College of Computing 1st year of the engineering degree of UM6P-CS Academic year 2024-2025 Omar Saadi (College of Computing) Algorithmic Foundations 3 (GA) Computer Science School 1 / 26",
    "source": "Chapter4_AlgoFoundations_GA.pdf",
    "page": 1
  },
  {
    "text": "Outline 1 Basic properties 2 Distance in trees and graphs 3 Binary trees 4 Huffman coding More details on this chapter can be found in Sect. 2.1 and (last part of) Sect 2.3 of D. B. West’s book. Omar Saadi (College of Computing) Algorithmic Foundations 3 (GA) Computer Science School 2 / 26",
    "source": "Chapter4_AlgoFoundations_GA.pdf",
    "page": 2
  },
  {
    "text": "Basic properties Outline 1 Basic properties 2 Distance in trees and graphs 3 Binary trees 4 Huffman coding Omar Saadi (College of Computing) Algorithmic Foundations 3 (GA) Computer Science School 3 / 26",
    "source": "Chapter4_AlgoFoundations_GA.pdf",
    "page": 3
  },
  {
    "text": "Basic properties Definition We say that a graph is acyclic if it doesn’t contain any cycle. A forest is an acyclic graph. A tree is a connected acyclic graph. A leaf (or pendant vertex) is a vertex of degree 1. Definition A spanning subgraph of G is a subgraph with vertex set V (G). A spanning tree is a spanning subgraph that is a tree. Omar Saadi (College of Computing) Algorithmic Foundations 3 (GA) Computer Science School 4 / 26",
    "source": "Chapter4_AlgoFoundations_GA.pdf",
    "page": 4
  },
  {
    "text": "Basic properties Examples The following graph is a forest (no cycles). The following two graphs are trees (connected and no cycles). Their union is a forest. A tree is a connected forest, and every component of a forest is a tree. A graph with no cycles has no odd cycles ; hence trees and forests are bipartite. Omar Saadi (College of Computing) Algorithmic Foundations 3 (GA) Computer Science School 5 / 26",
    "source": "Chapter4_AlgoFoundations_GA.pdf",
    "page": 5
  },
  {
    "text": "Basic properties Examples (Cont.) Paths are trees. A tree is a path if and only if its maximum degree is 2. A graph that is a tree has exactly one spanning tree ; the full graph itself. Remark A spanning subgraph of G doesn’t need to be connected, and a connected subgraph of G doesn’t need to be a spanning subgraph. For example : If n(G) ≥ 2, then the subgraph with vertex set V (G) and edge set ∅ is spanning but not connected. If n(G) ≥ 3, then a subgraph consisting of one edge and its endpoints is connected but not necessarily spanning. Omar Saadi (College of Computing) Algorithmic Foundations 3 (GA) Computer Science School 6 / 26",
    "source": "Chapter4_AlgoFoundations_GA.pdf",
    "page": 6
  },
  {
    "text": "Basic properties Lemma Every tree with at least two vertices has at least two leaves. Deleting a leaf from an n-vertex tree produces a tree with n − 1 vertices. Proof : Consider the endpoints of a maximal path and use the connectivity and the acyclicity properties of the tree. Remark This lemma implies that every tree with more than one vertex arises from a smaller tree by adding a vertex of degree 1. This remark is useful for induction reasoning when dealing with trees : growing an n + 1-vertex tree from an arbitrary n-vertex tree by adding a new neighbor at an arbitrary old vertex generates all trees with n + 1 vertices. Omar Saadi (College of Computing) Algorithmic Foundations 3 (GA) Computer Science School 7 / 26",
    "source": "Chapter4_AlgoFoundations_GA.pdf",
    "page": 7
  },
  {
    "text": "Basic properties Characterizations of trees Theorem For an n-vertex graph G (with n ≥ 1), the following assertions are equivalent (and characterize the trees with n vertices) : 1 G is connected and has no cycles. 2 G is connected and has n - 1 edges. 3 G has n − 1 edges and no cycles. 4 For all u, v ∈ V (G), G has exactly one u, v-path. Proof : First prove the equivalence of 1 , 2, and 3 by proving that any two properties among the set {connected, acyclic, n − 1 edges} imply the third one. Then prove that 1 and 4 are equivalent. Omar Saadi (College of Computing) Algorithmic Foundations 3 (GA) Computer Science School 8 / 26",
    "source": "Chapter4_AlgoFoundations_GA.pdf",
    "page": 8
  },
  {
    "text": "Basic properties Corollary 1 Every edge of a tree is a cut-edge. 2 Adding one edge to a tree forms exactly one cycle. 3 Every connected graph contains a spanning tree. Proof : Omar Saadi (College of Computing) Algorithmic Foundations 3 (GA) Computer Science School 9 / 26",
    "source": "Chapter4_AlgoFoundations_GA.pdf",
    "page": 9
  },
  {
    "text": "Basic properties Proposition If T , T ′ are spanning trees of a connected graph G and e ∈ E(T ) − E(T ′), then there is an edge e′ ∈ E(T ′) − E(T ) such that T − e + e′ is a spanning tree of G. Their is also an edge e′′ ∈ E(T ′) − E(T ) such that T ′ + e − e′′ is a spanning tree of G. Proof : Consider the two components of T − e for the first result, and consider the unique cycle obtained by adding e to T ′ for the second result. Omar Saadi (College of Computing) Algorithmic Foundations 3 (GA) Computer Science School 10 / 26",
    "source": "Chapter4_AlgoFoundations_GA.pdf",
    "page": 10
  },
  {
    "text": "Distance in trees and graphs Outline 1 Basic properties 2 Distance in trees and graphs 3 Binary trees 4 Huffman coding Omar Saadi (College of Computing) Algorithmic Foundations 3 (GA) Computer Science School 11 / 26",
    "source": "Chapter4_AlgoFoundations_GA.pdf",
    "page": 11
  },
  {
    "text": "Distance in trees and graphs Definition If G has a u, v-path, then the distance from u to v, written dG (u, v) or simply d(u, v), is the least length of a u, v-path. If G has no such path, then d(u, v) = ∞. The diameter (diam(G)) is max u,v∈V (G) d(u, v). The eccentricity of a vertex u, written ϵ(u), is max v∈V (G) d(u, v). The radius of a graph G, written rad( G), is min u∈V (G) ϵ(u). Remarks The diameter equals the maximum of the vertex eccentricities. In a disconnected graph, the diameter and radius (and every eccentricity) are infinite, because distance between vertices in different components is infinite. We use the word “diameter” due to its use in geometry, where it is the greatest distance between two elements of a set. Omar Saadi (College of Computing) Algorithmic Foundations 3 (GA) Computer Science School 12 / 26",
    "source": "Chapter4_AlgoFoundations_GA.pdf",
    "page": 12
  },
  {
    "text": "Distance in trees and graphs Examples The Petersen graph has diameter 2, since nonadjacent vertices have a common neighbor. The cycle Cn has diameter ⌊n/2⌋. In these two examples, every vertex has the same eccentricity, and diam(G) = rad(G). Every path in a tree is the shortest (the only) path between its endpoints, so the diameter of a tree is the length of its longest path. In the graph below, each vertex is labeled with its eccentricity. The radius is 2, the diameter is 4, and the length of the longest path is 7. Omar Saadi (College of Computing) Algorithmic Foundations 3 (GA) Computer Science School 13 / 26",
    "source": "Chapter4_AlgoFoundations_GA.pdf",
    "page": 13
  },
  {
    "text": "Distance in trees and graphs Proposition If G is a simple graph, then diam( G) ≥ 3 ⇒ diam(G) ≤ 3. Proof : Use the fact that if diam( G) ≥ 3 then there exist nonadjacent vertices u, v ∈ V (G) with no common neighbor. Omar Saadi (College of Computing) Algorithmic Foundations 3 (GA) Computer Science School 14 / 26",
    "source": "Chapter4_AlgoFoundations_GA.pdf",
    "page": 14
  },
  {
    "text": "Distance in trees and graphs Definition The center of a graph G is the subgraph induced by the vertices of minimum eccentricity. Remark : The center of a graph is the full graph if and only if the radius and diameter are equal. Theorem (Jordan [1869]) The center of a tree is a vertex or an edge. Proof : We use induction on the number of vertices in a tree T , where we obtain a smaller tree T ′ by deleting every leaf of T . Use the fact that ∀u ∈ V (T ′), ϵT′ (u) = ϵT (u) − 1. Omar Saadi (College of Computing) Algorithmic Foundations 3 (GA) Computer Science School 15 / 26",
    "source": "Chapter4_AlgoFoundations_GA.pdf",
    "page": 15
  },
  {
    "text": "Binary trees Outline 1 Basic properties 2 Distance in trees and graphs 3 Binary trees 4 Huffman coding Omar Saadi (College of Computing) Algorithmic Foundations 3 (GA) Computer Science School 16 / 26",
    "source": "Chapter4_AlgoFoundations_GA.pdf",
    "page": 16
  },
  {
    "text": "Binary trees Definition A rooted tree is a tree with one vertex r chosen as root. For each vertex v, let P(v) be the unique v, r-path. The parent of v is its neighbor on P(v) ; its children are its other neighbors. Its ancestors are the vertices of P(v) − v. Its descendants are the vertices u such that P(u) contains v. The leaves are the vertices with no children. A rooted plane tree or planted tree is a rooted tree with a left-to-right ordering specified for the children of each vertex. Example : Omar Saadi (College of Computing) Algorithmic Foundations 3 (GA) Computer Science School 17 / 26",
    "source": "Chapter4_AlgoFoundations_GA.pdf",
    "page": 17
  },
  {
    "text": "Binary trees Definition A binary tree is a rooted plane tree where each vertex has at most two children, and each child of a vertex is designated as its left child or right child. The subtrees rooted at the children of the root are the left subtree and the right subtree of the tree. A k-ary tree allows each vertex up to k children. Remark Binary trees permit storage of data for quick access. We store each item at a leaf and access it by following the path from the root. We encode the path by recording 0 when we move to a left child and 1 when we move to a right child. For each leaf, the search time is the length of its code word. Omar Saadi (College of Computing) Algorithmic Foundations 3 (GA) Computer Science School 18 / 26",
    "source": "Chapter4_AlgoFoundations_GA.pdf",
    "page": 18
  },
  {
    "text": "Huffman coding Outline 1 Basic properties 2 Distance in trees and graphs 3 Binary trees 4 Huffman coding Omar Saadi (College of Computing) Algorithmic Foundations 3 (GA) Computer Science School 19 / 26",
    "source": "Chapter4_AlgoFoundations_GA.pdf",
    "page": 19
  },
  {
    "text": "Huffman coding In a text file, each character is stored as a binary list. The classical way to encode characters is to use the ASCII code. It encodes each character using 7 bits. This allows to encode 128 characters that appear in text files. The ASCII table is the following : Omar Saadi (College of Computing) Algorithmic Foundations 3 (GA) Computer Science School 20 / 26",
    "source": "Chapter4_AlgoFoundations_GA.pdf",
    "page": 20
  },
  {
    "text": "Huffman coding Prefix-free coding Objective : Given a text file, we want to encode each character appearing in that text using a binary list with the goal of minimizing the total number of bits needed to store this text file. The length of code words may vary ; so we need a way to recognize the end of the current word. Idea : If no code word is an initial portion of another, we can easily recover our characters. We can do this by going through the list of bits (from left to right), and at each time that a sublist of bits correspond to a code of a character we make the transformation “bits to character”, and start again for the remaining bits. Definition : We call such coding a prefix-free coding. Example : If we have only three characters a, b, c with the following prefix-free coding a ↔ 1, b ↔ 01, c ↔ 00, then we can get the following encodings : aabacbc ↔ 11011000100 and ccbaac ↔ 0000011100. Omar Saadi (College of Computing) Algorithmic Foundations 3 (GA) Computer Science School 21 / 26",
    "source": "Chapter4_AlgoFoundations_GA.pdf",
    "page": 21
  },
  {
    "text": "Huffman coding Remark Under the prefix-free condition, the binary code words correspond to the leaves of a binary tree using the left/right encoding described above. Input : The text file uses n characters c1, ··· , cn with probabilities of occurrence p1, ··· , pn respectively. Notice that pi is the number of occurrences of ci divided by the total number N of characters in the text. Goal : Construct the binary tree such that the bit lengths l1, ··· , ln assigned to the leaves c1, ··· , cn are such that : The expected length of a code word X i∈[n] pi li is minimal. Notice that we are minimizing indeed the total length of the encoding N × P i∈[n] pi li . Omar Saadi (College of Computing) Algorithmic Foundations 3 (GA) Computer Science School 22 / 26",
    "source": "Chapter4_AlgoFoundations_GA.pdf",
    "page": 22
  },
  {
    "text": "Huffman coding Huffman’s Algorithm Input : Weights (frequencies or probabilities) p1, ··· , pn. Output : Prefix-free code (equivalently, a binary tree). Idea : Infrequent items should have longer codes ; put infrequent items deeper by combining them into parent vertices. Initial case : When n = 2, the optimal length is one, with 0 and 1 being the codes assigned to the two items (the tree has a root and two leaves). Recursion : When n > 2, replace the two items with the smallest probabilities p, p′ with a single item q of weight p + p′. Treat the smaller set as a problem with n − 1 items. After solving it, give children with weights p, p′ to the resulting leaf with weight q. Equivalently, replace the code computed for the combined item q with its extensions by 1 and 0, assigned to the items that were replaced. Notice that each vertex that is not a leaf has exactly two children. Omar Saadi (College of Computing) Algorithmic Foundations 3 (GA) Computer Science School 23 / 26",
    "source": "Chapter4_AlgoFoundations_GA.pdf",
    "page": 23
  },
  {
    "text": "Huffman coding Example Consider eight items with frequencies 5 , 1, 1, 7, 8, 2, 3, 6. Huffman’s algorithm will output the tree below in the left, working from the bottom up. The values of the vertices are the (combined) frequencies. We obtain code words as shown in the tree below in the right. In their original order, the items have code words 100, 00000, 00001, 01, 11, 0001, 001, and 101. The expected length of a code word is L = P i pi li = 90/33. Omar Saadi (College of Computing) Algorithmic Foundations 3 (GA) Computer Science School 24 / 26",
    "source": "Chapter4_AlgoFoundations_GA.pdf",
    "page": 24
  },
  {
    "text": "Huffman coding Optimality of Huffman coding Theorem Given a probability distribution {p1, ··· , pn} on n items, Huffman’s algorithm produces the prefix-free code with minimum expected length. Proof We use induction on n. Basis step : n = 2. The algorithm encodes each item as a single bit, which is optimal. Induction step : n > 2. Suppose that the algorithm computes the optimal code when given a distribution for n − 1 items. With n items, an optimal binary tree T must assign the items with probabilities p1 ≥ ··· ≥pn to leaves in increasing order of depth, because otherwise we can exchange two items and get a strictly better encoding. Therefore the two items with the smallest probabilities are assigned to leaves of greatest depth. Since permuting items at a given depth doesn’t change the expected length, we may assume that items n and n −1 appear as siblings at greatest depth in T . Omar Saadi (College of Computing) Algorithmic Foundations 3 (GA) Computer Science School 25 / 26",
    "source": "Chapter4_AlgoFoundations_GA.pdf",
    "page": 25
  },
  {
    "text": "Huffman coding Proof (Cont.) Let T ′ be the tree obtained from T by deleting these leaves, and let {q1, ··· , qn−1} be the probability distribution obtained by replacing {pn−1, pn} by qn−1 = pn−1 + pn, and ∀i ∈ [n − 2], qi = pi . The tree T ′ yields a code for {qi }. Notice that, if k is the depth of the leaf assigned of weight qn−1 in T ′, we have length(T ) = X i∈[n] pi li = X i∈[n−2] pi li + (k + 1)(pn−1 + pn) = P i∈[n−1] qi li + qn−1 = length( T ′) + qn−1 . So it suffices that T ′ is optimal to get that T is optimal because they have the same length up to the fixed constant qn−1. By the induction hypothesis, an optimal choice for T ′ is obtained by applying Huffman’s algorithm to {qi }. Since the replacement of {pn−1, pn} by qn−1 is exactly the first step of Huffman’s Algorithm for {pi }, we conclude that Huffman’s algorithm generates the optimal tree T for {pi }. Omar Saadi (College of Computing) Algorithmic Foundations 3 (GA) Computer Science School 26 / 26",
    "source": "Chapter4_AlgoFoundations_GA.pdf",
    "page": 26
  },
  {
    "text": "Chapter 5 : Minimum Spanning Tree Omar Saadi omar.saadi@um6p.ma UM6P - College of Computing 1st year of the engineering degree of UM6P-CS Academic year 2024-2025 Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 1 / 16",
    "source": "Chapter5_AlgoFoundations_GA.pdf",
    "page": 1
  },
  {
    "text": "Outline 1 Introduction 2 Kruskal’s Algorithm 3 Prim’s Algorithm Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 2 / 16",
    "source": "Chapter5_AlgoFoundations_GA.pdf",
    "page": 2
  },
  {
    "text": "Introduction Outline 1 Introduction 2 Kruskal’s Algorithm 3 Prim’s Algorithm Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 3 / 16",
    "source": "Chapter5_AlgoFoundations_GA.pdf",
    "page": 3
  },
  {
    "text": "Introduction Introduction Definition A weighted graphis a graph with numerical labels on the edges. For a weighted connected graph, a minimal spanning treeis a spanning tree that has the smallest (total) weight possible. Example : Consider a graph such that : Each vertex is a city When two cities can be connected by a road, then there is an edge between them in the graph, and its weight is the cost of constructing that road. Here, connecting all the cities with the smallest total construction cost is exaclty the minimum spanning tree problem. In the following the weights are allowed to be positive or negative. Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 4 / 16",
    "source": "Chapter5_AlgoFoundations_GA.pdf",
    "page": 4
  },
  {
    "text": "Kruskal’s Algorithm Outline 1 Introduction 2 Kruskal’s Algorithm 3 Prim’s Algorithm Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 5 / 16",
    "source": "Chapter5_AlgoFoundations_GA.pdf",
    "page": 5
  },
  {
    "text": "Kruskal’s Algorithm Kruskal’s Algorithm Input : A weighted connected graph. Idea : Maintain an acyclic spanning subgraph H, enlarging it by edges with low weight to form a spanning tree. Initialization :Set V (H) = V (G) and E(H) = ∅. Iteration :If the next cheapest edge joins two components of H, then include it in H ; otherwise, discard it. Terminate when H is connected. Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 6 / 16",
    "source": "Chapter5_AlgoFoundations_GA.pdf",
    "page": 6
  },
  {
    "text": "Kruskal’s Algorithm Example The following example shows a weighted connected graph with the spanning tree given by Kruskal’s algorithm. The edges are add the following order : 1 , 2, 3, 4, 7, 10. Remark Unsophisticated locally optimal heuristics are called greedy algorithms. They usually don’t guarantee optimal solutions, but Kruskal’s algorithm is an example of a greedy algorithm that gives an optimal solution (see next theorem). Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 7 / 16",
    "source": "Chapter5_AlgoFoundations_GA.pdf",
    "page": 7
  },
  {
    "text": "Kruskal’s Algorithm Example The following example shows a weighted connected graph with the spanning tree given by Kruskal’s algorithm. The edges are add the following order : 1 , 2, 3, 4, 7, 10. Remark Unsophisticated locally optimal heuristics are called greedy algorithms. They usually don’t guarantee optimal solutions, but Kruskal’s algorithm is an example of a greedy algorithm that gives an optimal solution (see next theorem). Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 7 / 16",
    "source": "Chapter5_AlgoFoundations_GA.pdf",
    "page": 8
  },
  {
    "text": "Kruskal’s Algorithm Theorem (Kruskal [1956]) In a connected weighted graphG, Kruskal’s algorithm constructs a minimum-weight spanning tree. Proof First we show that the algorithm produces a tree. Indeed, at each iteration, while H is not yet connected, there is at least an edge that connects two different components of H (because G is connected), so at each iteration an edge is added to H. It becomes connected exactely after adding n − 1 edges, and so it becomes a tree. Let T be the resulting tree, and let T′ be a spanning tree of minimum weight. If T = T′, we are done. If T ̸= T′, let e be the first edge chosen for T that is not in T′. Adding e to T′ creates one cycle C. Since T has no cycle, C has an edge e′ ̸∈ E(T). Consider the spanning tree T′′ = T′ + e − e′. Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 8 / 16",
    "source": "Chapter5_AlgoFoundations_GA.pdf",
    "page": 9
  },
  {
    "text": "Kruskal’s Algorithm Proof (Cont.) Since T′ contains e′ and all the edges of T chosen before e, then e′ could be added to T when the algorithm choosed e (without creating a cycle). So both e′ and e were available when the algorithm choosed e, and hence w(e) ≤ w(e′). Thus T′′ = T′ + e − e′ is a spanning tree with weight that doesn’t exceed the weight of T′ and that agrees with T for a longer initial list of edges than T′ does. In particular T′′ is also a minimum spanning tree. Repeating this argument yields a minimum-weight spanning tree that agrees completely with T. Therefore T is a minimum spanning tree. Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 9 / 16",
    "source": "Chapter5_AlgoFoundations_GA.pdf",
    "page": 10
  },
  {
    "text": "Kruskal’s Algorithm Let n = |V (G)| and m = |E(G)|. Implementation steps : First sort the m edge weights. Maintain for each vertex the label of the component containing it. Add the next cheapest edge if its endpoints have different labels, and merge the two components by changing the label of each vertex in the smaller component to the label of the larger one. Complexity : Since the size of the component at least doubles when a label changes (we are changing the smaller one), each label changes at most log2(n) times, and the total number of changes is at most n log2(n). With this labeling method, the running time for large graphs depends on the time to sort m weights of the edges, which is O(m log(m)). Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 10 / 16",
    "source": "Chapter5_AlgoFoundations_GA.pdf",
    "page": 11
  },
  {
    "text": "Prim’s Algorithm Outline 1 Introduction 2 Kruskal’s Algorithm 3 Prim’s Algorithm Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 11 / 16",
    "source": "Chapter5_AlgoFoundations_GA.pdf",
    "page": 12
  },
  {
    "text": "Prim’s Algorithm Input : A weighted connected graph. Idea : Growing a subgraph H from one isolated vertex by iteratively adding the cheapest edge from a vertex already reached to a vertex not yet reached to form a spanning tree. Initialization :V (H) = {v} and E(H) = ∅ for a given starting vertex v. Iteration :Add to H the cheapest edge (with its other endpoint) from a vertex already in H to a vertex outside H. Terminate when H is spanning. Example : In the above example, if we start from the vertex in the bottom left, we obtain the same spanning tree in the following order of edges : 10 , 2, 4, 3, 7, 1. Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 12 / 16",
    "source": "Chapter5_AlgoFoundations_GA.pdf",
    "page": 13
  },
  {
    "text": "Prim’s Algorithm Input : A weighted connected graph. Idea : Growing a subgraph H from one isolated vertex by iteratively adding the cheapest edge from a vertex already reached to a vertex not yet reached to form a spanning tree. Initialization :V (H) = {v} and E(H) = ∅ for a given starting vertex v. Iteration :Add to H the cheapest edge (with its other endpoint) from a vertex already in H to a vertex outside H. Terminate when H is spanning. Example : In the above example, if we start from the vertex in the bottom left, we obtain the same spanning tree in the following order of edges : 10 , 2, 4, 3, 7, 1. Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 12 / 16",
    "source": "Chapter5_AlgoFoundations_GA.pdf",
    "page": 14
  },
  {
    "text": "Prim’s Algorithm Theorem (Jarnik [1930], Prim [1957]) In a connected weighted graphG, Prim’s algorithm constructs a minimum-weight spanning tree. Proof First we show that the algorithm produces a spanning tree. Indeed, at each iteration, we are keeping H connected, and we are adding one edge that does not create any cycle. So H is always a tree. After exactely n − 1 iterations, it becomes a spanning tree. Using the same idea of proof as Kruskal’s algorithm : Let T be the resulting tree from Prim’s algorithm, and let T′ be a spanning tree of minimum weight. If T = T′, we are done. If T ̸= T′, let e be the first edge chosen (at iteration i) for T that is not in T′. At iteration i − 1, we denote the set obtain by {v0, ··· , vi −1}. the edge e connects {v0, ··· , vi −1} to V (G) \\ {v0, ··· , vi −1}. Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 13 / 16",
    "source": "Chapter5_AlgoFoundations_GA.pdf",
    "page": 15
  },
  {
    "text": "Prim’s Algorithm Proof (Cont.) Now, adding e to T′ creates one cycle C. So, C − e is a path from a vertex in {v0, ··· , vi −1} to a vertex in V (G) \\ {v0, ··· , vi −1}. Therefore, C − e contains an edge e′ that connects {v0, ··· , vi −1} to V (G) \\ {v0, ··· , vi −1}. Then e′ could be added to T when the algorithm choosed e (at iteration i), and hence w(e) ≤ w(e′). Thus T′′ = T′ + e − e′ is a spanning tree with weight that doesn’t exceed the weight of T′ and that agrees with T for a longer initial list of edges than T′ does. In particular T′′ is also a minimum spanning tree. Repeating this argument yields a minimum-weight spanning tree that agrees completely with T. Therefore T is a minimum spanning tree. Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 14 / 16",
    "source": "Chapter5_AlgoFoundations_GA.pdf",
    "page": 16
  },
  {
    "text": "Prim’s Algorithm Implementation steps and complexity : Using the weights matrix w(u, v) giving the weight of the edge ( u, v) (with w(u, v) = +∞ if (u, v) ̸∈ E(H)). Idea : Construct and update a first list d[v], v ̸∈ H, with d[v] being the cheapest weight of an edge that connects v to some vertex u already in H. Also consider a second list p[v], v ̸∈ H, with p[v] = u being the neighbor in H to which v is connected with the cheapest weight d[v]. The steps and the corresponding complexity are as follows (next slide) : Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 15 / 16",
    "source": "Chapter5_AlgoFoundations_GA.pdf",
    "page": 17
  },
  {
    "text": "Prim’s Algorithm 1 Start with H = v0 (any initial vertex v0). [This takes O(1)] 2 For all v ̸= v0, initialize d[v] = w(v0, v), and p[v] = v0 if (v, v0) ∈ E(H) and p[v] = ∅ otherwise. [This takes O(n)] 3 While H is still not spanning do the following : Extract the cheapest weight in d corresponding to some vertex vi . Remove d[vi ] and p[vi ] from d and p. Add the vertex vi and the edge (p[vi ], vi ) to H. [this takes O(n)]. Update d and p : For all neighbours v of vi such that v ̸∈ H, do : If w(v, vi ) < d[v], then d[v] = w(v, vi ) and p[v] = vi . [this takes O(deg(vi ) = O(n)]. Complexity :Combining the above complexities, we get that the total time complexity is O(n2). Remark :This complexity can be improved to O(m log(n)) using other data structures to represent the graph (where m = |E(G)|). Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 16 / 16",
    "source": "Chapter5_AlgoFoundations_GA.pdf",
    "page": 18
  },
  {
    "text": "Chapter 6 : Breadth-First Search and Depth-First Search Omar Saadi omar.saadi@um6p.ma UM6P - College of Computing 1st year of the engineering degree of UM6P-CS Academic year 2024-2025 Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 1 / 24",
    "source": "Chapter6_AlgoFoundations_GA.pdf",
    "page": 1
  },
  {
    "text": "Outline 1 Breadth-First Search (BFS) 2 Depth-First Search (DFS) Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 2 / 24",
    "source": "Chapter6_AlgoFoundations_GA.pdf",
    "page": 2
  },
  {
    "text": "Breadth-First Search (BFS) Outline 1 Breadth-First Search (BFS) 2 Depth-First Search (DFS) Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 3 / 24",
    "source": "Chapter6_AlgoFoundations_GA.pdf",
    "page": 3
  },
  {
    "text": "Breadth-First Search (BFS) Introduction Given a graph G, there are several problems one wants to solve related to the connectivity of the graph. Those include the following : Given a pair of vertices ( u, v), is there a path in G from u to v ? Given a pair of vertices ( u, v), what is the distance d(u, v) and a shortest path from u to v ? Given a vertex s, find d(s, v) for all v ∈ V (G) and a shortest path tree* containing a shortest path from s to every v ∈ V . * : A shortest path tree is a tree of G where the unique path from s to any vertex v inside this tree is a shortest path from s to v in G. Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 4 / 24",
    "source": "Chapter6_AlgoFoundations_GA.pdf",
    "page": 4
  },
  {
    "text": "Breadth-First Search (BFS) Introduction Given a graph G, there are several problems one wants to solve related to the connectivity of the graph. Those include the following : Given a pair of vertices ( u, v), is there a path in G from u to v ? Given a pair of vertices ( u, v), what is the distance d(u, v) and a shortest path from u to v ? Given a vertex s, find d(s, v) for all v ∈ V (G) and a shortest path tree* containing a shortest path from s to every v ∈ V . * : A shortest path tree is a tree of G where the unique path from s to any vertex v inside this tree is a shortest path from s to v in G. Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 4 / 24",
    "source": "Chapter6_AlgoFoundations_GA.pdf",
    "page": 5
  },
  {
    "text": "Breadth-First Search (BFS) Introduction Given a graph G, there are several problems one wants to solve related to the connectivity of the graph. Those include the following : Given a pair of vertices ( u, v), is there a path in G from u to v ? Given a pair of vertices ( u, v), what is the distance d(u, v) and a shortest path from u to v ? Given a vertex s, find d(s, v) for all v ∈ V (G) and a shortest path tree* containing a shortest path from s to every v ∈ V . * : A shortest path tree is a tree of G where the unique path from s to any vertex v inside this tree is a shortest path from s to v in G. Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 4 / 24",
    "source": "Chapter6_AlgoFoundations_GA.pdf",
    "page": 6
  },
  {
    "text": "Breadth-First Search (BFS) Introduction Given a graph G, there are several problems one wants to solve related to the connectivity of the graph. Those include the following : Given a pair of vertices ( u, v), is there a path in G from u to v ? Given a pair of vertices ( u, v), what is the distance d(u, v) and a shortest path from u to v ? Given a vertex s, find d(s, v) for all v ∈ V (G) and a shortest path tree* containing a shortest path from s to every v ∈ V . * : A shortest path tree is a tree of G where the unique path from s to any vertex v inside this tree is a shortest path from s to v in G. Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 4 / 24",
    "source": "Chapter6_AlgoFoundations_GA.pdf",
    "page": 7
  },
  {
    "text": "Breadth-First Search (BFS) Shortest Paths Tree Question How to return a shortest path from the source vertex s to every vertex v in the graph ? Problem Many paths could have length Ω( |V |), so returning every path could require Ω(|V |2) time. Idea For every v ∈ V , store its parent P(v) in a shortest path from s to v. This set of parents has an Ω( |V |) size and comprises a shortest paths tree. It provides a reversed shortest paths back to s from every vertex v reachable from s Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 5 / 24",
    "source": "Chapter6_AlgoFoundations_GA.pdf",
    "page": 8
  },
  {
    "text": "Breadth-First Search (BFS) Shortest Paths Tree Question How to return a shortest path from the source vertex s to every vertex v in the graph ? Problem Many paths could have length Ω( |V |), so returning every path could require Ω(|V |2) time. Idea For every v ∈ V , store its parent P(v) in a shortest path from s to v. This set of parents has an Ω( |V |) size and comprises a shortest paths tree. It provides a reversed shortest paths back to s from every vertex v reachable from s Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 5 / 24",
    "source": "Chapter6_AlgoFoundations_GA.pdf",
    "page": 9
  },
  {
    "text": "Breadth-First Search (BFS) Shortest Paths Tree Question How to return a shortest path from the source vertex s to every vertex v in the graph ? Problem Many paths could have length Ω( |V |), so returning every path could require Ω(|V |2) time. Idea For every v ∈ V , store its parent P(v) in a shortest path from s to v. This set of parents has an Ω( |V |) size and comprises a shortest paths tree. It provides a reversed shortest paths back to s from every vertex v reachable from s Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 5 / 24",
    "source": "Chapter6_AlgoFoundations_GA.pdf",
    "page": 10
  },
  {
    "text": "Breadth-First Search (BFS) Breadth-First Search (BFS) We want to : Store d(s, v) and P(v) in Set data structures mapping vertices v to distance and parent respectively. (If no path from s to v, do not store v in P and set d(s, v) to ∞). Idea : Explore graph nodes in increasing order of distance from s. Goal :Compute level sets Li = {v | v ∈ V and d(s, v) = i} (i.e., all vertices at distance i from s). Claims : Every vertex v ∈ Li must be adjacent to some vertex u ∈ Li−1 (i.e., v ∈ Adj(u)). No vertex that is in Lj for some j < i, appears in Li . Invariant :d(s, v) and P(v) have been computed correctly for all v in any Lj for j < i. Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 6 / 24",
    "source": "Chapter6_AlgoFoundations_GA.pdf",
    "page": 11
  },
  {
    "text": "Breadth-First Search (BFS) Breadth-First Search (BFS) We want to : Store d(s, v) and P(v) in Set data structures mapping vertices v to distance and parent respectively. (If no path from s to v, do not store v in P and set d(s, v) to ∞). Idea : Explore graph nodes in increasing order of distance from s. Goal :Compute level sets Li = {v | v ∈ V and d(s, v) = i} (i.e., all vertices at distance i from s). Claims : Every vertex v ∈ Li must be adjacent to some vertex u ∈ Li−1 (i.e., v ∈ Adj(u)). No vertex that is in Lj for some j < i, appears in Li . Invariant :d(s, v) and P(v) have been computed correctly for all v in any Lj for j < i. Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 6 / 24",
    "source": "Chapter6_AlgoFoundations_GA.pdf",
    "page": 12
  },
  {
    "text": "Breadth-First Search (BFS) Breadth-First Search (BFS) We want to : Store d(s, v) and P(v) in Set data structures mapping vertices v to distance and parent respectively. (If no path from s to v, do not store v in P and set d(s, v) to ∞). Idea : Explore graph nodes in increasing order of distance from s. Goal :Compute level sets Li = {v | v ∈ V and d(s, v) = i} (i.e., all vertices at distance i from s). Claims : Every vertex v ∈ Li must be adjacent to some vertex u ∈ Li−1 (i.e., v ∈ Adj(u)). No vertex that is in Lj for some j < i, appears in Li . Invariant :d(s, v) and P(v) have been computed correctly for all v in any Lj for j < i. Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 6 / 24",
    "source": "Chapter6_AlgoFoundations_GA.pdf",
    "page": 13
  },
  {
    "text": "Breadth-First Search (BFS) Breadth-First Search (BFS) We want to : Store d(s, v) and P(v) in Set data structures mapping vertices v to distance and parent respectively. (If no path from s to v, do not store v in P and set d(s, v) to ∞). Idea : Explore graph nodes in increasing order of distance from s. Goal :Compute level sets Li = {v | v ∈ V and d(s, v) = i} (i.e., all vertices at distance i from s). Claims : Every vertex v ∈ Li must be adjacent to some vertex u ∈ Li−1 (i.e., v ∈ Adj(u)). No vertex that is in Lj for some j < i, appears in Li . Invariant :d(s, v) and P(v) have been computed correctly for all v in any Lj for j < i. Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 6 / 24",
    "source": "Chapter6_AlgoFoundations_GA.pdf",
    "page": 14
  },
  {
    "text": "Breadth-First Search (BFS) BFS Algorithm Base case (i = 0) : L0 = {s}, d(s, s) = 0, P(s) = None Inductive Step : To compute Li : For every vertex u ∈ Li−1 : For every vertex v ∈ Adj(u) that does not appear in any Lj for j < i, do : Add v to Li , set d(s, v) = i, and set P(v) = u. Repeatedly compute Li from Lj for j < i for increasing i until Li is the empty set. Set d(s, v) = ∞ for any v ∈ V for which d(s, v) was not set. Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 7 / 24",
    "source": "Chapter6_AlgoFoundations_GA.pdf",
    "page": 15
  },
  {
    "text": "Breadth-First Search (BFS) Remarks The principle of the above Breadth-First Search algorithm is : Start with the source vertex s Visit his neighbours Then visit the neighbours of its neighbours (that were not yet explored) and so on ··· until visiting all reachable vertices from s. Recall that the largest distance from a fixed vertex u to another vertex is the eccentricity ϵ(u). Hence BFS started at u allows to compute this eccentricity. So we can compute the diameter of a graph by running Breadth-First Search from each vertex. BFS can be applied in its same above format for directed graphs. In this case, the neighbors of u are taken in the directed sens. BFS can also be applied to the particular case of trees to do a tree traversal. Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 8 / 24",
    "source": "Chapter6_AlgoFoundations_GA.pdf",
    "page": 16
  },
  {
    "text": "Breadth-First Search (BFS) Remarks The principle of the above Breadth-First Search algorithm is : Start with the source vertex s Visit his neighbours Then visit the neighbours of its neighbours (that were not yet explored) and so on ··· until visiting all reachable vertices from s. Recall that the largest distance from a fixed vertex u to another vertex is the eccentricity ϵ(u). Hence BFS started at u allows to compute this eccentricity. So we can compute the diameter of a graph by running Breadth-First Search from each vertex. BFS can be applied in its same above format for directed graphs. In this case, the neighbors of u are taken in the directed sens. BFS can also be applied to the particular case of trees to do a tree traversal. Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 8 / 24",
    "source": "Chapter6_AlgoFoundations_GA.pdf",
    "page": 17
  },
  {
    "text": "Breadth-First Search (BFS) Remarks The principle of the above Breadth-First Search algorithm is : Start with the source vertex s Visit his neighbours Then visit the neighbours of its neighbours (that were not yet explored) and so on ··· until visiting all reachable vertices from s. Recall that the largest distance from a fixed vertex u to another vertex is the eccentricity ϵ(u). Hence BFS started at u allows to compute this eccentricity. So we can compute the diameter of a graph by running Breadth-First Search from each vertex. BFS can be applied in its same above format for directed graphs. In this case, the neighbors of u are taken in the directed sens. BFS can also be applied to the particular case of trees to do a tree traversal. Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 8 / 24",
    "source": "Chapter6_AlgoFoundations_GA.pdf",
    "page": 18
  },
  {
    "text": "Breadth-First Search (BFS) Remarks The principle of the above Breadth-First Search algorithm is : Start with the source vertex s Visit his neighbours Then visit the neighbours of its neighbours (that were not yet explored) and so on ··· until visiting all reachable vertices from s. Recall that the largest distance from a fixed vertex u to another vertex is the eccentricity ϵ(u). Hence BFS started at u allows to compute this eccentricity. So we can compute the diameter of a graph by running Breadth-First Search from each vertex. BFS can be applied in its same above format for directed graphs. In this case, the neighbors of u are taken in the directed sens. BFS can also be applied to the particular case of trees to do a tree traversal. Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 8 / 24",
    "source": "Chapter6_AlgoFoundations_GA.pdf",
    "page": 19
  },
  {
    "text": "Breadth-First Search (BFS) Example Performing BFS search on the following tree, starting from the vertex 1, leads to the next traversal of the tree : Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 9 / 24",
    "source": "Chapter6_AlgoFoundations_GA.pdf",
    "page": 20
  },
  {
    "text": "Breadth-First Search (BFS) Proposition Breadth-first search correctly computes all d(s, v) and P(v) for every v ∈ V . Proof : Straightforward by an induction over i. Proposition Breadth-first search runs in O(|V | + |E|) time (i.e. linear time). Proof : (Next slide) Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 10 / 24",
    "source": "Chapter6_AlgoFoundations_GA.pdf",
    "page": 21
  },
  {
    "text": "Breadth-First Search (BFS) Proposition Breadth-first search correctly computes all d(s, v) and P(v) for every v ∈ V . Proof : Straightforward by an induction over i. Proposition Breadth-first search runs in O(|V | + |E|) time (i.e. linear time). Proof : (Next slide) Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 10 / 24",
    "source": "Chapter6_AlgoFoundations_GA.pdf",
    "page": 22
  },
  {
    "text": "Breadth-First Search (BFS) Proof Algorithm adds each vertex u to at most one level Li and spends O(1) time for each v ∈ Adj(u). Time complexity upper bounded by O(1) × P u∈V deg(u) = O(|E|) Spend O(|V |) at end to assign d(s, v) for vertices v ∈ V not reachable from s ⇒ Breadth-first search runs in linear time O(|V | + |E|) Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 11 / 24",
    "source": "Chapter6_AlgoFoundations_GA.pdf",
    "page": 23
  },
  {
    "text": "Breadth-First Search (BFS) Proof Algorithm adds each vertex u to at most one level Li and spends O(1) time for each v ∈ Adj(u). Time complexity upper bounded by O(1) × P u∈V deg(u) = O(|E|) Spend O(|V |) at end to assign d(s, v) for vertices v ∈ V not reachable from s ⇒ Breadth-first search runs in linear time O(|V | + |E|) Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 11 / 24",
    "source": "Chapter6_AlgoFoundations_GA.pdf",
    "page": 24
  },
  {
    "text": "Breadth-First Search (BFS) Application : checking strongly connectivity Problem Given a directed graph, check if it is strongly connected or not. Solution 1 (a na ¨ ıve one) Perform BFS starting from every vertex in the graph. If each BFS call visits every other vertex in the graph, then the graph is strongly connected. Complexity :We are performing BFS |V | times, and each call of BFS need O(|V | + |E|), so the total complexity is O(|V |(|V | + |E|)). Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 12 / 24",
    "source": "Chapter6_AlgoFoundations_GA.pdf",
    "page": 25
  },
  {
    "text": "Breadth-First Search (BFS) Application : checking strongly connectivity Problem Given a directed graph, check if it is strongly connected or not. Solution 1 (a na ¨ ıve one) Perform BFS starting from every vertex in the graph. If each BFS call visits every other vertex in the graph, then the graph is strongly connected. Complexity :We are performing BFS |V | times, and each call of BFS need O(|V | + |E|), so the total complexity is O(|V |(|V | + |E|)). Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 12 / 24",
    "source": "Chapter6_AlgoFoundations_GA.pdf",
    "page": 26
  },
  {
    "text": "Breadth-First Search (BFS) Solution 2 Choose a vertex u, and perform BFS starting from u. Reverse the direction of all edges in the digraph G, and perform again BFS starting from the same vertex u. Proposition The graph is strongly connected if and only if both calls of BFS visit every vertex in the graph. Proof : There is a path from u to every other vertex v, and also a path from every other vertex v to u. Complexity :We are performing BFS twice, so the total complexity is O(|V | + |E|). Remark :this problem can be solved also by DFS (described below) instead of BFS. Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 13 / 24",
    "source": "Chapter6_AlgoFoundations_GA.pdf",
    "page": 27
  },
  {
    "text": "Breadth-First Search (BFS) Solution 2 Choose a vertex u, and perform BFS starting from u. Reverse the direction of all edges in the digraph G, and perform again BFS starting from the same vertex u. Proposition The graph is strongly connected if and only if both calls of BFS visit every vertex in the graph. Proof : There is a path from u to every other vertex v, and also a path from every other vertex v to u. Complexity :We are performing BFS twice, so the total complexity is O(|V | + |E|). Remark :this problem can be solved also by DFS (described below) instead of BFS. Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 13 / 24",
    "source": "Chapter6_AlgoFoundations_GA.pdf",
    "page": 28
  },
  {
    "text": "Breadth-First Search (BFS) Solution 2 Choose a vertex u, and perform BFS starting from u. Reverse the direction of all edges in the digraph G, and perform again BFS starting from the same vertex u. Proposition The graph is strongly connected if and only if both calls of BFS visit every vertex in the graph. Proof : There is a path from u to every other vertex v, and also a path from every other vertex v to u. Complexity :We are performing BFS twice, so the total complexity is O(|V | + |E|). Remark :this problem can be solved also by DFS (described below) instead of BFS. Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 13 / 24",
    "source": "Chapter6_AlgoFoundations_GA.pdf",
    "page": 29
  },
  {
    "text": "Depth-First Search (DFS) Outline 1 Breadth-First Search (BFS) 2 Depth-First Search (DFS) Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 14 / 24",
    "source": "Chapter6_AlgoFoundations_GA.pdf",
    "page": 30
  },
  {
    "text": "Depth-First Search (DFS) Depth-First Search (DFS) Goal : Search a graph from a vertex s, like BFS. Solve Single Source Reachability : i.e. Given a vertex s (source), identify all the vertices u reachable from s and provide a path from s to u (not necessarily a shortest path). Return parent tree of parent pointers back to s (like BFS, but not necessarily shortest paths). Idea : Visit outgoing adjacencies recursively, but never revisit a vertex. Meaning that : follow any path until you get stuck, backtrack until finding an unexplored path to explore. Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 15 / 24",
    "source": "Chapter6_AlgoFoundations_GA.pdf",
    "page": 31
  },
  {
    "text": "Depth-First Search (DFS) Depth-First Search (DFS) Goal : Search a graph from a vertex s, like BFS. Solve Single Source Reachability : i.e. Given a vertex s (source), identify all the vertices u reachable from s and provide a path from s to u (not necessarily a shortest path). Return parent tree of parent pointers back to s (like BFS, but not necessarily shortest paths). Idea : Visit outgoing adjacencies recursively, but never revisit a vertex. Meaning that : follow any path until you get stuck, backtrack until finding an unexplored path to explore. Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 15 / 24",
    "source": "Chapter6_AlgoFoundations_GA.pdf",
    "page": 32
  },
  {
    "text": "Depth-First Search (DFS) DFS algorithm : P(s) = None, then run visit( s), where the function “visit” is defined recursively as follows : visit(u) : for every v ∈ Adj(u) that does not appear in P : Set P(v) = u and recursively call visit( v). DFS finishes visiting vertex u (for use later !). Remarks : The principle of the above Depth-First Search algorithm is : Start with the source vertex s Visit one of his neighbours Then visit one neighbour of this neighbour (that was not yet explored) and so on ··· until visiting all reachable vertices from s (when no more neighbors exist backtrack until finding an unexplored path to explore). DFS can also be applied to directed graphs. Here the neighbors of u are taken in the directed sens. DFS can also be applied to the particular case of trees to do a tree traversal. Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 16 / 24",
    "source": "Chapter6_AlgoFoundations_GA.pdf",
    "page": 33
  },
  {
    "text": "Depth-First Search (DFS) DFS algorithm : P(s) = None, then run visit( s), where the function “visit” is defined recursively as follows : visit(u) : for every v ∈ Adj(u) that does not appear in P : Set P(v) = u and recursively call visit( v). DFS finishes visiting vertex u (for use later !). Remarks : The principle of the above Depth-First Search algorithm is : Start with the source vertex s Visit one of his neighbours Then visit one neighbour of this neighbour (that was not yet explored) and so on ··· until visiting all reachable vertices from s (when no more neighbors exist backtrack until finding an unexplored path to explore). DFS can also be applied to directed graphs. Here the neighbors of u are taken in the directed sens. DFS can also be applied to the particular case of trees to do a tree traversal. Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 16 / 24",
    "source": "Chapter6_AlgoFoundations_GA.pdf",
    "page": 34
  },
  {
    "text": "Depth-First Search (DFS) DFS algorithm : P(s) = None, then run visit( s), where the function “visit” is defined recursively as follows : visit(u) : for every v ∈ Adj(u) that does not appear in P : Set P(v) = u and recursively call visit( v). DFS finishes visiting vertex u (for use later !). Remarks : The principle of the above Depth-First Search algorithm is : Start with the source vertex s Visit one of his neighbours Then visit one neighbour of this neighbour (that was not yet explored) and so on ··· until visiting all reachable vertices from s (when no more neighbors exist backtrack until finding an unexplored path to explore). DFS can also be applied to directed graphs. Here the neighbors of u are taken in the directed sens. DFS can also be applied to the particular case of trees to do a tree traversal. Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 16 / 24",
    "source": "Chapter6_AlgoFoundations_GA.pdf",
    "page": 35
  },
  {
    "text": "Depth-First Search (DFS) Example Performing DFS search on the same tree as the example above, starting from the vertex 1, leads to the next traversal of the tree (left), to compare to the previous BFS traversal (right) : Figure – DFS traversal Figure – BFS traversal Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 17 / 24",
    "source": "Chapter6_AlgoFoundations_GA.pdf",
    "page": 36
  },
  {
    "text": "Depth-First Search (DFS) Proposition DFS visits v and correctly sets P(v) for every vertex v reachable from s. Proof Induction on k, for the result on only vertices within distance k from the source vertex s. Base case ( k = 0) : P(s) is set correctly for s and s is visited. Inductive step : Consider vertex v with d(s, v) = k + 1. Consider vertex u, the second to last vertex on some shortest path from s to v. By induction, since d(s, u) = k, DFS visits u and sets P(u) correctly. While visiting u, DFS considers v ∈ Adj(u). Either v is in P, so has already been visited, or v will be visited while visiting u. In either case, v will be visited by DFS and added correctly to P. Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 18 / 24",
    "source": "Chapter6_AlgoFoundations_GA.pdf",
    "page": 37
  },
  {
    "text": "Depth-First Search (DFS) Proposition DFS visits v and correctly sets P(v) for every vertex v reachable from s. Proof Induction on k, for the result on only vertices within distance k from the source vertex s. Base case ( k = 0) : P(s) is set correctly for s and s is visited. Inductive step : Consider vertex v with d(s, v) = k + 1. Consider vertex u, the second to last vertex on some shortest path from s to v. By induction, since d(s, u) = k, DFS visits u and sets P(u) correctly. While visiting u, DFS considers v ∈ Adj(u). Either v is in P, so has already been visited, or v will be visited while visiting u. In either case, v will be visited by DFS and added correctly to P. Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 18 / 24",
    "source": "Chapter6_AlgoFoundations_GA.pdf",
    "page": 38
  },
  {
    "text": "Depth-First Search (DFS) Proposition DFS visits v and correctly sets P(v) for every vertex v reachable from s. Proof Induction on k, for the result on only vertices within distance k from the source vertex s. Base case ( k = 0) : P(s) is set correctly for s and s is visited. Inductive step : Consider vertex v with d(s, v) = k + 1. Consider vertex u, the second to last vertex on some shortest path from s to v. By induction, since d(s, u) = k, DFS visits u and sets P(u) correctly. While visiting u, DFS considers v ∈ Adj(u). Either v is in P, so has already been visited, or v will be visited while visiting u. In either case, v will be visited by DFS and added correctly to P. Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 18 / 24",
    "source": "Chapter6_AlgoFoundations_GA.pdf",
    "page": 39
  },
  {
    "text": "Depth-First Search (DFS) Proposition DFS visits v and correctly sets P(v) for every vertex v reachable from s. Proof Induction on k, for the result on only vertices within distance k from the source vertex s. Base case ( k = 0) : P(s) is set correctly for s and s is visited. Inductive step : Consider vertex v with d(s, v) = k + 1. Consider vertex u, the second to last vertex on some shortest path from s to v. By induction, since d(s, u) = k, DFS visits u and sets P(u) correctly. While visiting u, DFS considers v ∈ Adj(u). Either v is in P, so has already been visited, or v will be visited while visiting u. In either case, v will be visited by DFS and added correctly to P. Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 18 / 24",
    "source": "Chapter6_AlgoFoundations_GA.pdf",
    "page": 40
  },
  {
    "text": "Depth-First Search (DFS) Proposition DFS visits v and correctly sets P(v) for every vertex v reachable from s. Proof Induction on k, for the result on only vertices within distance k from the source vertex s. Base case ( k = 0) : P(s) is set correctly for s and s is visited. Inductive step : Consider vertex v with d(s, v) = k + 1. Consider vertex u, the second to last vertex on some shortest path from s to v. By induction, since d(s, u) = k, DFS visits u and sets P(u) correctly. While visiting u, DFS considers v ∈ Adj(u). Either v is in P, so has already been visited, or v will be visited while visiting u. In either case, v will be visited by DFS and added correctly to P. Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 18 / 24",
    "source": "Chapter6_AlgoFoundations_GA.pdf",
    "page": 41
  },
  {
    "text": "Depth-First Search (DFS) Running time of DFS Algorithm visits each vertex u at most once and spends O(1) time for each v ∈ Adj(u). Total complexity upper bounded by O(1) × P u deg(u) = O(|E|). Unlike BFS, we don’t return a distance for each vertex, so DFS runs in O(|E|) time. Remark :If we want to identify also the vertices that are not reachable from s, i.e. that does not appear in P, the complexity becomes O(|V | + |E|), like BFS. Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 19 / 24",
    "source": "Chapter6_AlgoFoundations_GA.pdf",
    "page": 42
  },
  {
    "text": "Depth-First Search (DFS) Running time of DFS Algorithm visits each vertex u at most once and spends O(1) time for each v ∈ Adj(u). Total complexity upper bounded by O(1) × P u deg(u) = O(|E|). Unlike BFS, we don’t return a distance for each vertex, so DFS runs in O(|E|) time. Remark :If we want to identify also the vertices that are not reachable from s, i.e. that does not appear in P, the complexity becomes O(|V | + |E|), like BFS. Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 19 / 24",
    "source": "Chapter6_AlgoFoundations_GA.pdf",
    "page": 43
  },
  {
    "text": "Depth-First Search (DFS) Running time of DFS Algorithm visits each vertex u at most once and spends O(1) time for each v ∈ Adj(u). Total complexity upper bounded by O(1) × P u deg(u) = O(|E|). Unlike BFS, we don’t return a distance for each vertex, so DFS runs in O(|E|) time. Remark :If we want to identify also the vertices that are not reachable from s, i.e. that does not appear in P, the complexity becomes O(|V | + |E|), like BFS. Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 19 / 24",
    "source": "Chapter6_AlgoFoundations_GA.pdf",
    "page": 44
  },
  {
    "text": "Depth-First Search (DFS) Running time of DFS Algorithm visits each vertex u at most once and spends O(1) time for each v ∈ Adj(u). Total complexity upper bounded by O(1) × P u deg(u) = O(|E|). Unlike BFS, we don’t return a distance for each vertex, so DFS runs in O(|E|) time. Remark :If we want to identify also the vertices that are not reachable from s, i.e. that does not appear in P, the complexity becomes O(|V | + |E|), like BFS. Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 19 / 24",
    "source": "Chapter6_AlgoFoundations_GA.pdf",
    "page": 45
  },
  {
    "text": "Depth-First Search (DFS) Full-BFS and Full-DFS Suppose we want to explore entire graph, not just vertices reachable from one vertex : A simple BFS or DFS from one vertex is not sufficient especially in the case of non connected graph (non strongly connected digraph). Idea :Repeat the graph search algorithm (BFS or DFS) on any unvisited vertex. This unvisited vertex will allow to explore another connected component (or strongly connected component in a digraph). We call this algorithm a Full-BFS or a Full-DFS. Notice that they are providing parent forests instead of parent trees. Remark :A Full-BFS (or a Full-DFS) allows to identify the connected components (or the strongly connected components in a digraph). Complexity :Since we are going through all the vertices, the time complexity of both algorithms Full-BFS and Full-DFS is O(|V | + |E|). Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 20 / 24",
    "source": "Chapter6_AlgoFoundations_GA.pdf",
    "page": 46
  },
  {
    "text": "Depth-First Search (DFS) Full-BFS and Full-DFS Suppose we want to explore entire graph, not just vertices reachable from one vertex : A simple BFS or DFS from one vertex is not sufficient especially in the case of non connected graph (non strongly connected digraph). Idea :Repeat the graph search algorithm (BFS or DFS) on any unvisited vertex. This unvisited vertex will allow to explore another connected component (or strongly connected component in a digraph). We call this algorithm a Full-BFS or a Full-DFS. Notice that they are providing parent forests instead of parent trees. Remark :A Full-BFS (or a Full-DFS) allows to identify the connected components (or the strongly connected components in a digraph). Complexity :Since we are going through all the vertices, the time complexity of both algorithms Full-BFS and Full-DFS is O(|V | + |E|). Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 20 / 24",
    "source": "Chapter6_AlgoFoundations_GA.pdf",
    "page": 47
  },
  {
    "text": "Depth-First Search (DFS) Full-BFS and Full-DFS Suppose we want to explore entire graph, not just vertices reachable from one vertex : A simple BFS or DFS from one vertex is not sufficient especially in the case of non connected graph (non strongly connected digraph). Idea :Repeat the graph search algorithm (BFS or DFS) on any unvisited vertex. This unvisited vertex will allow to explore another connected component (or strongly connected component in a digraph). We call this algorithm a Full-BFS or a Full-DFS. Notice that they are providing parent forests instead of parent trees. Remark :A Full-BFS (or a Full-DFS) allows to identify the connected components (or the strongly connected components in a digraph). Complexity :Since we are going through all the vertices, the time complexity of both algorithms Full-BFS and Full-DFS is O(|V | + |E|). Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 20 / 24",
    "source": "Chapter6_AlgoFoundations_GA.pdf",
    "page": 48
  },
  {
    "text": "Depth-First Search (DFS) Application : Topological Sort Definition A Directed Acyclic Graph(DAG) is a directed graph that contains no directed cycles. A Topological Orderof a graph G = (V , E) is an ordering f on the vertices such that : every edge ( u, v) ∈ E satisfies f (u) < f (v). Recall (ES4 - ex7) : A directed graph admits a topological ordering if and only if it is a DAG. Question : How to find a topological order ? Definition A Finishing Orderis the order in which a Full-DFS finishes visiting each vertex in G. Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 21 / 24",
    "source": "Chapter6_AlgoFoundations_GA.pdf",
    "page": 49
  },
  {
    "text": "Depth-First Search (DFS) Application : Topological Sort Definition A Directed Acyclic Graph(DAG) is a directed graph that contains no directed cycles. A Topological Orderof a graph G = (V , E) is an ordering f on the vertices such that : every edge ( u, v) ∈ E satisfies f (u) < f (v). Recall (ES4 - ex7) : A directed graph admits a topological ordering if and only if it is a DAG. Question : How to find a topological order ? Definition A Finishing Orderis the order in which a Full-DFS finishes visiting each vertex in G. Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 21 / 24",
    "source": "Chapter6_AlgoFoundations_GA.pdf",
    "page": 50
  },
  {
    "text": "Depth-First Search (DFS) Application : Topological Sort Definition A Directed Acyclic Graph(DAG) is a directed graph that contains no directed cycles. A Topological Orderof a graph G = (V , E) is an ordering f on the vertices such that : every edge ( u, v) ∈ E satisfies f (u) < f (v). Recall (ES4 - ex7) : A directed graph admits a topological ordering if and only if it is a DAG. Question : How to find a topological order ? Definition A Finishing Orderis the order in which a Full-DFS finishes visiting each vertex in G. Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 21 / 24",
    "source": "Chapter6_AlgoFoundations_GA.pdf",
    "page": 51
  },
  {
    "text": "Depth-First Search (DFS) Application : Topological Sort Proposition A digraph G = (V , E) is a DAG if and only if the reverse of a finishing order of a Full-DFS is a topological order. Proof ⇐) This implication is trivial. ⇒) Need to prove, for every edge ( u, v) ∈ E that u is ordered before v, i.e. that the visit to v finishes before the visit to u finishes. Two cases : If u is visited before v : Before visit to u finishes, will visit v (via (u, v) or otherwise). Thus the visit to v finishes before finishing the visit to u. If v is visited before u : u can’t be reached from v since graph is acyclic. Thus the visit to v finishes before visiting u. Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 22 / 24",
    "source": "Chapter6_AlgoFoundations_GA.pdf",
    "page": 52
  },
  {
    "text": "Depth-First Search (DFS) Application : Cycle detection Full-DFS will find a topological order iff the graph G = (V , E) is acyclic. If G contains a cycle then reverse finishing order for Full-DFS (called f ) is not a topological order. Meaning that there exists ( u, v) ∈ E such that Full-DFS finishes visiting u before it finishes visiting v (i.e. f (v) < f (u)). Proposition Let G be a diagraph that contains a cycle, and ( u, v) ∈ E such that Full-DFS finishes visiting u before it finishes visiting v, then this Full-DFS contains a path from v to u constructed by considering the reversed path back to s from vertex v. Adding the edge ( u, v) to this path we obtain a cycle. Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 23 / 24",
    "source": "Chapter6_AlgoFoundations_GA.pdf",
    "page": 53
  },
  {
    "text": "Depth-First Search (DFS) Application : Cycle detection Proof (of Prop.) Necessarily v is visited before u (because otherwise using the edge ( u, v), v will be visited and finished before u). Since Full-DFS finishes visiting u before v this means that during the visit of v, Full-DFS starts and finishes the visit of u, and this means that v is an ancestor of u, giving a path from v to u. Complexity :We need to run Full-DFS needing O(|V | + |E|). Then, we need to find an edge ( u, v) such that f (v) < f (u) and this will take O(|E|) to go through all the edges. Finally we need to go through the reverse path from v to u, and this will need at most O(|V |). Finally, the total complexity is O(|V | + |E|). Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 24 / 24",
    "source": "Chapter6_AlgoFoundations_GA.pdf",
    "page": 54
  },
  {
    "text": "Chapter 7 : Shortest Path Problem - Dijkstra’s Algorithm Omar Saadi omar.saadi@um6p.ma UM6P - College of Computing 1st year of the engineering degree of UM6P-CS Academic year 2024-2025 Omar Saadi (College of Computing) Algorithmic Foundations 3 (GA) Computer Science School 1 / 12",
    "source": "Chapter7_AlgoFoundations3_GA.pdf",
    "page": 1
  },
  {
    "text": "Outline 1 Introduction 2 Dijkstra’s Algorithm Omar Saadi (College of Computing) Algorithmic Foundations 3 (GA) Computer Science School 2 / 12",
    "source": "Chapter7_AlgoFoundations3_GA.pdf",
    "page": 2
  },
  {
    "text": "Introduction Outline 1 Introduction 2 Dijkstra’s Algorithm Omar Saadi (College of Computing) Algorithmic Foundations 3 (GA) Computer Science School 3 / 12",
    "source": "Chapter7_AlgoFoundations3_GA.pdf",
    "page": 3
  },
  {
    "text": "Introduction Shortest path problem Some real-world problems : How can you find the shortest routes from your home to every place in town ? How to find the cheapest flight(s) from Casablanca to Hong Kong allowing stopovers ? How to find the fastest driving route (Google Maps) ? The internet : how to find the fastest path to send information through a network of routers ? All these problems can be modeled using weighted (di)graphs, and solving the shortest path problem. Omar Saadi (College of Computing) Algorithmic Foundations 3 (GA) Computer Science School 4 / 12",
    "source": "Chapter7_AlgoFoundations3_GA.pdf",
    "page": 4
  },
  {
    "text": "Introduction Framework We have a graph (or digraph) G = (V , E). Each edge ( u, v) ∈ E has a nonnegative weight w(u, v) (that represents a distance, a cost, a time, ...). We have a starting vertex s and a destination vertex d. Shortest path problem We want to solve the following problem : minimize P a s,d-path in G w(P), where w(P) is the weight of the path P given as the sum of the weights of the edges forming it. Omar Saadi (College of Computing) Algorithmic Foundations 3 (GA) Computer Science School 5 / 12",
    "source": "Chapter7_AlgoFoundations3_GA.pdf",
    "page": 5
  },
  {
    "text": "Dijkstra’s Algorithm Outline 1 Introduction 2 Dijkstra’s Algorithm Omar Saadi (College of Computing) Algorithmic Foundations 3 (GA) Computer Science School 6 / 12",
    "source": "Chapter7_AlgoFoundations3_GA.pdf",
    "page": 6
  },
  {
    "text": "Dijkstra’s Algorithm Dijkstra’s Algorithm Input :A graph (or digraph) with nonnegative edge weights and a starting vertex s. Output :Distance d(s, t) from s to each other vertex t and a shortest path tree given by identifying the parent P(u) of each vertex u. Key idea :∀t ∈ V , the algorithm computes an estimate d[t] of the distance of t from the source s such that : 1 At any point in time, d[t] ≥ d(s, t), and 2 when t is finished, d[t] = d(s, t). Let w(x, y) = +∞ if (x, y) is not an edge. Omar Saadi (College of Computing) Algorithmic Foundations 3 (GA) Computer Science School 7 / 12",
    "source": "Chapter7_AlgoFoundations3_GA.pdf",
    "page": 7
  },
  {
    "text": "Dijkstra’s Algorithm Dijkstra’s Algorithm Idea : Maintain the set S of vertices to which a shortest path from s is known, enlarging S to include all vertices. To do this, maintain a tentative distance d[t] from s to each t ̸∈ S, being the length of the shortest s, t-path yet found. Initialization :Set S = {s}; d[s] = 0; P(s) = None. For t ̸= s, set d[t] = w(s, t) and if w(s, t) ̸= +∞ then take P(t) = s. Iteration :Select a vertex x ̸∈ S such that d[x] = mint̸∈S d[t]. Add the vertex x to S. For each edge ( x, y) ∈ E such that y ̸∈ S : update d[y] to min{d[y], d[x] + w(x, y)}. If d[y] changes, then update P(y) to x. The iteration continues until S = V (G) or until d[y] = +∞ for every y ̸∈ S. At the end, set d(s, x) = d[x] for all x ∈ V . Omar Saadi (College of Computing) Algorithmic Foundations 3 (GA) Computer Science School 8 / 12",
    "source": "Chapter7_AlgoFoundations3_GA.pdf",
    "page": 8
  },
  {
    "text": "Dijkstra’s Algorithm Lemma 1 For every u, at any point of time d[u] ≥ d(s, u) and d[u] represents the length of some path from s to u. Proof We proceed by induction over the iterations of the algorithm. Base case :we know that d[s] = 0 = d(s, s) and for each vertex t ̸= s we have d[t] = w(s, t) ≥ d(s, t), so we know that the claim holds initially. Induction :when d[u] is changed to d[x] + w(x, u) then (by the induction hypothesis) there is a path from s to x of weight d[x] (that does not contain u) and an edge ( x, u) of weight w(x, u). This means there is a path from s to u of weight d[u] = d[x] + w(x, u). This implies that d[u] is at least the weight of the shortest path = d(s, u). Omar Saadi (College of Computing) Algorithmic Foundations 3 (GA) Computer Science School 9 / 12",
    "source": "Chapter7_AlgoFoundations3_GA.pdf",
    "page": 9
  },
  {
    "text": "Dijkstra’s Algorithm Lemma 2 The assertion d[x] ≤ d[y] for all y ̸∈ S stays true at all points after x is inserted into S. Proof We take F = V \\ S. By absurd, assume that at some point for some y ∈ F we get d[y] < d[x] and let y be the first such y. Before d[y] was updated d [y′] ≥ d[x] for all y′ ∈ F . But then when d[y] was changed, it was due to some neighbor y′ of y in F , such that d[y] = d [y′] + w(y′, y) ≥ d [y′] ≥ d[x], so we get a contradiction. Omar Saadi (College of Computing) Algorithmic Foundations 3 (GA) Computer Science School 10 / 12",
    "source": "Chapter7_AlgoFoundations3_GA.pdf",
    "page": 10
  },
  {
    "text": "Dijkstra’s Algorithm Proposition When vertex x is placed in S, d[x] = d(s, x), and backtracking from x to s using the parent list P yields a shortest path from s to x. Proof We take F = V \\ S. We do an induction on the order of placement of vertices into S. For the base case, s is placed into S where d[s] = d(s, s) = 0 and P(s) = None, so initially, the claim holds. Inductive step :we assume that for all vertices y currently in S, d[y] = d(s, y) and P yields shortest paths for them. Let x be the vertex that will be added to S, i.e. satisfying d[x] = mint̸∈S d[t]. Let p be a shortest path from s to x. Suppose z is the vertex on p closest to x for which d[z] = d(s, z). We know z exists since there is at least one such vertex, namely s, where d[s] = d(s, s). By the choice of z, for every vertex y on p between z (not inclusive) to x (inclusive), d[y] > d(s, y). Omar Saadi (College of Computing) Algorithmic Foundations 3 (GA) Computer Science School 11 / 12",
    "source": "Chapter7_AlgoFoundations3_GA.pdf",
    "page": 11
  },
  {
    "text": "Dijkstra’s Algorithm Proof (Cont.) We have the following two options for z : 1 If z = x, then d[x] = d(s, x) and we are done. 2 Suppose z ̸= x. Then there is a vertex z′ after z on p. We know that d[z] = d(s, z) ≤ d(s, x) ≤ d[x]. Finally, towards a contradiction, suppose d[z] < d[x]. By the choice of x ∈ F we know d[x] = mint∈F d[t]. Thus, since d[z] < d[x], we know z ∈ S. This means the edges out of z, and in particular ( z, z′), were already considered by our algorithm. But this means that d [z′] ≤ d(s, z) + w (z, z′) = d (s, z′), because z is on the shortest path from s to z′. However, this contradicts z being the closest vertex on p to x meeting the criteria d[z] = d(s, z). Thus, the assumption d[z] < d[x] is false and d[x] must equal d(s, x). Note that updating the parent P(y) whenever d[y] is improved insures that at the end P contains the parents in shortest paths with lengths d[x] = d(s, x) for all x ∈ V . Omar Saadi (College of Computing) Algorithmic Foundations 3 (GA) Computer Science School 12 / 12",
    "source": "Chapter7_AlgoFoundations3_GA.pdf",
    "page": 12
  },
  {
    "text": "Chapter 8 : Shortest Path Problem - Bellman-Ford Algorithm Omar Saadi omar.saadi@um6p.ma UM6P - College of Computing 1st year of the engineering degree of UM6P-CS Academic year 2024-2025 Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 1 / 13",
    "source": "Chapter8_AlgoFoundations_GA.pdf",
    "page": 1
  },
  {
    "text": "Outline 1 Introduction 2 Bellman-Ford Algorithm Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 2 / 13",
    "source": "Chapter8_AlgoFoundations_GA.pdf",
    "page": 2
  },
  {
    "text": "Introduction Outline 1 Introduction 2 Bellman-Ford Algorithm Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 3 / 13",
    "source": "Chapter8_AlgoFoundations_GA.pdf",
    "page": 3
  },
  {
    "text": "Introduction Framework Definition A negative cycleon a graph (or digraph) is a cycle such that the sum of the weights of its edges is negative (i.e. < 0). We have a graph (or digraph) G = (V , E). Each edge ( u, v) ∈ E has a weight w(u, v) (that can be negative), but G has no negative cycles. We have a starting vertex s and a destination vertex d. We want to solve the shortest path problem : minimize P a s,d-path in G w(P), where w(P) is the weight of the path P given as the sum of the weights of the edges forming it. Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 4 / 13",
    "source": "Chapter8_AlgoFoundations_GA.pdf",
    "page": 4
  },
  {
    "text": "Bellman-Ford Algorithm Outline 1 Introduction 2 Bellman-Ford Algorithm Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 5 / 13",
    "source": "Chapter8_AlgoFoundations_GA.pdf",
    "page": 5
  },
  {
    "text": "Bellman-Ford Algorithm Bellman-Ford Algorithm Input :A graph (or digraph) with no negative cycles and a starting vertex s. Output :Distance d(s, u) from s to each other vertex u and a shortest path tree given by identifying the parent P(u) of each vertex u. Key idea :∀u ∈ V , the algorithm computes an estimate d[u] of the distance of u from the source s such that : At iteration k, d[u] is the length of a path from s to u. The estimate d[u] is non-increasing and it is updated in a dynamic programming way (a step by step way). At the last iteration n − 1, d[u] will contain d(s, u). Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 6 / 13",
    "source": "Chapter8_AlgoFoundations_GA.pdf",
    "page": 6
  },
  {
    "text": "Bellman-Ford Algorithm Bellman-Ford Algorithm Initialization :Set d[s] = 0; P(s) = None. For u ̸= s, set d[u] = +∞ and P(u) = None. Iteration : for i from 1 to n − 1 do : for (u, v) ∈ E do : d[v] = min{d[v], d[u] + w(u, v)}, and if d[v] changes then P[v] = u. for (u, v) ∈ E do : if d[v] > d[u] + w(u, v) then return “A negative cycle exists” Return d[u] and P[u] for all u ∈ V . Remark :The order in which the edges are considered impacts the execution of the algorithm. A possible order of edges is to order the vertices and then take the outgoing edges of each one of them. Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 7 / 13",
    "source": "Chapter8_AlgoFoundations_GA.pdf",
    "page": 7
  },
  {
    "text": "Bellman-Ford Algorithm Bellman-Ford detects negative cycles Proposition If there is a negative cycle reachable from the source s, then at the end we will have for some edge ( u, v) ∈ E, d(v) > d(u) + w(u, v). Proof Suppose v0 → v1 → ··· →vk is a negative cycle reachable from s, where v0 = vk , i.e. Pk i=1 w (vi−1, vi ) < 0. By absurd, suppose that at the end we have d(vi ) ≤ d(vi−1) + w(vi−1, vi ) for all i = 1, . . . ,k. Then taking the sum, we getPk i=1 d (vi ) ≤ Pk i=1 d (vi−1) + Pk i=1 w (vi−1, vi ). Observing that the first two terms are the same (because v0 = vk ), we deduce that : kX i=1 w (vi−1, vi ) ≥ 0. (which is absurd) Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 8 / 13",
    "source": "Chapter8_AlgoFoundations_GA.pdf",
    "page": 8
  },
  {
    "text": "Bellman-Ford Algorithm To prove correctness of the distance estimates (at the end), we reformulate the algorithm in a dynamic programming way. This formulation is useful for the proof but not for space (and time) complexity : Bellman-Ford Algorithm (Variant 1) Initialization :For all k ∈ {0, ··· , n − 1}, set dk [s] = 0; Pk (s) = None and for all u ̸= s, set dk [u] = +∞ and Pk (u) = None. Iteration : for k from 1 to n − 1 do : for (u, v) ∈ E do* : dk [v] = min{dk−1[v], dk−1[u] + w(u, v), dk [v]}, and if dk [v] changes to dk−1[v] then Pk [v] = Pk−1[v], and if dk [v] changes to dk−1[u] + w(u, v) then Pk [v] = u. for (u, v) ∈ E do : if dn−1[v] > dn−1[u] + w(u, v) then return “A negative cycle exists” Return dn−1[u] and Pn−1[u] for all u ∈ V . *Note that this variant 1 of the algorithm is slower (and takes more storage space). Indeed, to have the same iteration as the original version we need to take dk [v] = min{dk−1[v], dk−1[u] + w(u, v), dk [v], dk [u] + w(u, v)}. ⇒ Proving that this variant 1 is correct proves the original version is correct too. Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 9 / 13",
    "source": "Chapter8_AlgoFoundations_GA.pdf",
    "page": 9
  },
  {
    "text": "Bellman-Ford Algorithm Proposition If the graph G has no negative cycles, then dn−1[v] = d(s, v) for all vertices v, and backtracking from v to s using the parent list Pn−1 yields a shortest path from s to v. Proof By induction on k, we will prove that dk [v] is the minimum weight of a path from s to v that uses ≤ k edges. This will show that dn−1[v] is the distance from s to v because there is no negative cycles in the graph (a shortest path contains at most n − 1 edges). Base case :If k = 0, then dk [v] = 0 for v = s, and + ∞ otherwise. So the claim is satisfied. Inductive step :Suppose that for all vertices u, dk−1[u] is the minimum weight of a path from s to u that uses ≤ k − 1 edges. If v ̸= s, let P be a shortest simple path from s to v with ≤ k edges, and let u be the node just before v on P. Let Q be the path from s to u. Then Q is a shortest path from s to u that uses at most k − 1 edges. By the inductive hypothesis, w(Q) = dk−1[u]. Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 10 / 13",
    "source": "Chapter8_AlgoFoundations_GA.pdf",
    "page": 10
  },
  {
    "text": "Bellman-Ford Algorithm Proof (Cont.) In iteration k, we update dk [v] = min (dk−1[v], dk−1[u] + w(u, v)). We know that dk [v] ≤ dk−1[u] + w(u, v) = w(Q) + w(u, v) = w(P), i.e. dk [v] ≤ w(P). Furthermore, dk [v] is the length of a path from s to v with at most k edges, which must be at least as large as w(P). Therefore, dk [v] = w(P) is the minimum weight of a path from s to v that uses at most k edges. Note that the update of the parents Pk (y) insures that the parent list Pk contains the parents in the shortest paths using at most k edges. So that, Pn−1 contains the parents in the shortest paths from s to v for every v ∈ V . Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 11 / 13",
    "source": "Chapter8_AlgoFoundations_GA.pdf",
    "page": 11
  },
  {
    "text": "Bellman-Ford Algorithm Complexity Time complexity :We have the n − 1 iterations and in each iteration we go through all the edges in our graph. For each edge ( u, v) ∈ E, we do O(1) operations. Therefore the time complexity of the algorithm is : O(|V ||E|). Space complexity :The algorithm (in its original version) uses only the two lists d and P. Therefore, the space complexity is O(|V |). Remark :If at a given iteration k no distance d[u] is changed, then the algorithm can be stopped and the distance and shortest paths are found. Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 12 / 13",
    "source": "Chapter8_AlgoFoundations_GA.pdf",
    "page": 12
  },
  {
    "text": "Bellman-Ford Algorithm Example In the following digraph, find the distances from vertex A to all the other vertices and the list of parents providing the shortest paths. A B C D E F G H 2 4 −1 4 5 −3 1 2 2 3−2 2 −3 Omar Saadi (College of Computing) Algorithmic Foundations 3 Computer Science School 13 / 13",
    "source": "Chapter8_AlgoFoundations_GA.pdf",
    "page": 13
  },
  {
    "text": "Chapter 9 : Shortest Path Problem (Floyd-Warshall Algorithm) and Transitive Closure (Warshall’s Algorithm) Omar Saadi omar.saadi@um6p.ma UM6P - College of Computing 1st year of the engineering degree of UM6P-CS Academic year 2024-2025 Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 1 / 18",
    "source": "Chapter9_AlgoFoundations3_GA.pdf",
    "page": 1
  },
  {
    "text": "Outline 1 Introduction 2 Floyd-Warshall Algorithm 3 Transitive Closure Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 2 / 18",
    "source": "Chapter9_AlgoFoundations3_GA.pdf",
    "page": 2
  },
  {
    "text": "Introduction Outline 1 Introduction 2 Floyd-Warshall Algorithm 3 Transitive Closure Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 3 / 18",
    "source": "Chapter9_AlgoFoundations3_GA.pdf",
    "page": 3
  },
  {
    "text": "Introduction Framework We have a digraph (or graph) G = (V , E). Each edge ( u, v) ∈ E has a weight w(u, v) (that can be negative), but G has no negative cycles. All Pairs Shortest Path problem For each pair of vertices s, t, we want to solve the problem : minimize P a s,t-path in G w(P), where w(P) is the weight of the path P given as the sum of the weights of the edges forming it. Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 4 / 18",
    "source": "Chapter9_AlgoFoundations3_GA.pdf",
    "page": 4
  },
  {
    "text": "Floyd-Warshall Algorithm Outline 1 Introduction 2 Floyd-Warshall Algorithm 3 Transitive Closure Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 5 / 18",
    "source": "Chapter9_AlgoFoundations3_GA.pdf",
    "page": 5
  },
  {
    "text": "Floyd-Warshall Algorithm Floyd-Warshall Algorithm Input :A digraph (or graph) G = (V , E) with |V | = n vertices. Output :(If there is no negative cycles)Distance d(s, t) from each vertex s to each vertex t and a shortest path tree (from each starting vertex s) given by identifying the parent P(s, t) of each vertex t in the shortest path from s to t. Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 6 / 18",
    "source": "Chapter9_AlgoFoundations3_GA.pdf",
    "page": 6
  },
  {
    "text": "Floyd-Warshall Algorithm Floyd-Warshall Algorithm Key ideas : The vertices of the digraph are identified with the integers from 1 to n (i.e. V = {1, ··· , n}). If k is the biggest vertex appearing on an s, t-shortest path, then d(s, t) = d(s, k) + d(k, t) and moreover, the sub-paths from s to k and from k to t only use vertices up to k − 1 internally. For each k going from 1 to n, we consider the problem of computing dk (s, t) which is the smallest weight of an s, t-path that only uses vertices 1, ··· , k internally. Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 7 / 18",
    "source": "Chapter9_AlgoFoundations3_GA.pdf",
    "page": 7
  },
  {
    "text": "Floyd-Warshall Algorithm Floyd-Warshall Algorithm Floyd-Warshall Algorithm Initialization : ∀u ∈ V , ∀k ∈ {0, ··· , n}, dk (u, u) = 0, Pk (u, u) = None ∀u, v ∈ V , u ̸= v, ∀k ∈ {1, ··· , n}, dk (u, v) = +∞, Pk (u, v) = None ∀(u, v) ∈ E, d0(u, v) = w(u, v), P0(u, v) = u ∀(u, v) ̸∈ E, d0(u, v) = +∞, P0(u, v) = None Iterations : for k from 1 to n do : for (u, v) ∈ V × V do : if dk−1(u, k) + dk−1(k, v) < dk−1(u, v) then dk (u, v) = dk−1(u, k) + dk−1(k, v) and Pk (u, v) = Pk−1(k, v), else dk (u, v) = dk−1(u, v) and Pk (u, v) = Pk−1(u, v). Return dn(u, v) and Pn(u, v) for all ( u, v) ∈ V × V . Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 8 / 18",
    "source": "Chapter9_AlgoFoundations3_GA.pdf",
    "page": 8
  },
  {
    "text": "Floyd-Warshall Algorithm Correctness Proposition If the digraph G has no negative cycles, then dn(s, t) = d(s, t) for all couple of vertices ( s, t), and backtracking from t to s using the parent list Pn yields a shortest path from s to t. Proof We prove by induction over k ∈ {0, ··· , n}, that dk (s, t) is the minimal length of a path from s to t which is allowed to use internally only the vertices smaller of equal to k. Base case :For k = 0, we initialize each d0(u, v) as the edge weight w(u, v) if ( u, v) ∈ E, else we set it to + ∞, so that d0(u, v) is exactly the length of a shortest path from s to t that don’t use any other vertex internally. P0(u, v) is also properly initialized. Inductive step :Let k ∈ {1, ··· , n} and suppose that the property is true up to k − 1. Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 9 / 18",
    "source": "Chapter9_AlgoFoundations3_GA.pdf",
    "page": 9
  },
  {
    "text": "Floyd-Warshall Algorithm Proof (Cont.) Let P be a shortest path from u to v using 1, ··· , k as intermediate vertices. We can assume that P is a simple path (since there are no negative cycles). There are two cases : 1 P contains k : In this case, we know that neither the path from u to k nor the path from k to v contains any vertices that are greater than k − 1. Then, by the induction hypothesis we get dk−1(u, k) + dk−1(k, v) = w(P) ≤ dk−1(u, v). Therefore, in the algorithm, dk (u, v) = dk−1(u, k) + dk−1(k, v) = w(P) and Pk (u, v) = Pk−1(k, v) is the predecessor of v in a shortest path from u to v using only vertices between 1 and k internally. 2 P does not contain k : In this case P uses only vertices between 1 and k − 1, then w(P) = dk−1(u, v) ≤ dk−1(u, k) + dk−1(k, v). Therefore, from the algorithm, dk (u, v) = dk−1(u, v) = w(P) and Pk (u, v) = Pk−1(u, v) is an adequate update of the predecessor of v. Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 10 / 18",
    "source": "Chapter9_AlgoFoundations3_GA.pdf",
    "page": 10
  },
  {
    "text": "Floyd-Warshall Algorithm Detection of negative cycles Proposition The digraph contains a negative cycle if and only if Floyd-Warshall algorithm returns dn such that dn(u, u) < 0 for some vertex u. Proof If there is a cycle C from u to u of negative weight, then dn(u, u) will be at most the weight of this cycle C, and hence, will be negative. Otherwise, and since d0(u, u) = 0 in the beginning of the algorithm, no update can cause dn(u, u) to become negative. Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 11 / 18",
    "source": "Chapter9_AlgoFoundations3_GA.pdf",
    "page": 11
  },
  {
    "text": "Floyd-Warshall Algorithm Complexity Time Complexity :The runtime of the Floyd-Warshall algorithm is proportional to the size of the table {dk (u, v)}(k,u,v)∈V 3 (which is the same size of the table of predecessors {Pk (u, v)}(k,u,v)∈V 3 ). Indeed, filling each entry of the table d (and also of P) only depends on at most two other entries filled in before it. Therefore, the time complexity is : O \u0000 |V |3\u0001 . Space Complexity :Note that we can choose to store only the table of the current iteration ( dk (u, v), ∀(u, v) ∈ V 2) and the one of the previous iteration (dk−1(u, v), ∀(u, v) ∈ V 2) instead of storing all the tables dk , ∀k ∈ {0, ··· , n}. Therefore, the space complexity is : O \u0000 |V |2\u0001 . Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 12 / 18",
    "source": "Chapter9_AlgoFoundations3_GA.pdf",
    "page": 12
  },
  {
    "text": "Floyd-Warshall Algorithm Example In the following digraph, use Floyd-Warshall algorithm to find the distances between all pairs of vertices and the list of parents providing the shortest paths rooted at each vertex. A B CD 3 5 4 −2 1 3 2 −1 Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 13 / 18",
    "source": "Chapter9_AlgoFoundations3_GA.pdf",
    "page": 13
  },
  {
    "text": "Transitive Closure Outline 1 Introduction 2 Floyd-Warshall Algorithm 3 Transitive Closure Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 14 / 18",
    "source": "Chapter9_AlgoFoundations3_GA.pdf",
    "page": 14
  },
  {
    "text": "Transitive Closure Transitive Closure of a digraph Definition 1 Given a digraph G = (V , E), we call the transitive closureof G, the subset T of V × V such that for all ( u, v) ∈ V × V , (u, v) ∈ T if and only if there is a path from u to v in G. Definition 2 A digraph G is transitive if it satisfies : for all vertices u, v, w, if (u, v) ∈ E and (v, w) ∈ E then (u, w) ∈ E. Lemma The transitive closure T of a graph G gives the smallest set of edges that needs to be added to make the graph G transitive. Proof Remark that if a path from u to v exists in G then necessarily the edge (u, v) needs to be added to G to make it transitive and that adding all such edges suffices to make the digraph transitive. Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 15 / 18",
    "source": "Chapter9_AlgoFoundations3_GA.pdf",
    "page": 15
  },
  {
    "text": "Transitive Closure Warshall’s Algorithm Input :A digraph G = (V , E) with |V | = n vertices. Output :The set T of couples ( u, v) ∈ V × V , such that there exists a path from u to v in the graph G. Key ideas :Similar procedure as Floyd-Warshall algorithm. The vertices of the digraph are identified with the integers from 1 to n (i.e. V = {1, ··· , n}). If (u, v) ∈ T and k is the biggest vertex appearing on an u, v-path, then (u, k) ∈ T and (k, v) ∈ T and moreover, the sub-paths from u to k and from k to v only use vertices up to k − 1 internally. For each k going from 1 to n, we consider the problem of identifying if there is a u,v-path that only uses vertices 1 , ··· , k internally. If such path exists we put ck (u, v) = 1, otherwise we put ck (u, v) = 0. Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 16 / 18",
    "source": "Chapter9_AlgoFoundations3_GA.pdf",
    "page": 16
  },
  {
    "text": "Transitive Closure Warshall’s Algorithm Initialization : ∀u, v ∈ V , ∀k ∈ {1, ··· , n}, ck (u, v) = 0 ∀(u, v) ∈ E, c0(u, v) = 1 ∀(u, v) ̸∈ E, c0(u, v) = 0 Iterations : for k from 1 to n do : for (u, v) ∈ V × V do* : if ck−1(u, v) = 1 or ( ck−1(u, k) = 1 and ck−1(k, v) = 1) then ck (u, v) = 1, Return cn(u, v) for all ( u, v) ∈ V × V . * : This is similar to writing the following (with boolean variables) : ck (u, v) = ck−1(u, v) or \u0002 ck−1(u, k) and ck−1(k, v) \u0003 . Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 17 / 18",
    "source": "Chapter9_AlgoFoundations3_GA.pdf",
    "page": 17
  },
  {
    "text": "Transitive Closure Correctness and complexity of Warshall’s algorithm Proposition For all ( u, v) ∈ V × V , we have cn(u, v) = 1 if and only if there exists a path from u to v in the digraph G, i.e. T = {(u, v) ∈ V 2 | cn(u, v) = 1}. Proof : Similar to the proof for Floyd-Warshall algorithm. Similar to Floyd-Warshall algorithm, we have the following complexities for Warshall’s algorithm : Time Complexity :O(|V |3). Space Complexity :O(|V |2). Omar Saadi (College of Computing) Algorithmic Foundations (GA) Computer Science School 18 / 18",
    "source": "Chapter9_AlgoFoundations3_GA.pdf",
    "page": 18
  },
  {
    "text": "Theory of computation 1 Lecture 1: Automata and Regular languages Adnane Saoud Monday, September 2, 2024",
    "source": "Lecture 1.pdf",
    "page": 1
  },
  {
    "text": "• Engineering degree (École Mohammadia d’ingénieurs) • Ph.D. in Control theory and Computer science at CentraleSupélec and ENS Paris-Saclay, France (2019) • Postdoc researcher at UC Santa Cruz, USA (2020) • Postdoc researcher at UC Berkeley, USA (2021) • Professor at CentraleSupélec (Sep 2021 – Aug 2022) • Professor at UM6P-CC (Since Sept 2023) Research interests: Control theory, machine learning, computer science About Me 2",
    "source": "Lecture 1.pdf",
    "page": 2
  },
  {
    "text": "The Team 3 Instructor: Adnane Saoud Office: SHBM, College of Computing offices Office hours: I will be available Wednesday and Thursday (between 16:00 and 18:00) Or, send me an email and we will find a good time to meet Email: adnane.saoud@um6p.ma TA: Emmanuel Junior WAFO WEMBE (Ph.D. student, Control of monotone dynamical systems) Email: EmmanuelJunior.WafoWembe@um6p.ma TA: Sadek Belamfedel Alaoui (Postdoctoral researcher, Reinforcement learning -based control) Email: sadek.belamfedel@um6p.ma",
    "source": "Lecture 1.pdf",
    "page": 3
  },
  {
    "text": "4 What is computation theory ?",
    "source": "Lecture 1.pdf",
    "page": 4
  },
  {
    "text": "Introduction 5 Computability Theory - What is computable or not (problems that are solvable or not) - Examples: program verification, mathematical truth - Models of Computation: Finite automata: used in text processing, compilers, control of systems context- free grammar: used in programming languages and artificial intelligence Complexity Theory - What is computable in practice? - What makes some problems computationally easy or hard - Example: sorting and factoring problems - P versus NP problem",
    "source": "Lecture 1.pdf",
    "page": 5
  },
  {
    "text": "6 About this course",
    "source": "Lecture 1.pdf",
    "page": 6
  },
  {
    "text": "The main learning objectives of the course are as follows: • Formal Languages: Learn about formal languages, their syntax, and semantics. Describe and recognize different types of formal languages such as regular languages and context-free languages. • Automata Theory: Learn the fundamentals of automata theory such as finite automata, pushdown automata, and T uring machines. Learn how to design these machines, analyze their behavior, and understand their computational power. • Computability Theory: Learn the concept of computability, the Church-T uring thesis, and the Halting problem. Explore the limits of what is computationally solvable and understand undecidability. • Complexity Theory: Learn about computational complexity tools such as time and space complexity classes like P , NP . Be able to distinguish between problems that can be solved efficiently and those that are NP-complete or harder. • Problem Solving and Proof T echniques: Develop problem-solving skills by learning how to construct proofs. Be able to prove whether a problem is decidable or undecidable. • Practical Applications: While the primary focus is on theory, the course provides insights into practical applications, demonstrating how theoretical concepts relate to real-world computational problems. • Preparation for Advanced Studies: The course serves as a foundational course to pursue more advanced studies in computer science, in areas related to formal methods, automata theory. Learning Objectives 7",
    "source": "Lecture 1.pdf",
    "page": 7
  },
  {
    "text": "Five components: 1. Lectures ▪ Thursday’s (and sometimes also Monday) 2. T utorials ▪ Monday (First session) Guided exercises sessions with the instructor 3. Labs ▪ Thursdays (Second session) Hands-on experience on computation theory tools Labs will not be graded, but will help you to further understand the material presented in the lectures Final Labs will be graded 4. Recitations ▪ Thursdays (Second session) Open exercises sessions for discussions and for answering questions 5. Project ▪ Thursdays (Second session) Structure of the Course 8",
    "source": "Lecture 1.pdf",
    "page": 8
  },
  {
    "text": "Course Logistics • Canvas for Q&A and material – You will receive an email shortly providing a link the Canvas – All announcements will be posted there – Also, lecture slides, tutorials and labs – Please, participate and help each other! 9",
    "source": "Lecture 1.pdf",
    "page": 9
  },
  {
    "text": "Reading Material 10 • The book is publicly available in electronic form – Pointers to chapters for every lecture (see the canvas)",
    "source": "Lecture 1.pdf",
    "page": 10
  },
  {
    "text": "• Please ask questions, participate in discussions on Canvas • Play with software tools. Apply what you’ve learnt in theory – This is the actual goal of the lab sessions and the recitations • Give us your feedback! Some Personal Notes ☺ 11",
    "source": "Lecture 1.pdf",
    "page": 11
  },
  {
    "text": "12 Topics that will be covered",
    "source": "Lecture 1.pdf",
    "page": 12
  },
  {
    "text": "1. Introduction, Finite Automata, Regular Expressions 2. Nondeterminism, Closure Properties, from Regular Expressions to finite automata 3. The Regular Pumping Lemma, from Finite Automata to Regular Expressions, Context free grammar 4. Pushdown Automata, equivalence between pushdown automata and context free grammar 5. The Context free Pumping Lemma, Turing Machines 6. Turing machine Variants, the Church-Turing Thesis … Idea about the Schedule (Subject to Change) 13",
    "source": "Lecture 1.pdf",
    "page": 13
  },
  {
    "text": "14 Finite automata",
    "source": "Lecture 1.pdf",
    "page": 14
  },
  {
    "text": "Finite Automata 15 Input: finite string Output: Accept or Reject Computation process: Begin at start state, read input symbols, follow corresponding transitions, Accept if end with accept state, Reject if not. Examples: 01101 → Accept 00101 → Reject 𝑀1 accepts exactly those strings in 𝐴 where 𝐴 = {𝑤| 𝑤 contains substring 11}. 𝑀1 𝑞1 𝑞2 𝑞3 1 0,1 0 10 States: 𝑞1 𝑞2 𝑞3 Transitions: Start state: Accept states: 1 𝐴 is the language of 𝑀1 and that 𝑀1 recognizes 𝐴 and that 𝐴 = 𝐿(𝑀1).",
    "source": "Lecture 1.pdf",
    "page": 15
  },
  {
    "text": "Finite Automata – Formal Definition 16 Definition: A finite automaton 𝑀 is a 5-tuple (𝑄, Σ, 𝛿, 𝑞0, 𝐹) • 𝑄 finite set of states • Σ finite set of alphabet symbols • 𝛿 transition function 𝛿: 𝑄 × Σ → 𝑄 • 𝑞0 start state • 𝐹 set of accept states 𝛿 (𝑞, 𝑎) = 𝑟 means 𝑞 𝑟a 𝑀1 𝑞1 𝑞2 𝑞3 1 0,1 0 10 𝑀1 = (𝑄, Σ, 𝛿, 𝑞1, 𝐹) 𝑄 = {𝑞1, 𝑞2, 𝑞3} Σ = {0, 1} 𝐹 = {𝑞3} 0 1 𝑞1 𝑞1 𝑞2 𝑞2 𝑞1 𝑞3 𝑞3 𝑞3 𝑞3 𝛿 = Example:",
    "source": "Lecture 1.pdf",
    "page": 16
  },
  {
    "text": "17 Regular languages",
    "source": "Lecture 1.pdf",
    "page": 17
  },
  {
    "text": "Definition: A language is regular if some finite automaton recognizes it. Finite Automata and Regular languages 18 Strings and languages - A string is a finite sequence of symbols in Σ - A language is a set of strings - The empty string ε is the string of length 0 - The empty language ø is the set with no strings Definition: 𝑀 accepts string 𝑤 = 𝑤1𝑤2 … 𝑤𝑛 each 𝑤𝑖 𝜖 Σ if there is a sequence of states 𝑟0, 𝑟1, 𝑟2, , … , 𝑟𝑛 𝜖 𝑄 where: - 𝑟0 = 𝑞0 - 𝑟𝑖 = 𝛿(𝑟𝑖−1, 𝑤𝑖) for 1 ≤ 𝑖 ≤ 𝑛 - 𝑟𝑛 𝜖 𝐹Recognizing languages - 𝐿(𝑀) = {𝑤| 𝑀 accepts 𝑤} - 𝐿(𝑀) is the language of 𝑀 - 𝑀 recognizes 𝐿(𝑀)",
    "source": "Lecture 1.pdf",
    "page": 18
  },
  {
    "text": "Regular Languages – Examples 𝐿 𝑀1 = {𝑤| 𝑤 contains substring 11} = 𝐴 Therefore 𝐴 is regular 𝑀1 𝑞1 𝑞2 𝑞3 1 0,1 0 10 Example of a non regular language: Let 𝐶 = 𝑤 𝑤 has equal numbers of 0s and 1s} 𝐶 is not regular (will be proven in next lectures). 19",
    "source": "Lecture 1.pdf",
    "page": 19
  },
  {
    "text": "Regular Expressions Regular operations. Let 𝐴, 𝐵 be languages: - Union: 𝐴 ∪ 𝐵 = 𝑤 𝑤 ∈ 𝐴 or 𝑤 ∈ 𝐵} - Concatenation: 𝐴 ∘ 𝐵 = 𝑥𝑦 𝑥 ∈ 𝐴 and 𝑦 ∈ 𝐵} = 𝐴𝐵 - Star: 𝐴∗ = 𝑥1 … 𝑥𝑘 each 𝑥𝑖 ∈ 𝐴 for 𝑘 ≥ 0} Note: ε ∈ 𝐴∗ always Example. Let 𝐴 = {ab,bc} and 𝐵 = {fg, hk}. - 𝐴 ∪ 𝐵 - 𝐴 ∘ 𝐵 = 𝐴𝐵 - 𝐴∗ 20",
    "source": "Lecture 1.pdf",
    "page": 20
  },
  {
    "text": "Regular Expressions Goal: Show that finite automata are equivalent to regular expressions Regular expressions - Built from Σ, members Σ, ∅, ε [Atomic] - By using ∪,∘,∗ [Composite] Examples: - 0 ∪ 1 ∗ = Σ∗ gives all strings over Σ - Σ∗1 gives all strings that end with 1 - Σ∗11Σ∗ = all strings that contain 11 = 𝐿 𝑀1 21",
    "source": "Lecture 1.pdf",
    "page": 21
  },
  {
    "text": "22 Closure properties",
    "source": "Lecture 1.pdf",
    "page": 22
  },
  {
    "text": "Closure Properties for Regular Languages Theorem: If 𝐴1, 𝐴2 are regular languages, so is 𝐴1 ∪ 𝐴2 (closure under ∪) Proof: Let 𝑀1 = (𝑄1, Σ, 𝛿1 , 𝑞1 , 𝐹1 ) recognize 𝐴1 𝑀2 = (𝑄2, Σ, 𝛿2 , 𝑞2 , 𝐹2 ) recognize 𝐴2 Construct 𝑀 = (𝑄 , Σ, 𝛿 , 𝑞0, 𝐹 ) recognizing 𝐴1 ∪ 𝐴2 𝑀 should accept input 𝑤 if either 𝑀1 or 𝑀2 accept 𝑤. 𝑀2 𝑟 𝑀1 𝑞 ? 𝑀 23",
    "source": "Lecture 1.pdf",
    "page": 23
  },
  {
    "text": "Closure Properties for Regular Languages Theorem: If 𝐴1, 𝐴2 are regular languages, so is 𝐴1 ∪ 𝐴2 (closure under ∪) Proof: Let 𝑀1 = (𝑄1, Σ, 𝛿1 , 𝑞01 , 𝐹1 ) recognize 𝐴1 𝑀2 = (𝑄2, Σ, 𝛿2 , 𝑞02 , 𝐹2 ) recognize 𝐴2 Construct 𝑀 = (𝑄 , Σ, 𝛿 , 𝑞0, 𝐹 ) recognizing 𝐴1 ∪ 𝐴2 𝑀 should accept input 𝑤 if either 𝑀1 or 𝑀2 accept 𝑤. Components of 𝑴: 𝑄 = 𝑄1 × 𝑄2 = 𝑞1, 𝑞2 𝑞1 ∈ 𝑄1 and 𝑞2 ∈ 𝑄2} 𝑞0 = (𝑞01, 𝑞02) 𝛿 𝑞, 𝑟 , 𝑎 = 𝛿1 𝑞, 𝑎 , 𝛿2 𝑟, 𝑎 𝐹 = 𝐹1 × 𝐹2 𝐹 = 𝐹1 × 𝑄2 ∪ 𝑄1 × 𝐹2 when we need this? Theorem: If 𝐴1, 𝐴2 are regular languages, so is 𝐴1 ∩ 𝐴2 24",
    "source": "Lecture 1.pdf",
    "page": 24
  },
  {
    "text": "25 Summary",
    "source": "Lecture 1.pdf",
    "page": 25
  },
  {
    "text": "Summary 26 1. Introduction, outline of the course 2. Finite Automata and regular languages: formal definition 3. Regular Operations and Regular Expressions 4. Theorem: Class of regular languages is closed under union and intersection",
    "source": "Lecture 1.pdf",
    "page": 26
  },
  {
    "text": "Theory of computation 1 Lecture 3: Automata and Regular languages Adnane Saoud Thursday, September 19, 2024",
    "source": "Lecture 3.pdf",
    "page": 1
  },
  {
    "text": "Summary of lecture 2 Lecture 2 ▪ Nondeterministic finite automatas (NFAs) ▪ Closure under ∘ and ∗ ▪ How to convert regular expressions to finite automata Lecture 3 ▪ From finite automata to regular expressions ▪ Proving some languages are not regular ▪ Context free grammars 2",
    "source": "Lecture 3.pdf",
    "page": 2
  },
  {
    "text": "3 From finite automata to regular expressions",
    "source": "Lecture 3.pdf",
    "page": 3
  },
  {
    "text": "DFAs → Regular Expressions Recall Theorem: If a language is described by a regular expression then it is regular Proof: Conversion 𝑅 → NFA 𝑀 → DFA 𝑀′ Today’s Theorem: If a language 𝐴 is regular then it can described as regular expression Proof: Give conversion DFA 𝑀 → 𝑅 We need a new concept: GNFA. • From DFA to GNFA • From GNFA to Regular expressions Regular expression 𝑅 𝑀 Finite automaton 4",
    "source": "Lecture 3.pdf",
    "page": 4
  },
  {
    "text": "Generalized NFAs Definition: A Generalized Nondeterministic Finite Automaton (GNFA) is similar to an NFA, but allows regular expressions as transition labels 5 We first convert the GNFA to have this special form: • The start state has transition arrows going to every other state but no arrows coming in from any other state. • There is only a single accept state, and it has arrows coming in from every other state but no arrows going to any other state. Furthermore, the accept state is not the same as the start state. • Except for the start and accept states, one arrow goes from every state to every other state and also from each state to itself. 𝐺1 𝑞1 aba∗b∗ b a 𝑞3 a ∪ b aab 𝑞2",
    "source": "Lecture 3.pdf",
    "page": 5
  },
  {
    "text": "Generalized NFAs Definition: A Generalized Nondeterministic Finite Automaton (GNFA) is similar to an NFA, but allows regular expressions as transition labels 6 How to convert the GNFA into the special form: • The start state has transition arrows going to every other state but no arrows coming in from any other state. ➢ Add a new start state with an ε arrow to the old start state • There is only a single accept state, and it has arrows coming in from every other state but no arrows going to any other state. Furthermore, the accept state is not the same as the start state. ➢ Add a new accept state with ε arrows from the old accept states. • Except for the start and accept states, one arrow goes from every state to every other state and also from each state to itself. ➢ If there are multiple arrows going between the same two states in the same direction, we replace each with a single arrow whose label is the union of the previous labels. ➢ We add arrows labeled ∅ between states that had no arrows",
    "source": "Lecture 3.pdf",
    "page": 6
  },
  {
    "text": "7 GNFA → Regular Expressions Lemma: Every GNFA 𝐺 has an equivalent regular expression 𝑅 Proof: By induction on the number of states 𝑘 of 𝐺 Basis (𝑘 = 2): Let 𝑅 = 𝑟 Induction step (𝑘 > 2): Assume Lemma true for 𝑘 − 1 states and prove for 𝑘 states IDEA: Convert 𝑘-state GNFA to equivalent 𝑘 − 1 -state GNFA 𝑟𝐺 = 𝐺 is in special form 𝑘 states GNFA 𝑘 − 1 states GNFA",
    "source": "Lecture 3.pdf",
    "page": 7
  },
  {
    "text": "𝑘-state GNFA → (𝑘—1)-state GNFA 𝑘 − 1 states𝑘 states 𝑥 𝑥 𝑞𝑖 𝑞𝑗𝑞𝑖 𝑞𝑗 𝑟1 𝑟2 𝑟3 𝑟4 𝑟1 𝑟2 ∗𝑟3 ∪ 𝑟4 1. Pick any state 𝑥 except the start and accept states. 2. Remove 𝑥. 3. Repair the damage by recovering all paths that went through 𝑥. 4. Make the indicated change for each pair of states 𝑞𝑖, 𝑞𝑗. 8",
    "source": "Lecture 3.pdf",
    "page": 8
  },
  {
    "text": "9 Pumping Lemma",
    "source": "Lecture 3.pdf",
    "page": 9
  },
  {
    "text": "Non-Regular Languages 10 How do we show a language is not regular? - To show a language is regular, we give a DFA. - To show a language is not regular, we must give a proof. - It is not enough to say that you couldn’t find a DFA for it, therefore the language isn’t regular .",
    "source": "Lecture 3.pdf",
    "page": 10
  },
  {
    "text": "Method for Proving Non-regularity 11 Pumping Lemma: For every regular language 𝐴, there is a number 𝑝 (the “pumping length”) such that for any string 𝑠 ∈ 𝐴 satisfying 𝑠 ≥ 𝑝 there exist 𝑥, 𝑦 and 𝑧 such that 𝑠 = 𝑥𝑦𝑧 where 1) 𝑥𝑦𝑖𝑧 ∈ 𝐴 for all 𝑖 ≥ 0 𝑦𝑖 = 𝑦𝑦 ⋯ 𝑦 2) 𝑦 ≠ ε 3) 𝑥𝑦 ≤ 𝑝 Informally: 𝐴 is regular → every long string in 𝐴 can be pumped and the result stays in 𝐴. Proof: Let DFA 𝑀 recognize 𝐴. Let 𝑝 be the number of states in 𝑀. Pick 𝑠 ∈ 𝐴 where 𝑠 ≥ 𝑝. 𝑖 𝑥 𝑦 𝑧 𝑀 𝑞𝑗 𝑠 = 𝑥 𝑦 𝑧 𝑞𝑗 𝑞𝑗 𝑀 will repeat a state 𝑞𝑗 when reading 𝑠 because 𝑠 is so long. The path that 𝑀 follows when reading 𝑠. 𝑥 𝑦 𝑦 𝑧 𝑞𝑗 𝑞𝑗 𝑞𝑗 is also accepted",
    "source": "Lecture 3.pdf",
    "page": 11
  },
  {
    "text": "Example 1 of Proving Non-regularity 12 Let 𝐷 = 0𝑘1𝑘 𝑘 ≥ 0} Show: 𝐷 is not regular Proof by Contradiction: Assume (to get a contradiction) that 𝐷 is regular . The pumping lemma gives 𝑝 as above. Let 𝑠 = 0𝑝1𝑝 ∈ 𝐷. Pumping lemma says that can divide 𝑠 = 𝑥𝑦𝑧 satisfying the 3 conditions. But 𝑥𝑦𝑦𝑧 has excess 0s and thus 𝑥𝑦𝑦𝑧 ∉ 𝐷 contradicting the pumping lemma. Therefore our assumption (𝐷 is regular) is false. We conclude that 𝐷 is not regular. Pumping Lemma: For every regular language 𝐴, there is a 𝑝 such that for any string 𝑠 ∈ 𝐴 satisfying 𝑠 ≥ 𝑝 there exist 𝑥, 𝑦 and 𝑧 such that 𝑠 = 𝑥𝑦𝑧 where 1) 𝑥𝑦𝑖𝑧 ∈ 𝐴 for all 𝑖 ≥ 0 𝑦𝑖 = 𝑦𝑦 ⋯ 𝑦 2) 𝑦 ≠ ε 3) 𝑥𝑦 ≤ 𝑝 ≤ 𝑝 𝑧𝑥 𝑦 𝑠 = 000 ⋯ 000111 ⋯ 111",
    "source": "Lecture 3.pdf",
    "page": 12
  },
  {
    "text": "Example 2 of Proving Non-regularity 13 Let 𝐹 = 𝑤𝑤 𝑤 ∈ Σ∗} . Say Σ∗ = {0,1}. Show: 𝐹 is not regular Proof by Contradiction: Assume (for contradiction) that 𝐹 is regular . The pumping lemma gives 𝑝 as above. Need to choose 𝑠 ∈ 𝐹. Which 𝑠? Try 𝑠 = 0𝑝0𝑝 ∈ 𝐹. But that 𝑠 can be pumped and stay inside 𝐹. Bad choice. Try 𝑠 = 0𝑝10𝑝1 ∈ 𝐹. Show cannot be pumped 𝑠 = 𝑥𝑦𝑧 satisfying the 3 conditions. 𝑥𝑦𝑦𝑧 ∉ 𝐹 Contradiction! Therefore 𝐹 is not regular. 𝑠 = 000 ⋯ 001000 ⋯ 001 ≤ 𝑝 𝑧𝑥 𝑦",
    "source": "Lecture 3.pdf",
    "page": 13
  },
  {
    "text": "Example 3 of Proving Non-regularity 14 Let 𝐵 = 𝑤 𝑤 has equal numbers of 0s and 1s} Show: 𝐵 is not regular Proof by Contradiction: Assume (for contradiction) that 𝐵 is regular . We know that 0∗1∗ is regular so 𝐵 ∩ 0∗1∗ is regular (closure under intersection). But 𝐷 = 𝐵 ∩ 0∗1∗ and we already showed 𝐷 is not regular. Contradiction! Therefore our assumption is false, so 𝐵 is not regular. Variant: Combine closure properties with the Pumping Lemma.",
    "source": "Lecture 3.pdf",
    "page": 14
  },
  {
    "text": "Converting NFAs to DFAs 15 Theorem: If an NFA recognizes 𝐴 then 𝐴 is regular Proof: Let NFA 𝑀 = (𝑄, Σ, 𝛿, 𝑞0 , 𝐹) recognize 𝐴 Construct DFA 𝑀′ = (𝑄′, Σ, 𝛿′, 𝑞0 ′ , 𝐹′) recognizing 𝐴 (Ignore the ε-transitions, can easily modify to handle them) IDEA: DFA 𝑀′ keeps track of the subset of possible states in NFA 𝑀. ε- transitions, can easily modify to handle them) Construction of 𝑴′: 𝑄′ = 𝓟 𝑄 𝛿′ 𝑅, 𝑎 = 𝑞 𝑞 ∈ 𝛿(𝑟, 𝑎) for some 𝑟 ∈ 𝑅} 𝑞0 ′ = {q0} 𝐹′ = 𝑅 ∈ 𝑄′ 𝑅 intersects 𝐹} 6 Context-free Languages",
    "source": "Lecture 3.pdf",
    "page": 15
  },
  {
    "text": "Context Free Grammars 16 } ((Substitution) Rules Rule: Variable → string of variables and terminals Variables: Symbols appearing on left-hand side of rule Terminals: Symbols appearing only on right-hand side Start Variable: Top left symbol Grammars generate strings 1. Write down start variable 2. Replace any variable according to a rule Repeat until only terminals remain 3. Result is the generated string 4. 𝐿(𝐺) is the language of all generated strings. 3 rules R,S 0,1 S S → 0S1 S → R R → ε 𝐺1 In 𝐺1:",
    "source": "Lecture 3.pdf",
    "page": 16
  },
  {
    "text": "Context Free Grammars 17 } ((Substitution) Rules S → 0S1 S → R R → ε 𝐺1 Example of 𝐺1 generating a string S 0 S 1 0 S 1 R ε S 0S1 00S11 00R11 0011 𝐿 𝐺1 = 0𝑘1𝑘 𝑘 ≥ 0} Tree of substitutions Resulting string ∈ 𝐿 𝐺1",
    "source": "Lecture 3.pdf",
    "page": 17
  },
  {
    "text": "18 Summary",
    "source": "Lecture 3.pdf",
    "page": 18
  },
  {
    "text": "Summary 19 1. DFAs, NFAs, regular expressions are all equivalent 2. Proving languages not regular by using the pumping lemma and closure properties 3. Context Free Grammars",
    "source": "Lecture 3.pdf",
    "page": 19
  },
  {
    "text": "Analyse numériqueSéance 4: Résolution de systèmes d’équations linéaires. Partie 2Mohammed-Khalil Ferradikhalil.ferradi@um6p.ma1",
    "source": "chapitre_4.pdf",
    "page": 1
  },
  {
    "text": "2 Méthodes de résolution directes:- Méthode d’élimination de Gauss: le principe de la méthode est de transformer le système initial à résoudre 𝑨𝒙=𝒃en un système équivalent 𝑴𝑨𝒙=𝑴𝒃, tel que la matrice 𝑴𝑨est triangulaire supérieure et sans calculer explicitement la matrice 𝑴.Cette méthode est associée à la factorisation LUde la matrice: 𝑨=𝑳𝑼, avec LetU des matrices triangulaires inférieure et supérieure respectivement. Une fois la factorisation LU obtenue, on résout le système 𝑳𝒚=𝒃, puis le système 𝑼𝒙=𝒚. Pour obtenir 𝒙.-Méthode de Cholesky: cette méthode est valable pour une matrice 𝑨symétrique définie positive. Elle se base sur une factorisation de la matrice sous la forme suivante: 𝑨=𝑳்𝑳. On résout alors le système 𝑳்𝒚=𝒃, puis pour obtenir 𝒙le système 𝑳𝒙=𝒚.",
    "source": "chapitre_4.pdf",
    "page": 2
  },
  {
    "text": "3 - Le principe de la méthode de Gauss est de transformer le système à résoudre en une forme triangulaire.- La méthode est basée sur des combinaisons linéaires des lignes de la matrice. Par exemple pour un vecteur de dimension 2 : 𝒂்=𝑎ଵ𝑎ଶ, si 𝑎ଵ≠ 0, alors:De manière plus générale:Avec 𝑚௜=௔೔௔ೖ ,𝑖=𝑘+1,…,𝑛et 𝑎௞≠0est appelé pivot.1 0−𝑎ଶ𝑎ଵ1𝑎ଵ𝑎ଶ=𝑎ଵ0𝑴௞𝒂=1 … 0 0 … 0⋮ ⋱ ⋮ ⋮ ⋱ ⋮0 … 1 0 … 00 … −𝑚௞ାଵ 1 … 0⋮ ⋱ ⋮ ⋮ ⋱ ⋮0 … −𝑚௡0 … 1𝑎ଵ⋮𝑎௞𝑎௞ାଵ⋮𝑎௡=𝑎ଵ⋮𝑎௞0⋮0 Méthode de Gauss:",
    "source": "chapitre_4.pdf",
    "page": 3
  },
  {
    "text": "4 - La matrice 𝑴௞est appelée matrice d’élimination élémentaire. Elle rajoute des multiples de la ligne kaux lignes qui suivent, de manière à avoir des termes nuls au-dessous du terme en k.-La matrice 𝑴௞est triangulaire inférieure et inversible. Elle peut être représentée sous la forme suivante:Et 𝒆௞le kièmevecteur colonne de la matrice identité.- L’inverse de la matrice 𝑴௞noté 𝑳௞=𝑴௞ିଵest égale à:𝑴௞=𝑰−𝒎௞𝒆௞் , avec 𝒎௞=0 … 0𝑚௞ାଵ…𝑚௡்𝑳௞=𝑰+𝒎௞𝒆௞்",
    "source": "chapitre_4.pdf",
    "page": 4
  },
  {
    "text": "5 - Pour deux matrices d’éliminations élémentaires 𝑴௞et 𝑴௝, avec j > k, leur produit est égal à:Pareil pour 𝑳௞𝑳௝: 𝑳௞𝑳௝=𝑰+𝒎௞𝒆௞்+𝒎௝𝒆௝்Exemple:Pour 𝒂=24−2: 𝑴ଵ𝒂=1−2 11 0 124−2=200, 𝑴ଶ𝒂=10 10 1/2 124−2=240𝑴௞𝑴௝=𝑰−𝒎௞𝒆௞்𝑰−𝒎௝𝒆௝்=𝑰−𝒎௞𝒆௞்−𝒎௝𝒆௝்+𝒎௞𝒆௞்𝒎௝𝒆௝்𝑴௞𝑴௝=𝑰−𝒎௞𝒆௞்−𝒎௝𝒆௝்𝑴ଵ𝑴ଶ=1−2 11 1/2 1",
    "source": "chapitre_4.pdf",
    "page": 5
  },
  {
    "text": "6 - Pour la réduction d’un système d’équations linéaires 𝑨𝒙=𝒃sous forme triangulaire supérieure, on choisit d’abord 𝑎ଵଵ≠ 0comme pivot avec la matrice d’élimination 𝑴ଵcorrespondante, pour mettre à zéro les termes sous la diagonale de la 1èrecolonne :- Ensuite, avec le nouveau pivot 𝑎ଶଶ≠ 0de la matrice 𝑴ଵ𝑨, la matrice d’élimination 𝑴ଶcorrespondante est utilisée pourmettre à zéro les termes sous la diagonale de la 2ièmecolonne :- le processus est répété jusqu’à ce que le système soit complètement écrit sous forme triangulaire supérieure:Le système ainsi obtenu peut être résolu par substitution rétrograde (back-substitution). Ce processus est appelé méthode de Gauss. 𝑴ଵ𝑨𝒙=𝑴ଵ𝒃𝑴ଶ𝑴ଵ𝑨𝒙=𝑴ଶ𝑴ଵ𝒃𝑴௡ିଵ…𝑴ଶ𝑴ଵ𝑨𝒙=𝑴௡ିଵ…𝑴ଶ𝑴ଵ𝒃⇒ 𝑴𝑨𝒙=𝑴𝒃",
    "source": "chapitre_4.pdf",
    "page": 6
  },
  {
    "text": "7 Système initial à résoudre:𝑨𝒙=𝑎ଵଵ଴𝑎ଵଶ଴𝑎ଵଷ଴…𝑎ଵ௡଴𝑎ଶଵ଴𝑎ଶଶ଴𝑎ଶଷ଴…𝑎ଶ௡଴𝑎ଷଵ଴𝑎ଷଶ଴𝑎ଷଷ଴…𝑎ଷ௡଴⋮ ⋮ ⋮ ⋱ ⋮𝑎௡ଵ଴𝑎௡ଶ଴𝑎௡ଷ଴…𝑎௡௡଴𝑥ଵ𝑥ଶ𝑥ଷ⋮𝑥௡=𝑏ଵ଴𝑏ଶ଴𝑏ଷ଴⋮𝑏௡଴𝑎ଵଵ଴𝑎ଵଶ଴𝑎ଵଷ଴…𝑎ଵ௡଴0𝑎ଶଶଵ𝑎ଶଷଵ…𝑎ଶ௡ଵ0𝑎ଷଶଵ𝑎ଷଷଵ…𝑎ଷ௡ଵ⋮ ⋮ ⋮ ⋱ ⋮0𝑎௡ଶଵ𝑎௡ଷଵ…𝑎௡௡ଵ ,𝑏ଵଵ𝑏ଶଵ𝑏ଷଵ⋮𝑏௡ଵ1ièreélimination:𝑎ଵଵ଴𝑎ଵଶ଴𝑎ଵଷ଴…𝑎ଵ௡଴0𝑎ଶଶଵ𝑎ଶଷଵ…𝑎ଶ௡ଵ0 0𝑎ଷଷଶ…𝑎ଷ௡ଶ⋮ ⋮ ⋮ ⋱ ⋮0 0𝑎௡ଷଶ…𝑎௡௡ଶ ,𝑏ଵଶ𝑏ଶଶ𝑏ଷଶ⋮𝑏௡ଶ2ièmeélimination:(n-1)ièmeélimination:𝑎ଵଵ଴𝑎ଵଶ଴𝑎ଵଷ଴…𝑎ଵ௡଴0𝑎ଶଶଵ𝑎ଶଷଵ…𝑎ଶ௡ଵ0 0𝑎ଷଷଶ…𝑎ଷ௡ଶ⋮ ⋮ ⋮ ⋱ ⋮0 0 0 …𝑎௡௡௡ିଵ ,𝑏ଵ௡ିଵ𝑏ଶ௡ିଵ𝑏ଷ௡ିଵ⋮𝑏௡௡ିଵQue faire si lors de la ièmeélimination le pivot est nul ?",
    "source": "chapitre_4.pdf",
    "page": 7
  },
  {
    "text": "8 - L’inverse de la matrice 𝑴:Est une matrice triangulaire inférieure.- Le résultat 𝑼=𝑴𝑨est par construction une matrice triangulaire supérieure. On a donc:- Si au cours de la méthode de Gauss un des termes diagonaux est nul, la matrice 𝑼sera singulière. Néanmoins la factorisation LU est toujours possible.- La méthode de Gauss produit une factorisation LUd’une matrice donnée. La méthode de Gauss et la factorisation LUsont donc un même processus pour résoudre un système d’équations linéaires.- Une fois la factorisation LUde la matrice est obtenue, il devient facile d’obtenir le vecteur solution:𝑳=𝑴ିଵ=𝑴ଵିଵ…𝑴௡ିଵିଵ=𝑳ଵ…𝑳௡ି𝟏𝑨=𝑳𝑼on résout 𝑳𝒚=𝒃, puis 𝑼𝒙=𝒚",
    "source": "chapitre_4.pdf",
    "page": 8
  },
  {
    "text": "9 Exemple:𝑆: 1 2 53 2 −10 5 3𝒙=10,51𝑆ଵ: 1 0 0−3 1 00 0 1𝑴భ 1 2 53 2 −10 5 3𝒙=10,51 ⇒ 1 2 50 −4 −160 5 3𝒙=1−2,51𝑆ଶ: 1 0 00 1 00 1,25 1𝑴మ 1 2 50 −4 −160 5 3𝒙=1−2,51 ⇒ 1 2 50 −4 −160 0 −17𝒙=1−2,5−2,125𝑼𝒙ୀ𝑴𝒃Résolution du système 𝑆ଶpar substitution : 𝑥ଵ=𝑥ଶ=𝑥ଷ=ଵ଼",
    "source": "chapitre_4.pdf",
    "page": 9
  },
  {
    "text": "10 Pour obtenir explicitement la factorisation LU, on calcule la matrice L:On obtient donc la factorisation LU de la matrice A:𝑳=𝑴ଶ𝑴ଵିଵ=𝑳ଵ𝑳ଶ𝑳=1 0 03 1 00 0 11 0 00 1 00 −1,25 1=1 0 03 1 00 −1,25 1𝑨=1 2 53 2 −10 5 3=1 0 03 1 00 −1,25 11 2 50 −4 −160 0 −17=𝑳𝑼",
    "source": "chapitre_4.pdf",
    "page": 10
  },
  {
    "text": "11 Technique du pivot partiel (partial pivoting):- T oute valeur non nulle peut être utilisée comme pivot. Néanmoins en pratique le pivot peut être choisi de telle manière à minimiser les erreurs de calculs.- Pour ne pas amplifier les erreurs d’arrondis lors de la multiplication du reste de la matrice à triangulariser avec la matrice d’élimination élémentaire, la valeur absolue des termes de la matrice élémentaire ne doit pas dépasser 1.- Ceci est obtenu en choisissant comme pivot d’une étape de la méthode de Gauss, la plus grande valeur possible entre la valeur du terme de la diagonale et les valeurs en dessous dans la même colonne.- Cette procédure, appelée pivot partiel, est essentiel pour une implémentation numérique stable de la méthode de Gauss.",
    "source": "chapitre_4.pdf",
    "page": 11
  },
  {
    "text": "12 - Avec le pivot partiel, à chaque étape kde la méthode de Gauss, on effectue une permutation 𝑷௞avant de multiplier par la matrice 𝑴௞, de manière à toujours avoir le plus grand pivot possible dans la diagonale. On obtient toujours 𝑴𝑨=𝑼 avec 𝑼 une matrice triangulaire supérieure, mais avec 𝑴égale à:-La matrice 𝑳=𝑴ିଵn’est pas nécessairement triangulaire inférieure (triangulaire à npermutations près).-On peut effectuer les permutations en amont de la méthode de Gauss pour obtenir:Avec 𝑷=𝑷௡ିଵ…𝑷ଶ𝑷ଵreprésente les permutations effectuées sur les lignes de la matrice 𝑨, et 𝑳étant maintenant triangulaire inférieure.𝑴=𝑴௡ିଵ𝑷௡ିଵ…𝑴ଶ𝑷ଶ𝑴ଵ𝑷ଵ𝑷𝑨=𝑳𝑼",
    "source": "chapitre_4.pdf",
    "page": 12
  },
  {
    "text": "13 - La technique du pivot partiel peut être généralisé à celle du pivot total, en considérant cette fois ci comme pivot le plus grand terme du reste de la matrice à triangulariser .- Le pivot total requiert une permutation des colonnes en plus de celle des lignes de la matrice. La factorisation s’exprime donc sous la forme suivante:Avec 𝑷et𝑸les matrices permutations, et 𝑳et 𝑼étant les matrices triangulaires inférieure et supérieure de la factorisation.- La stabilité numérique du pivot total et plus grande que celle du pivot partiel, mais la recherche du pivot devient plus coûteuse en temps de calcul.𝑷𝑨𝑸=𝑳𝑼",
    "source": "chapitre_4.pdf",
    "page": 13
  },
  {
    "text": "14 Exemple:On considère la matrice à factoriser suivante : avec 0 <𝜀<𝜀௠௔௖௛Si les lignes ne sont pas permutées, le pivot est égal à 𝜀:Avec une arithmétique en virgule flottante, la matrice 𝑼devient: 𝑼=𝜀10 −1/𝜀et donc:𝑨=𝜀11 1𝑴𝑨=1 0−1𝜀1𝜀11 1=𝜀10 1 − 1/𝜀=𝑼 , 𝑳=1 01/𝜀1𝑳𝑼=1 01/𝜀1𝜀10 −1/𝜀=𝜀11 0≠𝑨- L’utilisation d’un pivot ayant une faible valeur a eu pour conséquence une perte en précision du procédé de factorisation",
    "source": "chapitre_4.pdf",
    "page": 14
  },
  {
    "text": "15 On utilise maintenant la méthode du pivot partiel. Le pivot est donc égal à 1 et on obtient:𝑴𝑷𝑨=1 0−𝜀11 1𝜀1=1 10 1 −𝜀=𝑼 , 𝑳=1 0𝜀1Avec une arithmétique en virgule flottante, la matrice 𝑼devient: 𝑼=1 10 1et donc:𝑳𝑼=1 0𝜀11 10 1=1 1𝜀1 +𝜀≈1 1𝜀1=𝑷𝑨",
    "source": "chapitre_4.pdf",
    "page": 15
  },
  {
    "text": "16 Pour certains types de matrices couramment utilisées, il n’est pas nécessaire d’effectuer un changement de pivot:-Matrice à diagonale strictement dominante:-Matrice symétrique définie positive: (les valeurs propres de la matrice sont toutes strictement positives)෍𝑎௜௝< |𝑎௜௜|௡௜ୀଵ,௜ஷ௝ , 𝑗= 1, … ,𝑛𝑨=𝑨் , 𝒙்𝑨𝒙> 0 pour tout 𝒙≠𝟎Une matrice définie positive est nécessairement à diagonale dominante, l’inverse n’étant pas forcément vérifié.",
    "source": "chapitre_4.pdf",
    "page": 16
  },
  {
    "text": "17 Exemple:En utilisant une précision à trois chiffres, on résout le système suivant:La méthode de Gauss avec pivot partiel mène au système triangulaire suivant:Qu’on résout par substitution rétrograde:Malgré un résidu faible correspondant à la précision à 3 chiffres utilisée, la solution approchée 𝒙ෝest loin de la solution exacte 𝒙்=1 1. Ceci est dû au conditionnement de la matrice (𝜅𝑨> 8000) 0,641 0,2420,321 0,121𝑥ଵ𝑥ଶ=0,8830,4420,641 0,2420 0,000242𝑥ଵ𝑥ଶ=0,883−0,000383⇒ 𝒙ෝ=0,7821,58 , 𝒓=𝒃−𝑨𝒙ෝ=−0,000622−0,000202",
    "source": "chapitre_4.pdf",
    "page": 17
  },
  {
    "text": "18 Remarques générales sur la factorisation LU:- Si deux factorisations de la même matrice sont disponibles: 𝑨=𝑳𝑼=𝑳෠𝑼෡, alors 𝑳෠ିଵ𝑳=𝑼෡𝑼ିଵ=𝑫, avec 𝑫une matrice diagonale. - Si 𝑳et 𝑳෠sont des matrices triangulaires inférieures unitaires (termes diagonaux égaux à 1), alors 𝑫=𝑰, et donc 𝑳=𝑳෠et 𝑼=𝑼෡. - Pour une factorisation 𝑨=𝑳𝑼avec 𝑳une matrice triangulaire inférieure unitaire, alors cette factorisation est unique.- La procédure de factorisation requiert environ 𝑛ଷ/3opérations de multiplication et un nombre similaire d’additions. Auxquelles il faut rajouter environ 2𝑛ଶopérations pour la résolution du système par substitution.- Le calcul direct de l’inverse 𝑨ିଵrequiert environ 𝑛ଷopérations. Pour le calcul de l’inverse, on résout 𝑛systèmes linéaires en plus de la factorisation, ce qui est donc plus coûteux que la résolution d’un seul système après la factorisation.",
    "source": "chapitre_4.pdf",
    "page": 18
  },
  {
    "text": "19 Méthode (factorisation) de Cholesky:- Si 𝑨est une matrice symétrique définie positive, alors la factorisation LUpeut être remplacée par la factorisation de Cholesky𝑼=𝑳்: Avec 𝑳une matrice triangulaire inférieure avec des termes diagonaux positifs.Exemple:𝑨=𝑳𝑳்𝑎ଵଵ𝑎ଶଵ𝑎ଶଵ𝑎ଶଶ=𝑙ଵଵ0𝑙ଶଵ𝑙ଶଶ𝑙ଵଵ𝑙ଶଵ0𝑙ଶଶ ⇒ 𝑙ଵଵ=𝑎ଵଵ , 𝑙ଶଵ=𝑎ଶଵ𝑙ଵଵ , 𝑙ଶଶ=𝑎ଶଶ−𝑙ଶଵଶ",
    "source": "chapitre_4.pdf",
    "page": 19
  },
  {
    "text": "20 Algorithme de factorisation de Cholesky pour une matrice 𝑨=𝑎௜௝ଵஸ௜,௝ஸ௡:Pour k = 1 , n𝑙௞௞=𝑎௞௞pour j = k+1 , n𝑙௝௞=𝑎௝௞/𝑙௞௞pour i = k+1 , j𝑎௜௝=𝑎௜௝−𝑙௜௞𝑙௝௞retourner𝑳=𝑙௜௝ଵஸ௜,௝ஸ௡- La factorisation de Cholesky requiert 𝑛ଷ/6opérations de multiplication, ce qui est deux fois moins que la factorisation LU.- Elle ne requiert aucun changement de pivot au cours de la procédure.",
    "source": "chapitre_4.pdf",
    "page": 20
  },
  {
    "text": "21 Raffinement itératif des résultats:On suppose qu’on a une première approximation 𝒙଴de la solution du système 𝑨𝒙=𝒃. Le résidu est égal à:Pour améliorer la solution, on résout 𝑨𝒛଴=𝒓଴et on incrémente la solution:𝒙ଵest une meilleure solution, puisque:Le processus peut être répété jusqu’à l’obtention d’un résultat à la machine précision près.𝒓଴=𝒃−𝑨𝒙଴𝒙ଵ=𝒙଴+𝒛଴𝑨𝒙ଵ=𝑨𝒙଴+𝒛଴=𝑨𝒙଴+𝑨𝒛଴𝑨𝒙ଵ=𝒃−𝒓଴+𝒓଴=𝒃",
    "source": "chapitre_4.pdf",
    "page": 21
  },
  {
    "text": "22 - Le raffinement itératif nécessite de stocker la matrice et sa factorisation en même temps.- Puisque la factorisation LU de la matrice 𝑨est déjà connue, le processus de raffinement du résultat nécessite seulement la résolution d’un système triangulaire par substitution.- Pour pallier les erreurs d’annulation, le résidu doit généralement être calculé avec une précision élevée pour produire des résultats pertinents.",
    "source": "chapitre_4.pdf",
    "page": 22
  },
  {
    "text": "23Fin de la séance",
    "source": "chapitre_4.pdf",
    "page": 23
  },
  {
    "text": "Analyse numériqueSéance 1: Introduction et résolution d’équations non-linéairesMohammed-Khalil Ferradikhalil.ferradi@um6p.ma1",
    "source": "chapitre_1.pdf",
    "page": 1
  },
  {
    "text": "2 Qu’est-ce que l’analyse numérique ?L’analyse numérique est une branche des mathématiques appliquées, ayant pour objectif l’étude des algorithmes permettant d’approximer les solutions de problèmes mathématiques.But:- Simulation de phénomènes physiques réels- Conception sur un modèle virtuelDomaines d’applications: - Calcul des structures: ponts, barrages, tunnels…- Mécanique des fluides: Modélisation du flux d’air chaud pour le séchage du phosphate- Optimisation- … Introduction:",
    "source": "chapitre_1.pdf",
    "page": 2
  },
  {
    "text": "3 Exemple 1:Approximation de Pi par la méthode d’Archimède: Pour n=96, Archimède a obtenu l’approximation suivante:3 +1071≤𝜋≤ 3 +17Utilisation d’une approche par l’intérieur et par l’extérieur pour obtenir un encadrement du résultat (et donc de l’erreur).",
    "source": "chapitre_1.pdf",
    "page": 3
  },
  {
    "text": "4 Simulation de la rupture d’un câble de précontrainte tendu à 350t Exemple 2: Simulation de la rupture d’un câble",
    "source": "chapitre_1.pdf",
    "page": 4
  },
  {
    "text": "5 - Au milieu de XXième siècle et avant l’avènement de l’ère informatique, les calculs étaient faits par des « humancomputers ». À la NASA, la résolution des équations pour le calcul de la trajectoire d’un engin spatial était faite par des algorithmes calculés manuellement.- Le développement de l’analyse numérique est intimement lié à celui des machines de calcul: résolution de problèmes à plusieurs millions de variables avec un simple ordinateur personnel.- Le but du modélisateur utilisant les méthodes d’analyse numérique est d’approximer la solution des équations mathématiques régissant un problème réel, tout en réduisant l’erreur numérique, le temps de calcul, et l’utilisation de la mémoire machine.",
    "source": "chapitre_1.pdf",
    "page": 5
  },
  {
    "text": "6 Résolution d’équations non-linéaires:Pour une fonction notée 𝑓𝑥, on cherche la valeur 𝑥∗solution de l’équation suivante: 𝑓𝑥= 0- La solution 𝑥∗est appelée racine ou zéro de la fonction f.- Plusieurs solutions peuvent exister .On peut deux distinguer deux cas:•Les fonctions à une dimension 𝑓: ℝ → ℝayant une solution scalaire. •Un système couplé à ndimensions, avec 𝒇: ℝ௡→ ℝ௡, dont la solution au système 𝒇𝒙=𝟎est un vecteur .",
    "source": "chapitre_1.pdf",
    "page": 6
  },
  {
    "text": "7 - Exemple d’équation non-linéaire à une dimension:- Exemple d’un système d’équations non-linéaires :tan𝑥−𝑥= 0Dont des solutions approximées sont: 7.72, 14.07, …𝑥ଵଶ−𝑥ଶ+ 0,25 = 0−𝑥ଵ+𝑥ଶଶ+ 0,25 = 0Dont la solution est le vecteur 𝒙்=0.5 0.5",
    "source": "chapitre_1.pdf",
    "page": 7
  },
  {
    "text": "8 - L’existence est l’unicité d’une solution est plus difficile à déterminer pour les équations non-linéaires que pour les équations linéaires.- Pour une fonction continue 𝑓:𝑎,𝑏→ ℝ, si signe𝑓𝑎≠ signe𝑓𝑏, alors le TVI implique ∃𝑥∗ tel que 𝑓𝑥∗= 0.- Il n’y a pas d’analogue à ce résultat en dimension 𝑛> 1.- Les équations non-linéaires peuvent avoir un nombre quelconque de solutions, voire pas de solution.𝑒௫+ 1 = 0 nᇱa pas de solution𝑥ଶ− 1 = 0 a deux solutionstan𝑥−𝑥=0 a une infinité de solutions",
    "source": "chapitre_1.pdf",
    "page": 8
  },
  {
    "text": "9 Méthode de dichotomie (ou de bissection):La méthode de dichotomie requiert un intervalle initial 𝑎,𝑏vérifiant signe𝑓𝑎≠ signe𝑓𝑏, dont la longueur est divisée par deux à chaque itération, jusqu’à l’obtention de la convergence avec la précision souhaitée.L’algorithme s’exprime comme suit:Choisir une précision 𝜀> 0et un intervalle initial 𝑎,𝑏vérifiant signe𝑓𝑎≠ signe𝑓𝑏T ant que 𝑏−𝑎>𝜀faire:𝑐=௔ା௕ଶsi signe𝑓𝑎= signe𝑓𝑐alorsa = csinonb = c",
    "source": "chapitre_1.pdf",
    "page": 9
  },
  {
    "text": "10 Exemple:Intervalle de recherche initial 1,3.𝑓𝑥=𝑥ଶ− 4 sin(𝑥) = 0",
    "source": "chapitre_1.pdf",
    "page": 10
  },
  {
    "text": "11 - La méthode de la bissection n'utilise pas les valeurs des fonctions ni leurs dérivées, seulement leurs signes. - La méthode est certaine de converger, mais très lentement (la convergence est linéaire).- Pour un intervalle de départ donné [a, b], la longueur de l'intervalle après k itérations est (b - a)/2k, donc pour atteindre une tolérance d'erreur 𝜀, il faut environ quel que soit la fonctionf impliquée.logଶ𝑏−𝑎𝜀 itérations",
    "source": "chapitre_1.pdf",
    "page": 11
  },
  {
    "text": "12 Méthode du point fixe:- On appelle 𝑥un point fixe d’une fonction 𝑔: ℝ → ℝs’il vérifie:- De nombreuses méthodes itératives pour résoudre des équations non-linéaires utilisent un schéma d'itération à point fixe ayant la forme suivante:où les points fixes de gsont des solutions de l'équation 𝑓𝑥= 0.Exemple: le point fixe de la fonction 𝑔𝑥= tan𝑥est une solution de l’équation 𝑓𝑥= tan𝑥−𝑥= 0.- Pour une équation donnée 𝑓𝑥= 0, on peut avoir plusieurs représentations équivalentes sous la forme d’un problème de recherche de point fixe 𝑥=𝑔𝑥, avec différents choix pour la fonction g.𝑥=𝑔𝑥𝑥௞ାଵ=𝑔𝑥௞",
    "source": "chapitre_1.pdf",
    "page": 12
  },
  {
    "text": "13 - T oute fonction 𝑔𝑥 n’admet pas forcément un point fixe.- Si la suite 𝑥௞définie par une valeur initiale 𝑥଴et les itérations 𝑥௞ାଵ=𝑔𝑥௞converge, alors elle converge nécessairement vers un point fixe.- Un schéma d’itération à point fixe ne converge pas forcément, même si la fonction admet un point fixe.- Si 𝑥∗=𝑔𝑥∗et 𝑔ᇱ𝑥∗< 1, alors il existe un intervalle contenant 𝑥∗tel que les itérations 𝑥௞ାଵ=𝑔𝑥௞convergent vers 𝑥∗.",
    "source": "chapitre_1.pdf",
    "page": 13
  },
  {
    "text": "14 Pour la fonction 𝑓𝑥=𝑥ଶ−𝑥− 2, les points fixes des fonctions suivantes:représentent des solutions de l’équation 𝑓𝑥= 0.𝑔𝑥=𝑥ଶ− 2𝑔𝑥=𝑥+ 2𝑔𝑥= 1 +2𝑥𝑔𝑥=𝑥ଶ+ 22𝑥− 1",
    "source": "chapitre_1.pdf",
    "page": 14
  },
  {
    "text": "15",
    "source": "chapitre_1.pdf",
    "page": 15
  },
  {
    "text": "16",
    "source": "chapitre_1.pdf",
    "page": 16
  },
  {
    "text": "17",
    "source": "chapitre_1.pdf",
    "page": 17
  },
  {
    "text": "18 Méthode de Newton:On souhaite calculer la solution de l’équation suivante:𝑓𝑥= 0À partir d’un point initial 𝑥଴, l’équation à résoudre est linéarisée en utilisant l’approximation (linéarisation) suivante:ℒ𝑓𝑥= 0 ⇒ 𝑓𝑥଴+𝑓ᇱ𝑥଴Δ𝑥= 0Δ𝑥= −𝑓𝑥଴𝑓ᇱ𝑥଴ ⇒ 𝑥ଵ=𝑥଴+ Δ𝑥Calcul de l’incrément Δ𝑥:Vérification du critère de convergence: 𝑓𝑥ଵ<𝜀avec 𝜀> 0la précision souhaitéeLe calcul est répété jusqu’à l’atteinte de la convergence avec la précision fixée",
    "source": "chapitre_1.pdf",
    "page": 18
  },
  {
    "text": "19 L’algorithme de résolution de l’équation 𝑓𝑥= 0par la méthode de Newton est:Choisir un 𝑥଴initial et une précision 𝜀> 0T ant que 𝑓𝑥௜> εfaire:𝑥௜ାଵ=𝑥௜−௙௫೔௙ᇲ௫೔𝑖←𝑖+ 1Peut-on utiliser un critère de convergence (d’arrêt) autre que 𝑓𝑥௜<𝜀?",
    "source": "chapitre_1.pdf",
    "page": 19
  },
  {
    "text": "20 Représentation graphique de la méthode de Newton:𝑥଴𝑥ଵ𝑥ଷ𝑥ଶ𝑥∗𝑓𝑥଴𝑓𝑥ଵ𝑓𝑥ଶ𝑓𝑥ଷ 𝑥𝑓𝑥𝑥∗vérifiant 𝑓𝑥∗= 0𝑦",
    "source": "chapitre_1.pdf",
    "page": 20
  },
  {
    "text": "21 Exemple: calcul de la racine carrée de 2𝑓𝑥=𝑥ଶ− 2 = 0𝑥଴=𝟏𝑥ଵ=𝑥଴−𝑓𝑥଴𝑓ᇱ𝑥଴= 1 +12=𝟏, 5𝑥ଶ=𝟏,𝟒𝟏6666667𝑥ଷ=1,414215686274509803921568627451𝑥ସ=1.41421356237468991062629557889013491011655𝑥ହ=1.41421356237309504880168962350253024361498(24 chiffres exacts)(12 chiffres exacts)(6 chiffres exacts)",
    "source": "chapitre_1.pdf",
    "page": 21
  },
  {
    "text": "22 Définition de l’ordre de convergence:Soit une suite 𝑥௞convergeant vers sa limite 𝑥∗: lim௞→ାஶ𝑥௞=𝑥∗On dit que la suite converge en un ordre 𝛼s’il existe 𝐶> 0tel que:𝑥௞ାଵ−𝑥∗𝑥௞−𝑥∗ఈ≤𝐶 , 𝑘≥ 0la suite 𝑥௞converge:- linéairement s’il existe 0 <𝜏< 1tel que : 𝑥௞ାଵ−𝑥∗≤𝜏𝑥௞−𝑥∗pour ksuffisamment grand- Super-linéairement si : lim௫→ାஶ௫ೖశభି௫∗௫ೖି௫∗= 0- Quadratiquement s’il existe 𝐶> 0tel que: 𝑥௞ାଵ−𝑥∗≤𝐶𝑥௞−𝑥∗ଶpour ksuffisamment grand- Cubiquement s’il existe 𝐶>0tel que: 𝑥௞ାଵ−𝑥∗≤𝐶𝑥௞−𝑥∗ଷpour ksuffisamment grand",
    "source": "chapitre_1.pdf",
    "page": 22
  },
  {
    "text": "23 Preuve de la convergence quadratique de la méthode de Newton:On considère l’équation suivante 𝑓𝑥= 0, avec fau moins deux fois dérivable, et 𝑥∗vérifiant 𝑓𝑥∗= 0,𝑓ᇱ𝑥∗≠ 0.On écrit le développement de T aylor de fau voisinage de la solution 𝑥∗: 𝑓𝑥∗=𝑓𝑥௡+𝑓ᇱ𝑥௡𝑥∗−𝑥௡+𝑓ᇱᇱ𝜉௡2!𝑥∗−𝑥௡ଶAvec 𝜉௡compris entre 𝑥∗et 𝑥௡𝑥∗=𝑥௡−𝑓𝑥௡𝑓ᇱ𝑥௡௫೙శభ−𝑓ᇱᇱ𝜉௡2𝑓ᇱ𝑥௡𝑥∗−𝑥௡ଶ ⟹ 𝑥∗−𝑥௡ାଵ= −𝑓ᇱᇱ𝜉௡2𝑓ᇱ𝑥௡𝑥∗−𝑥௡ଶ𝑒௡ାଵ=𝑓ᇱᇱ𝜉௡2𝑓ᇱ𝑥௡𝑒௡ଶ ⟹ 𝑒௡ାଵ<𝐶𝑒௡ଶAvec 0 <𝐶= sup௫∈ூ௙ᇲᇲ௫ଶ௙ᇲ௫< +∞On démontre donc la convergence quadratique de la méthode",
    "source": "chapitre_1.pdf",
    "page": 23
  },
  {
    "text": "24 - Le choix du point de départ 𝑥଴a une influence sur la convergence de la méthode de Newton. En effet, 𝑥଴doit être choisi suffisamment proche de la solution pour que l’algorithme puisse converger .Exemple: 𝑓𝑥= tanିଵ𝑥= 0𝑥௞ାଵ=𝑥௞−1 +𝑥௞ଶtanିଵ𝑥௞ 𝑥ସ= 7,963 × 10ିଵ଴𝑥ଷ= −1,061 × 10ିଷ𝑥ଶ= 0,1169𝑥ଵ= −0,5707𝑥଴= 1𝑥ସ= 1,22 × 10ହ𝑥ଷ= −279,3𝑥ଶ= 13,95𝑥ଵ= −3,536𝑥଴= 2- Si la solution 𝑥∗vérifie 𝑓ᇱ𝑥∗= 0, la convergence n’est plus quadratique est devient donc lente.Exemple:𝑓𝑥=𝑥ଶ= 0𝑥௞ାଵ=𝑥௞−𝑥௞ଶ2𝑥௞=𝑥௞2(convergence linéaire)",
    "source": "chapitre_1.pdf",
    "page": 24
  },
  {
    "text": "25 Méthode de la sécante:- La méthode de Newton nécessite de calculer la dérivée de la fonction fà chaque itération, ce qui peut s’avérer coûteux en temps de calcul ou difficile à obtenir pour certaines fonctions.- La méthode de la sécante propose d’utiliser une approximation de la dérivée au lieu de la dérivée elle-même. Le calcul du nouveau point à une itération donnée s’exprime donc par:𝑥௜ାଵ=𝑥௜−𝑓𝑥௜𝑥௜−𝑥௜ିଵ𝑓𝑥௜−𝑓𝑥௜ିଵ",
    "source": "chapitre_1.pdf",
    "page": 25
  },
  {
    "text": "26𝑥଴𝑥ଵ𝑥ଶ𝑥∗𝑓𝑥଴𝑓𝑥ଵ𝑓𝑥ଶ 𝑥𝑓𝑥𝑦Représentation graphique de la méthode de la sécante: Le principe de la méthode de la sécante remonte à l’Egypte antique. Dans le papyrus de Berlin, la méthode est utilisée pour trouver les aires de deux carrés différents dont la somme est égale à 100 (coudées au carré), et dont le rapport des côtés de ces deux carrés étant de 1 pour (1/2 + 1/4). 𝑥ଷ",
    "source": "chapitre_1.pdf",
    "page": 26
  },
  {
    "text": "27Fin de la séance",
    "source": "chapitre_1.pdf",
    "page": 27
  },
  {
    "text": "UM6P College of Computing – School of Computer Science Numerical Analysis CHAPTER 2 Numerical Integration & Differentiation Contents 2.1 Integration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 2.2 Existence, Uniqueness, and Conditioning . . . . . . . . . . . . . . . . . . . . . . . . . . 5 2.3 Numerical Quadrature . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 2.3.1 Newton-Cotes Quadrature . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 2.3.2 Clenshaw-Curtis Quadrature . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 2.3.3 Gaussian Quadrature . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 2.3.4 Progressive Gaussian Quadrature . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 2.3.5 Composite Quadrature . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 2.3.6 Adaptive Quadrature . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 2.4 Other Integration Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 2.4.1",
    "source": "cours-de-integration.pdf",
    "page": 1
  },
  {
    "text": ". . . . . . . . . . 15 2.4 Other Integration Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 2.4.1 Tabular Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 2.4.2 Improper Integrals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 2.4.3 Double Integrals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 2.4.4 Multiple Integrals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 2.5 Integral Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 2.6 Numerical Differentiation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 2.6.1 Finite Difference Approximations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 2.6.2 Automatic Differentiation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 2.1. Integration In elementary geometry, one learns to calculate the area of simple shapes like rectangles, triangles, and circles, as well as the volume of basic solids such as cubes and spheres.",
    "source": "cours-de-integration.pdf",
    "page": 1
  },
  {
    "text": ". . . . . . . . . . . . . . . . . 23 2.1. Integration In elementary geometry, one learns to calculate the area of simple shapes like rectangles, triangles, and circles, as well as the volume of basic solids such as cubes and spheres. A key motivation for the development of integral calculus was the need to determine areas and volumes of regions with irregular shapes, a crucial problem in classical mechanics. One of the foundational principles in this field is that a rigid body can often be approximated as a point mass located at its centroid, and a distributed force can be treated as an equivalent concentrated force applied at a specific point. For example, consider a beam of length L bearing a distributed load, as illustrated in Fig. 2.1. If ρ(x) denotes the density of the load as a function of the horizontal coordinate x, then the total load W on the beam corresponds to the area under the curve ρ(x), given by the integral: W = Z L 0 ρ(x) dx Furthermore, the location of the equivalent concentrated load, known as the centroid C of the area under the curve, has a horizontal coordinate ¯x given by: 1",
    "source": "cours-de-integration.pdf",
    "page": 1
  },
  {
    "text": "UM6P College of Computing – School of Computer Science Numerical Analysis College of Computing / UM6P Numerical Analysis ¯x = 1 W Z L 0 xρ(x) dx Figure 2.1: Concentrated load equivalent to distributed load on beam Ancient mathematicians, such as Archimedes, devised methods to approximate the area of irregular regions by tiling them with small squares of known size and counting the number of squares that fit within the region. While these techniques lacked a rigorous theoretical foundation, such as the modern concept of a limit, they are conceptually similar to numerical methods used today for approximating integrals. For a function f : R → R on an interval [a, b], the modern definition of an integral, I(f) = Z b a f(x) dx is built upon Riemann sums: Rn = nX i=1 (xi+1 − xi) f (ξi) where a = x1 < x2 < ··· < xn < xn+1 = b and ξi ∈ [xi, xi+1] for i = 1, . . . , n. Define hn = max{xi+1 − xi : i = 1, . . . , n− 1}. If, as hn → 0, Rn converges to a finite value R for any choice of xi and ξi, then f is Riemann integrable on [a, b], and the value of the integral is R. This definition naturally leads to a numerical method: use a finite Riemann sum with a sufficiently large n to achieve the desired accuracy. However, unless xi and ξi are chosen carefully, many more evaluations of f may be needed than necessary. In this lecture, we will explore efficient methods that provide high accuracy at a relatively low cost, measured by the number of function evaluations required. Although there are more advanced concepts of integration, such as Lebesgue integration, which serve as powerful theoretical tools, their nonconstructive nature makes them unsuitable for numerical computation. Therefore, we will focus on Riemann integrals. Integration’s utility extends well beyond the initial geometric and mechanical applications and includes: • Integral transforms like the Laplace, Fourier, and Hankel transforms • Special functions in applied mathematics and mathematical physics, many of which have integral repre- sentations, including the gamma, beta, Bessel, and error functions, as well as Fresnel and elliptic integrals • Finite element and boundary element methods for solving partial differential equations • Integral equations and variational methods • Probability and statistics, where fundamental concepts such as probability",
    "source": "cours-de-integration.pdf",
    "page": 2
  },
  {
    "text": "many of which have integral repre- sentations, including the gamma, beta, Bessel, and error functions, as well as Fresnel and elliptic integrals • Finite element and boundary element methods for solving partial differential equations • Integral equations and variational methods • Probability and statistics, where fundamental concepts such as probability distributions, moments, and expectations are defined using integrals • Classical and quantum physics, where many systems’ potential or free energy is represented as an integral, such as the electrostatic potential generated by a charge distribution 2",
    "source": "cours-de-integration.pdf",
    "page": 2
  },
  {
    "text": "UM6P College of Computing – School of Computer Science Numerical Analysis College of Computing / UM6P Numerical Analysis 2.2. Existence, Uniqueness, and Conditioning If f : R → R is bounded and continuous almost everywhere (i.e., continuous except on a set of measure zero, which is a set that can be covered by the union of a countable number of open intervals with arbitrarily small total length) on an interval [a, b], then the Riemann integral I(f) exists. This sufficient condition is also necessary, meaning that unbounded functions are not properly Riemann integrable. In practical terms, integrable functions are bounded functions with at most a finite number of discontinuities within the integration interval. Since all Riemann sums defining the Riemann integral of a given function on a given interval must converge to the same limit, the uniqueness of the Riemann integral is inherent in the definition. The conditioning of an integration problem refers to how sensitive the result is to small changes in the input data, which in this case are the integrand function f and the integration interval [a, b]. To quantify this sensitivity, we use a norm for functions defined on a given interval. Analogous to the ∞-norm for finite-dimensional vectors, for a function f on a closed interval [a, b], we define ∥f∥∞ = max x∈[a,b] |f(x)| Now, let ˆf be a perturbed version of the integrand f. We then have |I( ˆf) − I(f)| = Z b a ˆf(x) dx − Z b a f(x) dx ≤ Z b a | ˆf(x) − f(x)|dx ≤ (b − a)∥ ˆf − f∥∞ This shows that the absolute condition number of the integration problem is at most b − a. This bound is realized when ˆf(x) = f(x) +c, where c is a positive constant, making the absolute condition number equal to b − a. This is a satisfactory result, as the effect of a perturbation in the integrand is at most proportional to the length of the integration interval. For the relative condition number, the preceding inequality implies |I( ˆf) − I(f)|/|I(f)| ∥ ˆf − f∥∞/∥f∥∞ ≤ (b − a)∥f∥∞ |I(f)| This quantity is always greater than or equal to 1 and can become arbitrarily large for some integrands because |I(f)| may be small (or even zero) while ∥f∥∞ remains large. In such cases, the absolute condition number is more appropriate. Nevertheless, the relatively small size of",
    "source": "cours-de-integration.pdf",
    "page": 3
  },
  {
    "text": "(b − a)∥f∥∞ |I(f)| This quantity is always greater than or equal to 1 and can become arbitrarily large for some integrands because |I(f)| may be small (or even zero) while ∥f∥∞ remains large. In such cases, the absolute condition number is more appropriate. Nevertheless, the relatively small size of the condition number indicates that integration problems are generally well-conditioned with respect to perturbations in the integrand. This result is intuitive, as integration is an averaging process that tends to smooth out small changes in the integrand. Now consider a perturbation ˆb of b, with ˆb > b(a similar analysis applies if ˆb < b, or for analogous perturbations of a). We have Z ˆb a f(x) dx − Z b a f(x) dx = Z ˆb b f(x) dx ≤ (ˆb − b) max x∈[b,ˆb] |f(x)| Thus, the absolute condition number with respect to perturbations in the limits of integration is usually moderate, but it can become very large if the integrand has a nearby singularity. 2.3. Numerical Quadrature In elementary calculus one learns to evaluate a definite integral I(f) = Z b a f(x)dx analytically by finding an antiderivative F of the integrand function f (i.e., a function F such that F′(x) = f(x)), and then using the Fundamental Theorem of Calculus to evaluate I(f) = F(b) − F(a) Unfortunately, some integrals cannot be evaluated in such a closed form (e.g., f(x) = exp \u0000 −x2\u0001 ), and many integrals arising in practice are too complicated to evaluate analytically even if this were possible in principle. Thus, we are often forced to employ numerical methods to evaluate definite integrals approximately. 3",
    "source": "cours-de-integration.pdf",
    "page": 3
  },
  {
    "text": "UM6P College of Computing – School of Computer Science Numerical Analysis College of Computing / UM6P Numerical Analysis The numerical approximation of definite integrals is known as numerical quadrature. This name, which derives from ancient methods for approximating areas of irregular or curved figures by tiling them with small squares, helps distinguish this topic from the numerical integration of differential equations, which we will consider later in this notes. In approximating integrals, we will take our cue from the Riemann sums that define the integral: the integral will be approximated by a weighted sum of integrand values at a finite number of sample points in the interval of integration. Specifically, the integral I(f) is approximated by an n-point quadrature rule, which has the form Qn(f) = nX i=1 wif (xi) where a ≤ x1 < x2 < ··· < xn ≤ b. The points xi at which the integrand f is evaluated are called nodes or abscissas, and the multipliers wi are called weights or coefficients. A quadrature rule is said to be open if a < x1 and xn < b, and closed if a = x1 and xn = b. Our main objective will be to choose the nodes and weights so that we obtain a desired level of accuracy at a reasonable computational cost in terms of the number of integrand evaluations required. Quadrature rules can be derived using polynomial interpolation. In effect, the integrand function f is evaluated at the points xi, i= 1, . . . , n, the polynomial of degree n − 1 that interpolates the function values at those points is determined, and the integral of the interpolant is then taken as an approximation to the integral of the original function. In practice, the interpolating polynomial is not determined explicitly each time a particular integral is to be evaluated. Instead, polynomial interpolation is used to determine the nodes and weights for a given quadrature rule, which can then be used in approximating the integral of any function over the interval. In particular, if Lagrange interpolation is used, then the weights are given by the integrals of the corresponding Lagrange basis functions for the given set of points xi, i= 1, . . . , n, wi = Z b a ℓi(x)dx, i = 1, . . . , n and these are the same for any integrand. For obvious reasons, the resulting",
    "source": "cours-de-integration.pdf",
    "page": 4
  },
  {
    "text": "by the integrals of the corresponding Lagrange basis functions for the given set of points xi, i= 1, . . . , n, wi = Z b a ℓi(x)dx, i = 1, . . . , n and these are the same for any integrand. For obvious reasons, the resulting quadrature rule is said to be interpolatory. An alternative method for deriving interpolatory quadrature rules, called the method of undetermined coefficients, is to choose the weights so that the rule integrates the first n polynomial basis functions exactly, which results in a system of n equations in n unknowns. With the monomial basis, for example, this strategy results in the system of moment equations w1 · 1 + w2 · 1 + ··· + wn · 1 = Z b a 1dx = b − a w1 · x1 + w2 · x2 + ··· + wn · xn = Z b a xdx = \u0000 b2 − a2\u0001 /2 ... w1 · xn−1 1 + w2 · xn−1 2 + ··· + wn · xn−1 n = Z b a xn−1dx = (bn − an) /n Writing this system in matrix form, we have   1 1 ··· 1 x1 x2 ··· xn ... ... ... ... xn−1 1 xn−1 2 ··· xn−1 n     w1 w2 ... wn   =   b − a\u0000 b2 − a2\u0001 /2 ... (bn − an) /n   The matrix of this system is a Vandermonde matrix, which is nonsingular because the nodes xi are assumed to be distinct. The unique solution of this system yields the weights w1, . . . , wn, which are the same as those given by integrating the Lagrange basis functions. Example 1: Method of Undetermined Coefficients We illustrate the method of undetermined coefficients by deriving a three-point quadrature rule Q3(f) = w1f (x1) + w2f (x2) + w3f (x3) 4",
    "source": "cours-de-integration.pdf",
    "page": 4
  },
  {
    "text": "UM6P College of Computing – School of Computer Science Numerical Analysis College of Computing / UM6P Numerical Analysis for the interval [a, b] using the monomial basis. We take the two endpoints and midpoint as the three nodes, i.e., x1 = a, x2 = (a + b)/2, and x3 = b. Written in matrix form, the moment equations give the Vandermonde system   1 1 1 a (a + b)/2 b a2 ((a + b)/2)2 b2     w1 w2 w3   =   b − a\u0000 b2 − a2\u0001 /2\u0000 b3 − a3\u0001 /3   Solving this system by Gaussian elimination, we obtain the weights w1 = (b − a)/6, w 2 = 2(b − a)/3, w 3 = (b − a)/6 The resulting quadrature rule is known as Simpson’s rule. By construction, an n-point interpolatory quadrature rule integrates each of the first n − 1 monomial basis functions exactly, and hence by linearity it integrates any polynomial of degree at most n − 1 exactly. A quadrature rule is said to be of degree d if it is exact (i.e., the error is zero) for every polynomial of degree d but is not exact for some polynomial of degree d + 1. As we have just seen, an n-point interpolatory quadrature rule is of degree at least n−1. Conversely, any quadrature rule with degree at least n−1 must be interpolatory, since it satisfies the moment equations. The significance of the degree is that it conveniently characterizes the accuracy of a given rule. If Qn is an interpolatory quadrature rule, and pn−1 is the polynomial of degree at most n − 1 interpolating a sufficiently smooth integrand f at the nodes x1, . . . , xn, then the error bound for the polynomial interpolant yields the following rough error bound for the approximate integral: |I(f) − Qn(f)| = |I(f) − I (pn−1)| = |I (f − pn−1)| ≤ (b − a) ∥f − pn−1∥∞ ≤ b − a 4n hn f(n) ∞ ≤ 1 4hn+1 f(n) ∞ where h = max {xi+1 − xi : i = 1, . . . , n− 1}. We will be able to make sharper error estimates when we consider specific quadrature rules, but the preceding general bound already indicates that we can obtain higher accuracy by taking n larger, or h smaller, or both. Indeed,",
    "source": "cours-de-integration.pdf",
    "page": 5
  },
  {
    "text": "− xi : i = 1, . . . , n− 1}. We will be able to make sharper error estimates when we consider specific quadrature rules, but the preceding general bound already indicates that we can obtain higher accuracy by taking n larger, or h smaller, or both. Indeed, the bound shows that Qn(f) → I(f) as n → ∞, as well as the minimum convergence rate we can expect, provided f(n) remains well behaved. In the absence of the latter assumption, however, convergence may not obtain or the rate may be arbitrarily slow. That some regularity assumptions on the integrand function are necessary to obtain satisfactory results should not be surprising, as otherwise any method based on sampling the integrand at only a finite number of points can be completely wrong. When the number of sample points is increased, say from n to m, an important factor affecting efficiency is whether the n function values already computed can be reused in the new rule, so that only m − n new function values need be computed. A sequence of quadrature rules is said to be progressive if the nodes of Qn1 are a subset of those of Qn2 for n2 > n1. Instead of (or in addition to) increasing the number of points (and hence the degree), the preceding bound also suggests that the error can be reduced by subdividing the interval of integration into smaller subintervals and applying the quadrature rule separately in each, since this will reduce h. This approach, which is equivalent to using piecewise polynomial interpolation on the original interval, leads to composite (or compound) quadrature rules, which we will consider in Section §2.3.5. For now, we will focus on simple quadrature rules, in which a single rule is applied over the entire given interval. The error bound just given, and others we will see later based on Taylor series expansions, depend on higher derivatives of the integrand function for which we may have no convenient bound. In practice, therefore, error estimates for quadrature rules are usually based on the difference between the results obtained when two different rules are used to approximate the same integral. The second rule may be obtained from the base rule either by increasing the number of points (and hence the degree) or by subdividing the interval of integration and applying the base rule in each subinterval. We will",
    "source": "cours-de-integration.pdf",
    "page": 5
  },
  {
    "text": "obtained when two different rules are used to approximate the same integral. The second rule may be obtained from the base rule either by increasing the number of points (and hence the degree) or by subdividing the interval of integration and applying the base rule in each subinterval. We will see examples of both of these approaches. To save on function evaluations, it is highly desirable for the two rules to be progressive. In addition to its accuracy, we must also be concerned with the stability of a quadrature rule. If ˆf is a perturbation to the integrand function f, then we have 5",
    "source": "cours-de-integration.pdf",
    "page": 5
  },
  {
    "text": "UM6P College of Computing – School of Computer Science Numerical Analysis College of Computing / UM6P Numerical Analysis Qn( ˆf) − Qn(f) = Qn( ˆf − f) = nX i=1 wi \u0010 ˆf (xi) − f (xi) \u0011 ≤ nX i=1 \u0010 |wi| · ˆf (xi) − f (xi) \u0011 ≤ nX i=1 |wi| ! ∥ ˆf − f∥∞ which says that the absolute condition number of the quadrature rule is at most Pn i=1 |wi|. The preceding bound is attainable for an appropriately chosen perturbation, so the absolute condition number is in fact equal to Pn i=1 |wi|. Recall from the first moment equation that for an interpolatory quadrature rule we havePn i=1 wi = b−a. If the weights are all nonnegative, then the absolute condition number of the quadrature rule is b − a, which the same as that of the underlying integration problem, and thus the quadrature rule is stable. If some of the weights are negative, however, then the absolute condition number can be much larger and the quadrature rule can be unstable. 2.3.1. Newton-Cotes Quadrature The simplest placement of nodes for an interpolatory quadrature rule is to choose equally spaced points in the interval [a, b], which is the defining property of NewtonCotes quadrature. An n-point open Newton-Cotes rule has nodes xi = a + i(b − a)/(n + 1), i = 1, . . . , n and an n-point closed Newton-Cotes rule has nodes xi = a + (i − 1)(b − a)/(n − 1), i = 1, . . . , n The following are some of the simplest and best known examples of Newton-Cotes quadrature rules: • Interpolating the function value at the midpoint of the interval by a polynomial of degree zero (i.e., a constant) gives the one-point open Newton-Cotes rule known as the midpoint rule: M(f) = (b − a)f \u0012a + b 2 \u0013 • Interpolating the function values at the two endpoints of the interval by a polynomial of degree one (i.e., a straight line) gives the two-point closed Newton-Cotes rule known as the trapezoid rule: T(f) = b − a 2 (f(a) + f(b)) • Interpolating the function values at the two endpoints and the midpoint by a polynomial of degree two (i.e., a quadratic) gives the three-point closed NewtonCotes rule known as Simpson’s rule: S(f) = b − a 6 \u0012 f(a) + 4f \u0012a",
    "source": "cours-de-integration.pdf",
    "page": 6
  },
  {
    "text": "= b − a 2 (f(a) + f(b)) • Interpolating the function values at the two endpoints and the midpoint by a polynomial of degree two (i.e., a quadratic) gives the three-point closed NewtonCotes rule known as Simpson’s rule: S(f) = b − a 6 \u0012 f(a) + 4f \u0012a + b 2 \u0013 + f(b) \u0013 which we derived in Example 1. Example 2: Newton-Cotes Quadrature To illustrate the application of Newton-Cotes quadrature rules, we approximate the integral I(f) = Z 1 0 e−x2 dx using each of the three Newton-Cotes quadrature rules just given. 6",
    "source": "cours-de-integration.pdf",
    "page": 6
  },
  {
    "text": "UM6P College of Computing – School of Computer Science Numerical Analysis College of Computing / UM6P Numerical Analysis M(f) = (1 − 0) exp(−0.25) ≈ 0.778801 T(f) = 1 2(exp(0) + exp(−1)) ≈ 0.683940 S(f) = 1 6(exp(0) + 4 exp(−0.25) + exp(−1)) ≈ 0.747180 The integrand and the interpolating polynomial for each rule are shown in Fig. 2.2. The correctly rounded result for this problem is 0.746824. It is somewhat surprising to see that the magnitude of the error from the trapezoid rule (0.062884) is about twice that from the midpoint rule (0.031977), and that Simpson’s rule, with an error of only 0.000356, seems remarkably accurate considering the size of the interval over which it is applied. We will soon see explanations for these phenomena. Figure 2.2: Integration of f(x) = e−x2 by Newton-Cotes quadrature rules The error in the midpoint quadrature rule can be estimated using a Taylor series expansion about the midpoint m = (a + b)/2 of the interval [a, b] : f(x) =f(m) + f′(m)(x − m) + f′′(m) 2 (x − m)2 + f(3)(m) 6 (x − m)3 + f(4)(m) 24 (x − m)4 + ··· Integrating this expression from a to b, the odd-order terms drop out, yielding I(f) = f(m)(b − a) + f′′(m) 24 (b − a)3 + f(4)(m) 1920 (b − a)5 + ··· = M(f) + E(f) + F(f) + ··· where E(f) and F(f) represent the first two terms in the error expansion for the midpoint rule. To derive a comparable error expansion for the trapezoid quadrature rule, we substitute x = a and x = b into the Taylor series, add the two resulting series together, observe once again that the odd-order terms drop out, solve for f(m), and substitute into the midpoint expansion to obtain I(f) = T(f) − 2E(f) − 4F(f) − ··· Note that T(f) − M(f) = 3E(f) + 5F(f) + ··· and hence the difference between the two quadrature rules provides an estimate for the dominant term in their error expansions, E(f) ≈ T(f) − M(f) 3 provided that the length of the interval is sufficiently small that (b − a)5 ≪ (b − a)3, and the integrand f is such that f(4) is well-behaved. Under these assumptions, we may draw several conclusions from the preceding derivations: • The midpoint rule is about twice as accurate as the trapezoid rule (as we",
    "source": "cours-de-integration.pdf",
    "page": 7
  },
  {
    "text": "the interval is sufficiently small that (b − a)5 ≪ (b − a)3, and the integrand f is such that f(4) is well-behaved. Under these assumptions, we may draw several conclusions from the preceding derivations: • The midpoint rule is about twice as accurate as the trapezoid rule (as we saw in Example 2), despite being based on a polynomial interpolant of degree one less. • The difference between the midpoint rule and the trapezoid rule can be used to estimate the error in either of them. 7",
    "source": "cours-de-integration.pdf",
    "page": 7
  },
  {
    "text": "UM6P College of Computing – School of Computer Science Numerical Analysis College of Computing / UM6P Numerical Analysis • Halving the length of the interval decreases the error in either rule by a factor of about 1/8. An appropriately weighted combination of the midpoint and trapezoid rules eliminates the leading term, E(f), from the error expansion, I(f) = 2 3M(f) + 1 3T(f) − 2 3F(f) + ··· = S(f) − 2 3F(f) + ··· which provides an alternative derivation for Simpson’s rule as well as an expression for its dominant error term. Example 3: Error Estimation We illustrate these error estimates by computing the approximate value for the integral R 1 0 x2dx. Using the midpoint rule, we obtain M(f) = (1 − 0) \u00121 2 \u00132 = 1 4 and using the trapezoid rule we obtain T(f) = 1 − 0 2 \u0000 02 + 12\u0001 = 1 2 Thus, we have the estimate E(f) ≈ T(f) − M(f) 3 = 1/4 3 = 1 12 We conclude that the error in M(f) is about 1 12 , and the error in T(f) is about −1 6 . In addition, we can now compute the approximate value given by Simpson’s rule for this integral, S(f) = 2 3M(f) + 1 3T(f) = 2 3 · 1 4 + 1 3 · 1 2 = 1 3 which is exact for this integral (as is to be expected since, by design, Simpson’s rule is exact for quadratic polynomials). Thus, the error estimates for M(f) and T(f) are exact for this integrand (though this would not be true in general). We observed previously that an n-point interpolatory quadrature has degree at least n − 1. Thus, we would expect the midpoint rule to have degree zero, the trapezoid rule degree one, Simpson’s rule degree two, and so on. We saw from the Taylor series expansion, however, that the error for the midpoint rule depends on the second and higher derivatives of the integrand, which vanish for linear as well as for constant polynomials. This implies that the midpoint rule integrates linear polynomials exactly, and hence its degree is one rather than zero. Similarly, the error for Simpson’s rule depends on the fourth and higher derivatives, which vanish for cubic as well as quadratic polynomials, so that Simpson’s rule is of degree three rather than two (which explains the surprisingly high",
    "source": "cours-de-integration.pdf",
    "page": 8
  },
  {
    "text": "linear polynomials exactly, and hence its degree is one rather than zero. Similarly, the error for Simpson’s rule depends on the fourth and higher derivatives, which vanish for cubic as well as quadratic polynomials, so that Simpson’s rule is of degree three rather than two (which explains the surprisingly high accuracy obtained in Example 2). In general, for any odd value of n, an n-point Newton-Cotes rule has degree one greater than that of the poly- nomial interpolant on which it is based. This phenomenon is due to cancellation of positive and negative errors, as illustrated for the midpoint and Simpson rules in Fig. 2.3, which, on the left, shows a linear polynomial and the constant function interpolating it at the midpoint and, on the right, a cubic and the quadratic interpolating it at the midpoint and endpoints. Integration of the linear polynomial by the midpoint rule yields two congruent triangles of equal area. The inclusion of one of the triangles compensates exactly for the omission of the other. A similar phenomenon occurs for the cubic polynomial, where the two shaded regions also have equal areas, so that the addition of one compensates for the subtraction of the other. Such cancellation does not occur, however, for an n-point Newton-Cotes rule if n is even. Thus, in general, an n point Newton-Cotes rule is of degree n − 1 if n is even, but of degree n if n is odd. Newton-Cotes quadrature rules are relatively easy to derive and to apply, but they have some serious drawbacks. Recall from the previous lecture that interpolation of a continuous function at equally spaced points by a high-degree polynomial may suffer from unwanted oscillation, and as the number of interpolation points grows, convergence to the underlying function is not guaranteed. The consequence for quadrature rules based on such interpolation is that every n-point Newton-Cotes rule with n ≥ 11 has at least one negative weight. Indeed, 8",
    "source": "cours-de-integration.pdf",
    "page": 8
  },
  {
    "text": "UM6P College of Computing – School of Computer Science Numerical Analysis College of Computing / UM6P Numerical Analysis Figure 2.3: Cancellation of errors in midpoint (left) and Simpson (right) rules it can be shown that Pn i=1 |wi| → ∞as n → ∞, which means that Newton-Cotes rules become arbitrarily ill-conditioned, and hence unstable, as the number of points grows. The presence of large positive and negative weights also means that the value of the integral is computed as a sum of large quantities of differing sign, and hence substantial cancellation is likely in finite-precision arithmetic. For the reasons just given, we cannot expect to attain arbitrarily high accuracy on a given interval by using a Newton-Cotes rule with a large number of points. In practice, therefore, Newton-Cotes rules are usually restricted to a modest number of points, and if higher accuracy is required, then the interval is subdivided and the rule is applied in each subinterval separately (such strategies will be discussed in Section §2.3.5). In this regard, a positive feature of Newton-Cotes rules is that they are progressive, but on the other hand, Newton-Cotes rules do not have the highest possible degree (and hence accuracy) for the number of points used (and hence the number of function evaluations required), and we will soon see that we can do much better. 2.3.2. Clenshaw-Curtis Quadrature We saw in the previous lecture that the Chebyshev points, suitably transformed from [−1, 1] to a given interval of interest, have distinct advantages over equally spaced points for interpolating a continuous function by a polynomial. In particular, the maximum error over the interval is generally much smaller with the Chebyshev points, and the resulting interpolants converge to any sufficiently smooth function as the number of points n → ∞. One might conjecture, therefore, that the Chebyshev points would also be a better choice of nodes for interpolatory quadrature rules, as was first suggested by Fejér, and indeed this expectation turns out to be the case. With the Chebyshev points as nodes for a given n, the corresponding weights could be computed in the usual manner by integrating the Lagrange basis functions or by the method of undetermined coefficients. It can be shown that the resulting weights are always positive for any n, and that the resulting approximate values converge to the exact integral as n → ∞. Thus, quadrature rules based on the Chebyshev",
    "source": "cours-de-integration.pdf",
    "page": 9
  },
  {
    "text": "manner by integrating the Lagrange basis functions or by the method of undetermined coefficients. It can be shown that the resulting weights are always positive for any n, and that the resulting approximate values converge to the exact integral as n → ∞. Thus, quadrature rules based on the Chebyshev points are extremely attractive in that they are always stable and significantly more accurate than Newton-Cotes rules for the same number of nodes. Another attractive feature of such quadrature rules is that they can be implemented in a self-contained manner: as Clenshaw and Curtis first observed, the weights corresponding to the Chebyshev points need not be tabulated in advance or even computed explicitly at all. Instead, based on the trigonometric definition of the Cheby- shev polynomials, they developed remarkably simple and efficient procedures for expressing the polynomial interpolant to the integrand function as a linear combination of Chebyshev polynomials, and then integrating the resulting approximation in closed form to arrive at an approximate value for the integral. Alternatively, techniques based on the fast Fourier transform can be used to implement this type of quadrature rule. These efficient, self-contained implementations of quadrature rules based on the Chebyshev points have become known as Clenshaw-Curtis quadrature. Although the zeros and extrema of the Chebyshev polynomials have similar properties in terms of the stability and accuracy of the resulting quadrature rules, the Chebyshev extrema, unlike the Chebyshev zeros, have the advantage of yielding progressive quadrature rules. If the number of Chebyshev extrema is increased from n to 2n − 1, then only n − 1 new points are added, so that only n − 1 new function evaluations are required. For this reason, use of the Chebyshev extrema is sometimes called practical Clenshaw-Curtis quadrature, whereas use of the Chebyshev zeros is called classical Clenshaw-Curtis quadrature or Fejér quadrature. We have seen that Clenshaw-Curtis quadrature rules have many virtues: stability, accuracy, simplicity, self- containment, and progressiveness. Nevertheless, the degree of an n-point rule is only n−1, which is well below the maximum possible. Next we will see that quadrature rules of maximum degree can be derived by exploiting all of the available degrees of freedom. 9",
    "source": "cours-de-integration.pdf",
    "page": 9
  },
  {
    "text": "derived by exploiting all of the available degrees of freedom. 9",
    "source": "cours-de-integration.pdf",
    "page": 9
  },
  {
    "text": "UM6P College of Computing – School of Computer Science Numerical Analysis College of Computing / UM6P Numerical Analysis 2.3.3. Gaussian Quadrature In the quadrature rules we have seen thus far, the n nodes were prespecified and the n corresponding weights were then optimally chosen to maximize the degree of the resulting quadrature rule. With only n parameters free to be chosen, the resulting degree is generally n − 1. If the locations of the nodes were also freely chosen, however, then there would be 2n free parameters, so that a degree of 2n − 1 should be achievable. In Gaussian quadrature, both the nodes and the weights are optimally chosen to maximize the degree of the resulting quadrature rule. In general, for each n there is a unique n-point Gaussian rule, and it is of degree 2n − 1. Gaussian quadrature rules therefore have the highest possible accuracy for the number of nodes used, but they are significantly more difficult to derive than Newton-Cotes rules. The nodes and weights can still be determined by the method of undetermined coefficients, but the resulting system of equations is nonlinear. Example 4: Gaussian Quadrature Rule To illustrate the derivation of a Gaussian quadrature rule, we will derive a two-point rule on the interval [−1, 1], I(f) = Z 1 −1 f(x)dx ≈ w1f (x1) + w2f (x2) = G2(f) where the nodes x1, x2 as well as the weights w1, w2 are to be chosen to maximize the resulting degree. Requiring that the rule integrate the first four monomials exactly gives the system of four moment equations w1 + w2 = Z 1 −1 1dx = 2 w1x1 + w2x2 = Z 1 −1 xdx = 0 w1x2 1 + w2x2 2 = Z 1 −1 x2dx = 2 3 w1x3 1 + w2x3 2 = Z 1 −1 x3dx = 0 One solution for this nonlinear system is given by x1 = −1/ √ 3, x 2 = 1/ √ 3, w 1 = 1, w 2 = 1 and the other solution is obtained by reversing the signs of x1 and x2. Thus, the two-point Gaussian quadrature rule has the form G2(f) = f(−1/ √ 3) + f(1/ √ 3) and by construction it has degree three. Alternatively, the nodes of a Gaussian quadrature rule can be obtained by using orthogonal polynomials. If p is a polynomial of degree n such",
    "source": "cours-de-integration.pdf",
    "page": 10
  },
  {
    "text": "x2. Thus, the two-point Gaussian quadrature rule has the form G2(f) = f(−1/ √ 3) + f(1/ √ 3) and by construction it has degree three. Alternatively, the nodes of a Gaussian quadrature rule can be obtained by using orthogonal polynomials. If p is a polynomial of degree n such that Z b a p(x)xkdx = 0, k = 0, . . . , n− 1 and hence p is orthogonal to all polynomials on [a, b] of degree less than n, then it is fairly easy to show that 1. The n zeros of p are real, simple, and lie in the open interval (a, b). 2. The n-point interpolatory quadrature rule on [a, b] whose nodes are the zeros of p has degree 2n − 1; i.e., it is the unique n-point Gaussian rule. The Legendre polynomial Pn is just such a polynomial. For this reason, the resulting rule is often called a Gauss-Legendre quadrature rule. Of course, the zeros of the Legendre polynomial must still be computed, and then the corresponding weights for the quadrature rule can be determined in the usual way. This method also extends naturally to various other weight functions and intervals corresponding to other families of orthogonal polynomials. The nodes and weights for a Gaussian quadrature rule can also be 10",
    "source": "cours-de-integration.pdf",
    "page": 10
  },
  {
    "text": "UM6P College of Computing – School of Computer Science Numerical Analysis College of Computing / UM6P Numerical Analysis computed by solving an eigenvalue problem for a tridiagonal matrix associated with the corresponding orthogonal polynomials and weight function. Example 4 is typical in that for any n the Gaussian nodes are symmetrically placed about the midpoint of the interval; for odd values of n the midpoint itself is always a node. Example 4 is also typical in that the nodes are usually irrational numbers even when the endpoints a and b are rational. This feature makes Gaussian rules relatively inconvenient for hand computation, compared with simple NewtonCotes rules. When using a computer, however, the nodes and weights are usually tabulated in advance and contained in a subroutine that can be called when needed, so the user need not compute or even know their actual values. Gaussian quadrature rules are also more difficult to apply than Newton-Cotes rules because the weights and nodes are derived for some specific interval, such as [−1, 1], and thus any other interval of integration [a, b] must be transformed into the standard interval for which the nodes and weights have been tabulated. If we wish to use a quadrature rule that is tabulated on the interval [α, β], Z β α f(x)dx ≈ nX i=1 wif (xi) to approximate an integral on the interval [a, b], I(g) = Z b a g(t)dt then we must use a change of variable from x in [α, β] to t in [a, b]. Many such transformations are possible, but a simple linear transformation t = (b − a)x + aβ − bα β − α has the advantage of preserving the degree of the quadrature rule. The integral is then given by I(g) = b − a β − α Z β α g \u0012(b − a)x + aβ − bα β − α \u0013 dx ≈ b − a β − α nX i=1 wig \u0012(b − a)xi + aβ − bα β − α \u0013 Example 5: Change of Interval To illustrate a change of interval, we use the two-point Gaussian quadrature rule G2 derived for the interval [−1, 1] in Example 4 to approximate the integral I(g) = Z 1 0 e−t2 dt from Example 2. Using the linear transformation of variable just given, we have t = x + 1 2 so that the integral",
    "source": "cours-de-integration.pdf",
    "page": 11
  },
  {
    "text": "use the two-point Gaussian quadrature rule G2 derived for the interval [−1, 1] in Example 4 to approximate the integral I(g) = Z 1 0 e−t2 dt from Example 2. Using the linear transformation of variable just given, we have t = x + 1 2 so that the integral is approximated by G2(g) = 1 2  exp  − (−1/ √ 3) + 1 2 !2  + exp  − (1/ √ 3) + 1 2 !2    ≈ 0.746595 which is slightly more accurate than the result given by Simpson’s rule for this integral (see Example 2), despite using only two points instead of three. By design, Gaussian quadrature rules have maximal degree, and hence optimal accuracy, for the number of points used. Moreover, it can be shown that the resulting weights are always positive for any n, so that Gaussian quadrature rules are always stable and the resulting approximate values converge to the exact integral as n → ∞. Unfortunately, Gaussian quadrature rules also have a serious drawback: for m ̸= n, Gm and Gn have no nodes in common (except for the midpoint when m and n are both odd). Thus, Gaussian rules are not progressive, 11",
    "source": "cours-de-integration.pdf",
    "page": 11
  },
  {
    "text": "UM6P College of Computing – School of Computer Science Numerical Analysis College of Computing / UM6P Numerical Analysis which means that when the number of nodes is increased, say from n to m, mnew evaluations of the integrand are required rather than m − n. We will see next how this deficiency can be remedied, but at some cost in the degree attainable. 2.3.4. Progressive Gaussian Quadrature We have just observed that Gaussian quadrature rules are not progressive: if all the nodes and weights are freely chosen to maximize the degree for any given number of nodes, then rules with different numbers of nodes will have essentially no nodes in common, which means that integrand values computed for one set of nodes cannot be reused in evaluating another rule with a different number of nodes. Avoiding this additional work is the motivation for Kronrod quadrature rules. Such rules come in pairs: an n-point Gaussian rule Gn and a (2n + 1)-point Kronrod rule K2n+1 whose nodes are optimally chosen subject to the constraint that all of the nodes of Gn are reused in K2n+1. Thus, n of the nodes used in K2n+1 are prespecified, leaving the remaining n + 1 nodes, as well as all 2n + 1 of the weights (including those corresponding to the nodes of Gn ), free to be chosen to maximize the degree of the resulting rule. The rule K2n+1 is therefore of degree 3n + 1, whereas a true (2n + 1)-point Gaussian rule would be of degree 4n + 1. Thus, there is a tradeoff between accuracy and efficiency. One of the main reasons for using two quadrature rules with different numbers of points is to obtain an error estimate for the approximate value of the integral based on the difference between the values given by the two rules. In using a GaussKronrod pair, the value of K2n+1 is taken as the approximation to the integral, and a realistic but conservative estimate for the error, based partly on theory and partly on experience, is given by (200 |Gn − K2n+1|)1.5 Because they efficiently provide both high accuracy and a reliable error estimate, Gauss-Kronrod rules are among the most effective quadrature methods available, and they form the basis for many of the quadrature routines in major software libraries. The pair of rules (G7, K15), in particular, has become a commonly used standard. This approach",
    "source": "cours-de-integration.pdf",
    "page": 12
  },
  {
    "text": "provide both high accuracy and a reliable error estimate, Gauss-Kronrod rules are among the most effective quadrature methods available, and they form the basis for many of the quadrature routines in major software libraries. The pair of rules (G7, K15), in particular, has become a commonly used standard. This approach of adding optimally chosen new nodes to a prespecified set from another rule is taken a step further in Patterson quadrature rules. In particular, adding 2n + 2 optimally chosen nodes to the 2n + 1 nodes of the Kronrod rule K2n+1 yields a quadrature rule with degree 6n + 4 that reuses all of the 2n + 1 integrand values already computed for Gn and K2n+1. For certain values of n, further extensions of this type are possible, and some of these have been tabulated for rules of up to 511 points. Before we leave this topic, we note that much more modest extensions of Gaussian quadrature rules are sometimes useful. A true Gaussian quadrature rule is always open, i.e., the nodes never include the endpoints of the interval of integration. But for some purposes, it is useful if one or both endpoints are included. In Gauss-Radau quadrature, one of the endpoints of the interval of integration is prespecified as a node, leaving the remaining n − 1 nodes, as well as all n weights, free to be chosen to maximize the degree of the resulting rule. There are 2n−1 free parameters, so an n-point Gauss-Radau rule has degree 2n−2. Similarly, in Gauss-Lobatto quadrature, both endpoints of the interval of integration are prespecified as nodes, leaving the remaining n − 2 nodes, as well as all n weights, free to be chosen to maximize the degree of the resulting rule. There are 2n − 2 free parameters, so an n-point Gauss-Lobatto rule has degree 2n − 3. 2.3.5. Composite Quadrature Thus far we have considered simple quadrature rules obtained by interpolating the integrand function by a single polynomial over the entire interval of integration. The accuracy of such a rule can be increased, and the error estimated, by increasing the number of interpolation points, and hence the corresponding degree of the polynomial interpolant. Another alternative is to subdivide the original interval into two or more subintervals and apply a simple quadrature rule in each subinterval. Summing these partial results then yields an approximation to the overall integral. Such an",
    "source": "cours-de-integration.pdf",
    "page": 12
  },
  {
    "text": "the number of interpolation points, and hence the corresponding degree of the polynomial interpolant. Another alternative is to subdivide the original interval into two or more subintervals and apply a simple quadrature rule in each subinterval. Summing these partial results then yields an approximation to the overall integral. Such an approach is equivalent to using piecewise polynomial interpolation on the original interval and then integrating the piecewise interpolant to approximate the integral. A composite, or compound, quadrature rule on a given interval [a, b] results from subdividing the interval into k subintervals, typically of uniform length h = (b − a)/k, applying an n-point simple quadrature rule Qn in each subinterval, and then taking the sum of these results as the approximate value of the integral. If the rule Qn is open, then evaluating the composite rule will require kn evaluations of the integrand function. If Qn is closed, on the other hand, then some of the points are repeated, so that only k(n − 1) + 1 evaluations of the integrand are required. 12",
    "source": "cours-de-integration.pdf",
    "page": 12
  },
  {
    "text": "UM6P College of Computing – School of Computer Science Numerical Analysis College of Computing / UM6P Numerical Analysis Example 6: Composite Quadrature Rules If the interval [a, b] is subdivided into k subintervals of length h = (b − a)/k and xj = a + jh, j= 0, . . . , k, then the composite midpoint rule is given by Mk(f) = kX j=1 (xj − xj−1) f \u0012xj−1 + xj 2 \u0013 = h kX j=1 f \u0012xj−1 + xj 2 \u0013 and the composite trapezoid rule is given by Tk(f) = kX j=1 (xj − xj−1) 2 (f (xj−1) + f (xj)) = h \u00121 2f(a) + f (x1) + ··· + f (xk−1) + 1 2f(b) \u0013 A composite rule is always stable provided the underlying rule Qn is stable. Convergence to the exact integral as the number of subintervals k → ∞is guaranteed, provided the underlying rule Qn has degree at least zero (i.e., it integrates constants exactly). To see why, let the composite rule be given by Ck(f) = kX j=1 nX i=1 wif (xij) ! where wi is the i th weight of Qn and xij is the i th node of Qn in the j th subinterval. Interchanging the order of summation, we have Ck(f) = nX i=1   kX j=1 wif (xij)   = nX i=1 wi   kX j=1 f (xij)   = 1 h nX i=1 wi   kX j=1 hf (xij)   The latter expression in parentheses is a Riemann sum, which by definition has limit I(f) as k → ∞. Thus, we have lim k→∞ Ck(f) = 1 h nX i=1 wi lim k→∞   kX j=1 hf (xij)   = I(f) 1 h nX i=1 wi = I(f) The last equality follows from the fact that Qn integrates constants exactly, so that the sum of the weights is equal to h, the subinterval length. Thus, in principle, by taking k sufficiently large it is possible to achieve arbitrarily high accuracy (up to the limit of the arithmetic precision) using a composite rule, even with an underlying rule Qn of low degree, although this may not be the most efficient way to attain a given level of accuracy. Indeed, the general error bound in Section §2.3 suggests that more can usually be gained by increasing n than by decreasing",
    "source": "cours-de-integration.pdf",
    "page": 13
  },
  {
    "text": "using a composite rule, even with an underlying rule Qn of low degree, although this may not be the most efficient way to attain a given level of accuracy. Indeed, the general error bound in Section §2.3 suggests that more can usually be gained by increasing n than by decreasing h. In practice, a compromise between these is usually most appropriate. Composite quadrature rules offer a particularly simple means of estimating the error by using different levels of subdivision, which can easily be made progressive. We observed in Section §2.3.1 that halving the interval length reduces the error in the midpoint or trapezoid rules by a factor of about 1/8. Halving their length doubles the number of subintervals, however, so the overall reduction in the error is by a factor of about 1/4. If the number of subintervals is k, and hence the subinterval length is h = (b − a)/k, then the dominant term in the remainder for the composite midpoint or trapezoid rules is O \u0000 kh3\u0001 = O \u0000 h2\u0001 , so the accuracy of these rules is said to be of second order. Similarly, the composite Simpson’s rule is of fourth-order accuracy, meaning that the dominant term in its remainder is O \u0000 h4\u0001 , and hence halving the subinterval length reduces the overall error by a factor of about 1/16. 2.3.6. Adaptive Quadrature A composite quadrature rule with an error estimate suggests a simple automatic quadrature procedure: continue subdividing all the subintervals until the estimated overall error meets the desired accuracy tolerance. Main- taining uniform subdivisions is grossly inefficient for many integrands, however, as large numbers of function 13",
    "source": "cours-de-integration.pdf",
    "page": 13
  },
  {
    "text": "UM6P College of Computing – School of Computer Science Numerical Analysis College of Computing / UM6P Numerical Analysis evaluations may be expended in regions where the integrand function is well behaved and the accuracy toler- ance is easily met. A more intelligent approach is adaptive quadrature, in which the interval of integration is selectively refined to reflect the behavior of any particular integrand function. A typical adaptive quadrature strategy works as follows. First we need a pair of quadrature rules, say Qn1 and Qn2 , whose difference provides an error estimate. A simple example is the trapezoid and midpoint rules, whose difference overestimates the error in the more accurate rule by a factor of three, as we saw in Section §2.3.1. Greater efficiency is usually obtained with rules of higher degree, however, such as the Gauss-Kronrod pair (G7, K15). Another alternative is to use a single rule at two different levels of subdivision; Simpson’s rule is a popular choice in this approach. In any case, to minimize the number of function evaluations required, the pair of rules should be progressive. The adaptive procedure is now conceptually simple: apply both rules Qn1 and Qn2 on the initial interval of integration [a, b]. If the resulting approximate values for the integral differ by more than the desired tolerance, divide the interval into two or more subintervals and repeat the procedure on each subinterval. If the tolerance is met on a given subinterval, then no further subdivision of that subinterval will be required. If the tolerance is not met on a given subinterval, then the subdivision process is repeated again, and so on until the tolerance is met on all subintervals. Such a strategy leads to a nonuniform sampling of the integrand function that places many sample points in regions where the function is difficult to integrate and relatively few points where the function is easily integrated, as illustrated in Fig. 2.4. Figure 2.4: T ypical placement of evaluation points by adaptive quadrature routine The high-level description just given glosses over a number of important implementation issues, including • How should the stopping criterion be implemented? For example, should the error tolerance be relative (usually preferable), or absolute (in case the value of the integral is near zero), or a combination of the two? • Can the error tolerance always eventually be met? Will the recursion always terminate? • How can we avoid",
    "source": "cours-de-integration.pdf",
    "page": 14
  },
  {
    "text": "criterion be implemented? For example, should the error tolerance be relative (usually preferable), or absolute (in case the value of the integral is near zero), or a combination of the two? • Can the error tolerance always eventually be met? Will the recursion always terminate? • How can we avoid wasting time subdividing unconverged subintervals that make a negligible contribution to the total integral? • How should we allow for the effects of finite-precision arithmetic? For example, what if the length of a subinterval becomes so small that it contains no machine numbers other than its endpoints? In many adaptive quadrature routines these issues are addressed by using some combination of relative and absolute error tests involving machine-dependent parameters (such as the machine precision ϵmach ), together with an upper limit on the number of levels of subdivision allowed. When confronted with a difficult problem, such as a noisy or unsmooth integrand or an unrealistically tight error tolerance, such routines may expend a large number of function evaluations before returning an inaccurate answer accompanied by a warning message that the subdivision limit was exceeded. Gander and Gautschi suggested an alternative approach that often does considerably better. The key idea is to develop a termination criterion that is robust, machine independent, and avoids arbitrary limits on the depth of the recursion. An important ingredient in such a criterion is a rough prior estimate of the magnitude of the integral, call it ˆI. Such a rough estimate, which need merely be of the correct order of magnitude, can be obtained in various ways, such as sampling the integrand f at a few randomly chosen points within the interval of integration [a, b]. For a given subinterval, relative (or absolute) agreement of the quadrature rules Qn1 and Qn2 to within machine precision can then be checked by testing whether ˆI + (Qn2 − Qn1 ) differs from ˆI. A user-supplied tolerance ϵ ≥ ϵmach can be implemented by increasing ˆI by a factor of ϵ/ϵmach . Finally, an arbitrary limit on subdivision levels can be avoided in a similarly machine-independent way by testing whether the computed midpoint of a subinterval lies strictly within the subinterval. A generic adaptive quadrature procedure with these features is summarized in Algorithm 8.1. Algorithm 8.1 Adaptive Quadrature $\\operatorname{procedure}$ adaptquad $(f, a, b, \\hat{I})$ $I_{1}=Q_{n_{1}}(f, a, b)$ $I_{2}=Q_{n_{2}}(f, a, b)$ 14",
    "source": "cours-de-integration.pdf",
    "page": 14
  },
  {
    "text": "whether the computed midpoint of a subinterval lies strictly within the subinterval. A generic adaptive quadrature procedure with these features is summarized in Algorithm 8.1. Algorithm 8.1 Adaptive Quadrature $\\operatorname{procedure}$ adaptquad $(f, a, b, \\hat{I})$ $I_{1}=Q_{n_{1}}(f, a, b)$ $I_{2}=Q_{n_{2}}(f, a, b)$ 14",
    "source": "cours-de-integration.pdf",
    "page": 14
  },
  {
    "text": "UM6P College of Computing – School of Computer Science Numerical Analysis College of Computing / UM6P Numerical Analysis $m=a+(b-a) / 2$ if $(m \\leq a$ or $m \\geq b)$ then issue warning return $I_{2}$ end if $\\hat{I}+\\left(I_{2}-I_{1}\\right)=\\hat{I}$ then return $I_{2}$ else return $(\\operatorname{adaptquad}(f, a, m, \\hat{I})+$ $\\operatorname{adaptquad}(f, m, b, \\hat{I}))$ end Although adaptive quadrature procedures tend to be very effective in practice, they can be fooled: both the approximate integral and the error estimate can be completely wrong. The reason is that the integrand function is sampled at only a finite number of points, so it is possible that significant features of the integrand may be missed. For example, it may happen that the interval of integration is very wide, but all of the \"interesting\" behavior of the integrand is confined to a very narrow range. In this case, sampling by the adaptive routine may completely miss the interesting part of the integrand’s behavior, and the resulting value for the integral may be completely wrong. This situation may seem unlikely, but it can happen, for example, if we are trying to evaluate an integral over an unbounded interval and have truncated it unwisely. Another potential difficulty with adaptive quadrature routines is that they may be very inefficient in handling discontinuities (finite jumps) in the integrand or its derivatives. For example, an adaptive routine may expend a great many function evaluations in refining the region around a discontinuity of the integrand because it assumes that the integrand is smooth (but very steep) at such a point. A good way to avoid such behavior is to split the interval at the point of discontinuity into two subintervals and call the quadrature routine separately in each, thereby obviating the need for the routine to resolve the discontinuity. 2.4. Other Integration Problems 2.4.1. Tabular Data Thus far we have assumed that the integrand function can be evaluated at any desired point within the interval of integration. This assumption may not be valid if the integrand is defined only by a table of its values at selected discrete points, as is typical of empirical measurements, for example. A reasonable approach to integrating such tabular data is by piecewise interpolation. For example, integrating the piecewise linear interpolant to tabular data gives a composite trapezoid rule. An excellent method for integrating tabular data is provided by Hermite cubic or cubic spline interpolation. In effect, the overall integral",
    "source": "cours-de-integration.pdf",
    "page": 15
  },
  {
    "text": "for example. A reasonable approach to integrating such tabular data is by piecewise interpolation. For example, integrating the piecewise linear interpolant to tabular data gives a composite trapezoid rule. An excellent method for integrating tabular data is provided by Hermite cubic or cubic spline interpolation. In effect, the overall integral is computed by integrating analytically each of the cubic pieces that make up the interpolant. 2.4.2. Improper Integrals Boundedness of both the integrand function and the interval of integration are inherent in the definition of the Riemann integral. If either the integrand or the interval is unbounded, then it may still be possible to define an improper integral. For an unbounded interval, say [a, ∞), the improper integral is defined as Z ∞ a f(x)dx = lim b→∞ Z b a f(x)dx provided f is integrable on [a, b] for any finite b, and the indicated limit exists and is finite. Unbounded intervals of the form (−∞, b] or (−∞, ∞) are treated similarly. For an integrand that is unbounded at a point c ∈ [a, b], i.e., a vertical asymptote or singularity of the integrand, the improper integral is defined as Z b a f(x)dx = lim γ→c− Z γ a f(x)dx + lim γ→c+ Z b γ f(x)dx provided that f is integrable on any subinterval [a, γ] ⊆ [a, c) and [γ, b] ⊆ (c, b], and the indicated limits exist and are finite. There are a number of ways of computing such improper integrals, assuming of course that the integral exists. For an unbounded interval of integration, one may be able to compute the improper integral using a standard quadrature routine for a finite interval. A number of approaches are possible: 15",
    "source": "cours-de-integration.pdf",
    "page": 15
  },
  {
    "text": "UM6P College of Computing – School of Computer Science Numerical Analysis College of Computing / UM6P Numerical Analysis • Replace any infinite limit of integration by a finite value. Such a finite limit should be chosen carefully so that the omitted tail is negligible or its contribution to the integral can be estimated. But the remaining finite interval should not be so wide that an adaptive quadrature routine will be fooled into sampling the integrand badly. • Transform the variable of integration so that the new interval is finite. T ypical transformations include x = −log t or x = t/(1 − t). Care must be taken not to introduce singularities or other difficulties by such a transformation. Another alternative is to use a quadrature rule, such as Gauss-Laguerre or GaussHermite, that is designed for an unbounded interval. For an integrand having an integrable singularity within the interval of integration, one may be tempted simply to try an adaptive quadrature routine and hope that it will work, but such an approach is unlikely to prove satisfactory. Outright failure will result if the integrand happens to be evaluated at the singularity, which will likely occur if the singularity lies at one of the endpoints, as singularities often do. Even if the routine is lucky enough to avoid evaluating the integrand at the singularity, an adaptive quadrature routine will generally be extremely inefficient for an integrand having a singularity because polynomials, which never have vertical asymptotes, cannot efficiently approximate functions that do (recall that our error bounds depend on higher derivatives of the integrand, which will inevitably be large near a singularity). A better approach for dealing with a singularity in the integrand is to remove the singularity either by transform- ing the variable of integration or by dividing out or subtracting off an analytically integrable function having the same singularity. As an example of the first approach, the integral Z π/2 0 cos(x)√x dx has a singularity at x = 0, but under the transformation x = t2 it becomes the innocuous integral Z π/2 0 cos(x)√x dx = Z √ π/2 0 cos \u0000 t2\u0001 t 2tdt = 2 Z √ π/2 0 cos \u0000 t2\u0001 dt which has no singularity and is easily integrable by an adaptive routine. As an example of the second approach, the integral Z π/2 0 1√ sin x dx has a singularity at x",
    "source": "cours-de-integration.pdf",
    "page": 16
  },
  {
    "text": "π/2 0 cos \u0000 t2\u0001 t 2tdt = 2 Z √ π/2 0 cos \u0000 t2\u0001 dt which has no singularity and is easily integrable by an adaptive routine. As an example of the second approach, the integral Z π/2 0 1√ sin x dx has a singularity at x = 0, but by subtracting off 1/√x we have Z π/2 0 1√ sin x dx = Z π/2 0 \u0012 1√ sin x − 1√x \u0013 dx + Z π/2 0 1√xdx The first of the two resulting integrands is now well behaved near 0 and can safely be replaced by 0 at 0 , and the second is analytically integrable to give the value√ 2π. There is often some art involved in finding such transformations, and not all singularities are removable in this manner. If there is more than one singularity, then the transformations required to remove them may conflict, in which case a remedy is to break the interval of integration into subintervals, each of which contains at most one singularity, typically at one endpoint. 2.4.3. Double Integrals Thus far we have considered only one-dimensional integrals, where we wish to determine the area under a curve over an interval. In evaluating a two-dimensional, or double integral, we wish to compute the volume under a surface over a planar region. For a rectangular region [a, b] × [c, d] ⊆ R2, a double integral has the form Z b a Z d c f(x, y)dxdy For a more general domain Ω ⊆ R2, the integral takes the form ZZ Ω f(x, y)dA By analogy with numerical quadrature for one-dimensional integrals, the numerical approximation of two- dimensional integrals is sometimes called numerical cubature. To evaluate a double integral, a number of approaches are available, including the following: 16",
    "source": "cours-de-integration.pdf",
    "page": 16
  },
  {
    "text": "UM6P College of Computing – School of Computer Science Numerical Analysis College of Computing / UM6P Numerical Analysis • Use a pair of adaptive one-dimensional quadrature routines, one for the outer integral and the other for the inner integral. Each time the outer routine calls its integrand function, the latter in turn calls the inner quadrature routine. This approach requires some care in setting the error tolerances for the respective quadrature routines. • Use a Cartesian product rule. Such rules result from applying one-dimensional quadrature rules in successive dimensions. This approach is limited to regions that can be decomposed into rectangles. • Use a nonproduct interpolatory cubature rule. Such rules, with error estimates, are available for a number of standard regions, the most important of which for adaptive use is triangles, since many two-dimensional regions can be efficiently triangulated to any desired level of refinement. 2.4.4. Multiple Integrals To evaluate a multiple integral in dimensions higher than two, the options just listed for double integrals still work in principle, but their cost grows rapidly with the number of dimensions. The only generally viable approach for computing integrals in higher dimensions is the Monte Carlo method. The function is sampled at n points distributed randomly in the domain of integration, and then the mean of these function values is multiplied by the area (or volume, etc.) of the domain to obtain an estimate for the integral. The error in this estimate goes to zero as 1/√n, which means, for example, that to gain an additional decimal digit of accuracy the number of sample points must be increased by a factor of 100. For this reason, it is not unusual for Monte Carlo calculations of integrals to require millions of evaluations of the integrand. The Monte Carlo method is not competitive for integrals in one or two dimensions, but the beauty of the method is that its convergence rate is independent of the number of dimensions. Thus, for example, one million points in six dimensions amounts to only ten points per dimension, which is much better than any type of conventional quadrature rule would require for the same level of accuracy. The efficiency of Monte Carlo integration can be enhanced by various methods for biasing the sampling, either to achieve more uniform coverage of the sampled volume (e.g., by avoiding undesirable random clumping of the sample points) or to concentrate sampling in",
    "source": "cours-de-integration.pdf",
    "page": 17
  },
  {
    "text": "rule would require for the same level of accuracy. The efficiency of Monte Carlo integration can be enhanced by various methods for biasing the sampling, either to achieve more uniform coverage of the sampled volume (e.g., by avoiding undesirable random clumping of the sample points) or to concentrate sampling in regions where the integrand is largest in magnitude (importance sampling) or in variability (stratified sampling), in a spirit similar to adaptive quadrature. 2.5. Integral Equations An integral equation is an equation in which the unknown to be determined is a function inside an integral sign. An integral equation can be thought of as a continuous analogue, or limiting case, of a system of algebraic equations. For example, the analogue of a linear system Ax = y is a Fredholm integral equation of the first kind, which has the form Z b a K(s, t)u(t)dt = f(s) where the functions K, called the kernel, and f are known, and the function u is to be determined. Integral equations arise naturally in many fields of science and engineering, particularly observational sciences (e.g., astronomy, seismology, spectrometry, tomography), where the kernel K represents the response function of an instrument (determined by calibration with known signals), f represents measured data, and u represents the underlying signal that is sought. In effect, we are trying to resolve the measured data f as a (continuous) linear combination of standard signals. Integral equations also result from Green’s function or boundary element methods [289, 290] for solving differential equations. Establishing the existence and uniqueness of solutions to integral equations is much more problematic than with algebraic equations. Moreover, when a solution does exist, it may be extremely sensitive to perturbations in the input data, which are often subject to random experimental or measurement errors. The reason for this sensitivity is that integration is a smoothing process, so its inverse (i.e., determining the integrand from the integral) is just the opposite. Integrating an arbitrary function u against a smooth kernel K dampens any high-frequency oscillation, so solving for u tends to introduce high-frequency oscillation in the result. According to the Riemann-Lebesgue Lemma, lim n→∞ Z b a K(s, t) sin(nt)dt = 0 for any integrable kernel K, which implies that an arbitrarily high-frequency component of u has an arbitrarily small effect on f. Thus, integral equations of the first kind with smooth kernels are always ill-conditioned. 17",
    "source": "cours-de-integration.pdf",
    "page": 17
  },
  {
    "text": "Riemann-Lebesgue Lemma, lim n→∞ Z b a K(s, t) sin(nt)dt = 0 for any integrable kernel K, which implies that an arbitrarily high-frequency component of u has an arbitrarily small effect on f. Thus, integral equations of the first kind with smooth kernels are always ill-conditioned. 17",
    "source": "cours-de-integration.pdf",
    "page": 17
  },
  {
    "text": "UM6P College of Computing – School of Computer Science Numerical Analysis College of Computing / UM6P Numerical Analysis A standard technique for solving integral equations numerically is to use a quadrature rule to replace the integral by an approximating finite sum. Denote the nodes and weights of the quadrature rule by tj and wj, j= 1, . . . , n. We also choose n points si for the variable s, often the same as the tj, but not necessarily so. Then the approximation to the integral equation becomes nX j=1 wjK (si, tj) u (tj) = f (si) , i = 1, . . . n This is a system of linear algebraic equations Ax = y, where aij = wjK (si, tj), yi = f (si), and xj = u (tj), which can be solved for x to obtain a discrete sample of approximate values of the solution function u. Example 7: Integral Equation Consider the integral equation Z 1 −1 (1 + αst)u(t)dt = 1 i.e., K(s, t) = 1 + αst and f(s) = 1, where α is a known positive constant whose value is unspecified for now. Using the composite midpoint quadrature rule with two subintervals, taking t1 = −1 2 , t2 = 1 2 , and w1 = w2 = 1, and also taking s1 = −1 2 and s2 = 1 2 , we obtain the linear system Ax = \u0014 1 + α/4 1 − α/4 1 − α/4 1 + α/4 \u0015\u0014 x1 x2 \u0015 = \u0014 1 1 \u0015 = y It is easily verified that the solution to this linear system is x = \u0002 1 2 1 2 \u0003T , independent of the value of α. Now suppose that the errors in the measured values of y1 = f (s1) and y2 = f (s2) are ϵ1 and ϵ2, respectively. Then by linearity, the change in the solution x is given by the same linear system, but with right-hand side \u0002 ϵ1 ϵ2 \u0003T . The resulting change in x is therefore given by ∆x = \u0014 ∆x1 ∆x2 \u0015 = \u0014 (ϵ1 − ϵ2) /α + (ϵ1 + ϵ2) /4 (ϵ2 − ϵ1) /α + (ϵ1 + ϵ2) /4 \u0015 Thus, if α is sufficiently small, the relative error in the computed value for x can be arbitrarily large. A very small value for α in",
    "source": "cours-de-integration.pdf",
    "page": 18
  },
  {
    "text": "∆x1 ∆x2 \u0015 = \u0014 (ϵ1 − ϵ2) /α + (ϵ1 + ϵ2) /4 (ϵ2 − ϵ1) /α + (ϵ1 + ϵ2) /4 \u0015 Thus, if α is sufficiently small, the relative error in the computed value for x can be arbitrarily large. A very small value for α in this particular kernel corresponds to a very insensitive instrument with a very flat response. This is reflected in the conditioning of the matrix A, whose columns become more nearly linearly dependent as α decreases in magnitude. This simple example is typical of integral equations with smooth kernels. Note that the sensitivity in the previous example is inherent in the problem and is not due to the method of solving it. In general, such an integral operator with a smooth kernel has zero as an eigenvalue (i.e., there are nonzero functions that it annihilates), and hence using a more accurate quadrature rule makes the conditioning of the linear system worse and the resulting solution more erratic. Because of this behavior, additional information may be required to obtain a physically meaningful solution. Such techniques include: • Truncated singular value decomposition. The solution to the system Ax = y is computed using the SVD of A; but the small singular values of A, which reflect the ill-conditioning, are omitted from the solution. • Regularization. A damped solution is obtained by solving the minimization problem min x \u0000 ∥y − Ax∥2 2 + µ∥x∥2 2 \u0001 where the nonnegative parameter µ determines the relative weight given to the norm of the residual and the norm of the solution. This minimization problem is equivalent to the linear least squares problem \u0014 A√µI \u0015 x ∼= \u0014 y 0 \u0015 More generally, other norms, usually based on first or second differences between its components, can also be used to weight the smoothness of the solution. The LevenbergMarquardt method for nonlinear least squares problems is another example of regularization. 18",
    "source": "cours-de-integration.pdf",
    "page": 18
  },
  {
    "text": "UM6P College of Computing – School of Computer Science Numerical Analysis College of Computing / UM6P Numerical Analysis • Constrained optimization. Some norm of the residual ∥y − Ax∥ is minimized subject to constraints on x that disallow nonphysical solutions. In many applications, for example, the components of the solution x are required to be nonnegative or monotonic. We have considered only Fredholm integral equations of the first kind. Many other types arise in practice, including integral equations of the second kind (eigenvalue problems), Volterra integral equations (which differ from Fredholm integral equations in that the upper limit of integration is the variable s instead of the fixed value b ), singular integral equations (in which one or both of the limits of integration are infinite), and nonlinear integral equations. All types of integral equations can be discretized by means of numerical quadrature, yielding a system of algebraic equations. Alternatively, the unknown function u can be approximated by a linear combination u(t) ≈ Pn j=1 cjϕj(t) of suitably chosen basis functions ϕj, which leads to a system of algebraic equations for the coefficients cj. 2.6. Numerical Differentiation We now turn briefly to numerical differentiation. It is important to realize that differentiation is an inherently sensitive problem, as small perturbations in the data can cause large changes in the result. Integration, on the other hand, is a smoothing process and is inherently stable in this respect. The contrast between differentiation and integration should not be surprising, since they are inverse processes to each other. The difference between them is illustrated in Fig. 2.5, which shows two functions that have equal definite integrals but very different derivatives. Figure 2.5: T wo functions whose integrals are equal but whose derivatives are not When approximating the derivative of a function whose values are known only at a discrete set of points, a good approach is to fit some smooth function to the given discrete data and then differentiate the approximating function to approximate the derivatives of the original function. If the given data are sufficiently smooth, then interpolation may be appropriate; but if the given data are noisy, then a smoothing approximating function, such as a least squares polynomial or spline, is more appropriate. 2.6.1. Finite Difference Approximations Although finite difference formulas are generally inappropriate for discrete or noisy data, they are very useful for approximating derivatives of a smooth function that is known",
    "source": "cours-de-integration.pdf",
    "page": 19
  },
  {
    "text": "given data are noisy, then a smoothing approximating function, such as a least squares polynomial or spline, is more appropriate. 2.6.1. Finite Difference Approximations Although finite difference formulas are generally inappropriate for discrete or noisy data, they are very useful for approximating derivatives of a smooth function that is known analytically, or can be evaluated accurately for any given argument, or is defined implicitly by a differential equation. We now develop some finite difference formulas that will be useful in our study of the numerical solution of differential equations. Given a smooth function f : R → R, we wish to approximate its first and second derivatives at a point x. For a given step size h, consider the Taylor series expansions f(x + h) = f(x) + f′(x)h + f′′(x) 2 h2 + f′′′(x) 6 h3 + ··· and f(x − h) = f(x) − f′(x)h + f′′(x) 2 h2 − f′′′(x) 6 h3 + ··· Solving for f′(x) in the first series, we obtain the forward difference formula f′(x) = f(x + h) − f(x) h − f′′(x) 2 h + ··· ≈ f(x + h) − f(x) h which gives an approximation that is first-order accurate since the dominant term in the remainder of the series is O(h). Similarly, from the second series we derive the backward difference formula 19",
    "source": "cours-de-integration.pdf",
    "page": 19
  },
  {
    "text": "UM6P College of Computing – School of Computer Science Numerical Analysis College of Computing / UM6P Numerical Analysis f′(x) = f(x) − f(x − h) h + f′′(x) 2 h + ··· ≈ f(x) − f(x − h) h which is also first-order accurate. Subtracting the second series from the first gives the centered difference formula f′(x) = f(x + h) − f(x − h) 2h − f′′′(x) 6 h2 + ··· ≈ f(x + h) − f(x − h) 2h which is second-order accurate. Finally, adding the two series together gives a centered difference formula for the second derivative f′′(x) = f(x + h) − 2f(x) + f(x − h) h2 − f(4)(x) 12 h2 + ··· ≈ f(x + h) − 2f(x) + f(x − h) h2 which is also second-order accurate. By using function values at additional points, x ± 2h, x± 3h, . . ., we can derive similar finite difference approximations with still higher accuracy or for higher-order derivatives. Note that higher-accuracy difference formulas require more function values. Whether these translate into higher overall cost depends on the particular situation, since a more accurate formula may permit the use of a larger step size h and correspondingly fewer steps. In choosing a value for h, rounding error must also be considered in addition to the truncation error given by the series expansion. Using Taylor series expansions to derive finite difference formulas becomes increasingly cumbersome for ap- proximations of increasingly high accuracy or higherorder derivatives. We next consider an alternative method based on polynomial interpolation that is not only more convenient, but will also more readily permit later generalization, for example to unequally spaced points. Let ti, i= 1, . . . , nbe equally spaced points in R, with step size h = ti+1 − ti, i= 1, . . . , n− 1. Let yi = f (ti) , i= 1, . . . , n, where f : R → R is a smooth function. For i = 1, . . . , n− 1, the polynomial of degree one interpolating the two data points (ti, yi) , (ti+1, yi+1) is given by the Lagrange interpolant p(t) = yi t − ti+1 ti − ti+1 + yi+1 t − ti ti+1 − ti = yi t − ti+1 −h + yi+1 t − ti h Differentiating this polynomial with respect to t, we",
    "source": "cours-de-integration.pdf",
    "page": 20
  },
  {
    "text": "data points (ti, yi) , (ti+1, yi+1) is given by the Lagrange interpolant p(t) = yi t − ti+1 ti − ti+1 + yi+1 t − ti ti+1 − ti = yi t − ti+1 −h + yi+1 t − ti h Differentiating this polynomial with respect to t, we have p′(t) = yi+1 − yi h which is the same as the first-order, forward difference formula for the first derivative that we derived earlier using Taylor series. The first-order, backward difference formula for the first derivative can be derived similarly by interpolating (ti−1, yi−1) , (ti, yi) For i = 2, . . . , n−1, the polynomial of degree two interpolating the three data points (ti−1, yi−1) , (ti, yi) , (ti+1, yi+1) is given by the Lagrange interpolant p(t) =yi−1 (t − ti) (t − ti+1) (ti−1 − ti) (ti−1 − ti+1) + yi (t − ti−1) (t − ti+1) (ti − ti−1) (ti − ti+1) + yi+1 (t − ti−1) (t − ti) (ti+1 − ti−1) (ti+1 − ti) =yi−1 (t − ti) (t − ti+1) 2h2 + yi (t − ti−1) (t − ti+1) −h2 + yi+1 (t − ti−1) (t − ti) 2h2 Differentiating this polynomial with respect to t, we have p′(t) = yi−1 (t − ti) + (t − ti+1) 2h2 + yi (t − ti−1) + (t − ti+1) −h2 + yi+1 (t − ti−1) + (t − ti) 2h2 , and evaluating the derivative at t = ti then gives 20",
    "source": "cours-de-integration.pdf",
    "page": 20
  },
  {
    "text": "UM6P College of Computing – School of Computer Science Numerical Analysis College of Computing / UM6P Numerical Analysis p′ (ti) = yi+1 − yi−1 2h which is the same as the second-order, centered difference formula for the first derivative that we derived earlier using Taylor series. Finally, differentiating a second time gives p′′(t) = yi−1 2 2h2 + yi 2 −h2 + yi+1 2 2h2 = yi+1 − 2yi + yi−1 h2 which is the same as the second-order, centered difference formula for the second derivative that we derived earlier using Taylor series. We can continue in this manner to compute approximate derivatives of higher accuracy or higher order by interpolating more points and using correspondingly higher degree polynomials. Indeed, we can achieve the highest possible accuracy or order for a given number of data points by interpolating all of the data points with a single polynomial. Moreover, this interpolatory approach is easily generalized. We could use other representations of the interpolating polynomial. We could use unequally spaced points, for example the Chebyshev points to achieve higher accuracy. We could use interpolating functions other than polynomials, for example trigonometric functions for periodic data. 2.6.2. Automatic Differentiation A number of alternatives are available for computing derivatives of a function, including finite difference ap- proximations or closed-form formulas determined either by hand or by a computer algebra package. Each of these methods has significant drawbacks, however: manual differentiation is tedious and error-prone; symbolic derivatives tend to be unwieldy for complicated functions; finite difference approximations require a sometimes delicate choice of step size, and their accuracy is limited by discretization error. Another alternative, at least for any function expressed by a computer program, is automatic differentiation, often abbreviated as AD. The basic idea of AD is simple: a computer program consists of basic arithmetic operations and elementary functions, each of whose derivatives is easily computed. Thus, the function computed by the program is, in effect, a composite of many simple functions whose derivatives can be propagated through the program by repeated use of the chain rule, effectively computing the derivative of the function step by step along with the function itself. The result is the true derivative of the original function, subject only to rounding error but suffering no discretization error. Though AD is conceptually simple, its practical implementation is more complicated, requiring careful analysis of the input program and clever strategies for",
    "source": "cours-de-integration.pdf",
    "page": 21
  },
  {
    "text": "step by step along with the function itself. The result is the true derivative of the original function, subject only to rounding error but suffering no discretization error. Though AD is conceptually simple, its practical implementation is more complicated, requiring careful analysis of the input program and clever strategies for reducing the potentially explosive complexity of the resulting derivative code. Fortunately, most of these practical impediments have been successfully overcome, and a number of effective software packages are now available for automatic differentiation. Some of these packages accept a Fortran or C input program and then output a second program for computing the desired derivatives, whereas other packages use operator overloading to perform derivative computations automatically along with the function evaluation. When applicable, AD can be much easier, more efficient, and more accurate than other methods for computing derivatives. AD can also be useful for determining the sensitivity of the output of a program to perturbations in its input parameters. Such information might otherwise be obtainable only through many repeated runs of the program, which could be prohibitively expensive for a large, complex program. 8.7 Richardson Extrapolation In many problems, such as numerical integration or differentiation, we compute an approximate value for some quantity based on some step size. Ideally, we would like to obtain the limiting value as the step size goes to zero, but we cannot take the step size to be arbitrarily small because of excessive cost or rounding error. Based on values for nonzero step sizes, however, we may be able to estimate what the value would be for a step size of zero. Let F(h) denote the value obtained with step size h. If we compute the value of F for some nonzero step sizes, and if we know the theoretical behavior of F(h) as h → 0, then we can extrapolate from the known values to obtain an approximate value for F(0). This extrapolated value should have a higher-order accuracy than the values on which it is based. We emphasize, however, that the extrapolated value, though an improvement, is still only an approximation, not the exact solution, and its accuracy is still limited by the step size and arithmetic precision used. To be more specific, suppose that F(h) = a0 + a1hp + O (hr) 21",
    "source": "cours-de-integration.pdf",
    "page": 21
  },
  {
    "text": "the exact solution, and its accuracy is still limited by the step size and arithmetic precision used. To be more specific, suppose that F(h) = a0 + a1hp + O (hr) 21",
    "source": "cours-de-integration.pdf",
    "page": 21
  },
  {
    "text": "UM6P College of Computing – School of Computer Science Numerical Analysis College of Computing / UM6P Numerical Analysis as h → 0 for some p and r, with r > p. We assume that we know the values of p and r, but not a0 or a1. Indeed, F(0) = a0 is the quantity we seek. Suppose that we have computed F for two step sizes, say, h and h/q for some positive integer q. Then we have F(h) = a0 + a1hp + O (hr) and F(h/q) = a0 + a1(h/q)p + O (hr) = a0 + a1q−php + O (hr) This system of two linear equations in the two unknowns a0 and a1 is easily solved to obtain a0 = F(h) + F(h) − F(h/q) q−p − 1 + O (hr) Thus, the accuracy of the improved value, a0, is O (hr) rather than O (hp). If F(h) is known for several values of h, then the extrapolation process can be repeated to produce still more accurate approximations, up to the limitations imposed by finite-precision arithmetic. For example, if we have computed F for the values h, h/2, and h/4, then the extrapolated value based on h and h/2 can be combined with the extrapolated value based on h/2 and h/4 in a further extrapolation to produce a still more accurate estimate for F(0). Example 8: Richardson Extrapolation To illustrate Richardson extrapolation, we use it to improve the accuracy of a finite difference approximation to the derivative of the function sin(x) at the point x = 1 . Using the first-order accurate, forward difference formula derived in Section §2.6.1, we have for this problem F(h) = a0 + a1h + O \u0000 h2\u0001 which means that p = 1 and r = 2 in this case. Using step sizes of h = 0.5 and h/2 = 0.25 (i.e., q = 2 ), we obtain F(h) = sin(1.5) − sin(1) 0.5 = 0.312048 and F(h/2) = sin(1.25) − sin(1) 0.25 = 0.430055 The extrapolated value is then given by F(0) = a0 = F(h) + F(h) − F(h/2) (1/2) − 1 = 2F(h/2) − F(h) = 0.548061 For comparison, the correctly rounded result is given by cos(1) = 0.540302. Example 9: Romberg Integration As another example of Richardson extrapolation, we evaluate the integral Z π/2 0 sin(x)dx If we use the composite trapezoid quadrature rule, we recall from Section",
    "source": "cours-de-integration.pdf",
    "page": 22
  },
  {
    "text": "− 1 = 2F(h/2) − F(h) = 0.548061 For comparison, the correctly rounded result is given by cos(1) = 0.540302. Example 9: Romberg Integration As another example of Richardson extrapolation, we evaluate the integral Z π/2 0 sin(x)dx If we use the composite trapezoid quadrature rule, we recall from Section §2.3.5 that F(h) = a0 + a1h2 + O \u0000 h4\u0001 which means that p = 2 and r = 4 in this case. With h = π/2, we obtain the value F(h) = 0.785398. Taking q = 2, we obtain the value F(h/2) = F(π/4) = 0.948059. The extrapolated value is then given by 22",
    "source": "cours-de-integration.pdf",
    "page": 22
  },
  {
    "text": "UM6P College of Computing – School of Computer Science Numerical Analysis College of Computing / UM6P Numerical Analysis F(0) = a0 = F(h) + F(h) − F(h/2) 2−2 − 1 = 4F(h/2) − F(h) 3 = 1.002280 which is substantially more accurate than either value previously computed (the exact answer is 1 ). In this example the extrapolation is quadratic, as can be seen on the right in Fig. 8.6, because the lowest-order term in h is quadratic. For any integer k ≥ 0, let Tk,0 denote the approximation to the integral R b a f(x)dx given by the composite trapezoid rule with step size hk = (b − a)/2k. Then for any integer j, j= 1, . . . , k, we can recursively define the successive extrapolated values Tk,j = 4jTk,j−1 − Tk−1,j−1 4j − 1 which form a triangular array T0,0 T1,0 T1,1 T2,0 T2,1 T2,2 T3,0 T3,1 T3,2 T3,3 ... ... ... ... ... . In this example we have already computed T0,0 = 0.785398, T1,0 = 0.948059, and the extrapolated value T1,1 = 1.002280. If we reduce the step size by another factor of two in the composite trapezoid rule, we obtain T2,0 = F(h/4) = F(π/8) = 0.987116 . We can now combine the results for h/2 and h/4 to obtain the extrapolated value T2,1 = F(h/2) + F(h/2) − F(h/4) 2−2 − 1 = 4T2,0 − T1,0 4 − 1 = 1.000135 Because we have eliminated the leading O \u0000 h2\u0001 error term for the composite trapezoid rule, the accuracy of the first level of extrapolated values is O \u0000 h4\u0001 . Thus, we can further extrapolate on these values, but now with p = 4, to obtain T2,2 = 42T2,1 − T1,1 42 − 1 = 16 × 1.000135 − 1.002280 15 = 0.999992 which is still more accurate than any of the values computed previously. Recursive computation of extrapolated values in this manner, based on the composite trapezoid rule with successively halved step sizes, is called Romberg integration. It is capable of producing very high accuracy (up to the limit imposed by the arithmetic precision) for very smooth integrands. It is often implemented in an automatic (though nonadaptive) fashion, with the recurrence continuing until the difference in successive diagonal entries of the triangular array falls below a specified error tolerance. 23",
    "source": "cours-de-integration.pdf",
    "page": 23
  },
  {
    "text": "by the arithmetic precision) for very smooth integrands. It is often implemented in an automatic (though nonadaptive) fashion, with the recurrence continuing until the difference in successive diagonal entries of the triangular array falls below a specified error tolerance. 23",
    "source": "cours-de-integration.pdf",
    "page": 23
  }
]